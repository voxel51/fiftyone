{
    "models": [
        {
            "base_name": "alexnet-imagenet-torch",
            "base_filename": "alexnet-owt-4df8aa71.pth",
            "version": null,
            "description": "Classic neural network that recognizes images and helped launch the deep learning revolution",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Alex Krizhevsky",
            "license": "BSD 3-Clause",
            "size_bytes": 244418560,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.alexnet.alexnet",
                    "entrypoint_args": {
                        "weights": "AlexNet_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "alexnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "deeplabv3-resnet101-coco-torch",
            "base_filename": "deeplabv3_resnet101_coco-586e9e4e.pth",
            "version": null,
            "description": "Labels everyday objects in images pixel by pixel for general scene understanding and analysis",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Liang-Chieh Chen, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 244545539,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/deeplabv3_resnet101_coco-586e9e4e.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.segmentation.deeplabv3_resnet101",
                    "entrypoint_args": {
                        "weights": "DeepLabV3_ResNet101_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor",
                    "output_processor_args": {
                        "has_softmax_out": false
                    },
                    "mask_targets_path": "{{eta-resources}}/voc-labels.txt",
                    "image_dim": 520,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225]
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segmentation",
                "coco",
                "torch",
                "resnet",
                "deeplabv3",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "segment-anything-vitb-torch",
            "base_filename": "sam_vit_b_01ec64.pth",
            "version": null,
            "description": "Interactive segmentation tool that instantly outlines any object you point to or describe",
            "source": "https://segment-anything.com",
            "author": "Alexander Kirillov, et al.",
            "license": "Apache 2.0",
            "size_bytes": 375042383,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam.SegmentAnythingModel",
                "config": {
                    "entrypoint_fcn": "segment_anything.build_sam.build_sam_vit_b",
                    "entrypoint_args": {},
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "segment-anything"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "sa-1b",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2023-05-12 11:40:51"
        },
        {
            "base_name": "segment-anything-vith-torch",
            "base_filename": "sam_vit_h_4b8939.pth",
            "version": null,
            "description": "Highest quality segmentation model creating extremely detailed masks for research and large-scale annotation projects",
            "source": "https://segment-anything.com",
            "author": "Alexander Kirillov, et al.",
            "license": "Apache 2.0",
            "size_bytes": 2564550879,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam.SegmentAnythingModel",
                "config": {
                    "entrypoint_fcn": "segment_anything.build_sam.build_sam_vit_h",
                    "entrypoint_args": {},
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "segment-anything"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "sa-1b",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2023-05-12 11:40:51"
        },
        {
            "base_name": "segment-anything-vitl-torch",
            "base_filename": "sam_vit_l_0b3195.pth",
            "version": null,
            "description": "Large segmentation model producing finer object outlines for professional editing and labeling workflows",
            "source": "https://segment-anything.com",
            "author": "Alexander Kirillov, et al.",
            "license": "Apache 2.0",
            "size_bytes": 1249524607,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam.SegmentAnythingModel",
                "config": {
                    "entrypoint_fcn": "segment_anything.build_sam.build_sam_vit_l",
                    "entrypoint_args": {},
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "segment-anything"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "sa-1b",
                "torch",
                "zero-shot",
                "transformer"
            ],
            "training_data": [
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2023-05-12 11:40:51"
        },
        {
            "base_name": "segment-anything-2-hiera-tiny-image-torch",
            "base_filename": "sam2_hiera_tiny_image.pt",
            "version": null,
            "description": "Smallest image segmentation model offering instant results for mobile apps and embedded systems",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 155906050,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_t.yaml" },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segment-anything", "torch", "zero-shot"],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2-hiera-small-image-torch",
            "base_filename": "sam2_hiera_small_image.pt",
            "version": null,
            "description": "Fast image segmentation model that runs efficiently on laptops and edge computing devices",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 184309650,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_s.yaml" },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2-hiera-base-plus-image-torch",
            "base_filename": "sam2_hiera_base_plus_image.pt",
            "version": null,
            "description": "Accurate image segmentation model for editing, labeling, and creative work with still pictures",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 323493298,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_b+.yaml" },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2-hiera-large-image-torch",
            "base_filename": "sam2_hiera_large_image.pt",
            "version": null,
            "description": "High-quality image segmenter producing detailed masks for demanding professional editing and annotation tasks",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 897952466,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_l.yaml" },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2-hiera-tiny-video-torch",
            "base_filename": "sam2_hiera_tiny_video.pt",
            "version": null,
            "description": "Tiny video segmenter enabling real-time object tracking on phones and compact devices",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 155906050,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_tiny.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_t.yaml" }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2-hiera-small-video-torch",
            "base_filename": "sam2_hiera_small_video.pt",
            "version": null,
            "description": "Quick video segmentation model delivering rapid object tracking on standard graphics cards",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 184309650,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_small.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_s.yaml" },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2-hiera-base-plus-video-torch",
            "base_filename": "sam2_hiera_base_plus_video.pt",
            "version": null,
            "description": "Video segmentation model that tracks and outlines objects throughout clips for editing and analysis",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 323493298,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_base_plus.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_b+.yaml" },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2-hiera-large-video-torch",
            "base_filename": "sam2_hiera_large_video.pt",
            "version": null,
            "description": "Advanced video segmenter providing fine object tracking throughout full videos for post-production work",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 897952466,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/072824/sam2_hiera_large.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_l.yaml" },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "med-sam-2-video-torch",
            "base_filename": "med-sam-2_pretrain.pth",
            "version": null,
            "description": "Medical segmentation tool that outlines organs and structures in medical videos and 3D scans",
            "source": "https://github.com/MedicineToken/Medical-SAM2",
            "author": "Jiayuan Zhu, et al.",
            "license": "Apache 2.0",
            "size_bytes": 78078138,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://huggingface.co/jiayuanz3/MedSAM2_pretrain/resolve/main/MedSAM2_pretrain.pth?download=true"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": { "model_cfg": "sam2_hiera_t.yaml" }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "med-SAM",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-Med2D-20M",
                    "url": "https://github.com/OpenGVLab/SAM-Med2D"
                },
                {
                    "name": "Medical Video Datasets",
                    "url": "https://github.com/MedicineToken/Medical-SAM2"
                }
            ],
            "date_added": "2024-08-17 14:48:00"
        },
        {
            "base_name": "segment-anything-2.1-hiera-tiny-image-torch",
            "base_filename": "sam2.1_hiera_tiny_image.pt",
            "version": null,
            "description": "Enhanced mobile image segmenter for apps, augmented reality filters, and on-device processing",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 155906050,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_t.yaml"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segment-anything", "torch", "zero-shot", "transformer"],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2.1-hiera-small-image-torch",
            "base_filename": "sam2.1_hiera_small_image.pt",
            "version": null,
            "description": "Balanced updated segmenter combining speed and accuracy for edge device image processing",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 184416285,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_s.yaml"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2.1-hiera-base-plus-image-torch",
            "base_filename": "sam2.1_hiera_base_plus_image.pt",
            "version": null,
            "description": "Updated image segmenter with improved mask accuracy for everyday editing and dataset creation",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 323606802,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_b+.yaml"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2.1-hiera-large-image-torch",
            "base_filename": "sam2.1_hiera_large_image.pt",
            "version": null,
            "description": "Large updated model offering even finer masks for high-resolution professional image workflows",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 898083611,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2ImageModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_l.yaml"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2.1-hiera-tiny-video-torch",
            "base_filename": "sam2.1_hiera_tiny_video.pt",
            "version": null,
            "description": "Upgraded mobile video segmenter for live effects on phones, wearables, and smart cameras",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 155906050,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_tiny.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_t.yaml"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2.1-hiera-small-video-torch",
            "base_filename": "sam2.1_hiera_small_video.pt",
            "version": null,
            "description": "Improved video segmenter maintaining quick performance on compact hardware while enhancing mask quality",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 184416285,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_small.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_s.yaml"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2.1-hiera-base-plus-video-torch",
            "base_filename": "sam2.1_hiera_base_plus_video.pt",
            "version": null,
            "description": "Enhanced video segmenter with better tracking quality for video analysis and scene understanding",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 323606802,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_base_plus.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_b+.yaml"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "segment-anything-2.1-hiera-large-video-torch",
            "base_filename": "sam2.1_hiera_large_video.pt",
            "version": null,
            "description": "Large video model producing exceptionally detailed masks throughout long videos for intensive production",
            "source": "https://ai.meta.com/sam2/",
            "author": "Nikhila Ravi, et al.",
            "license": "Apache 2.0, BSD 3-Clause",
            "size_bytes": 898083611,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://dl.fbaipublicfiles.com/segment_anything_2/092824/sam2.1_hiera_large.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.sam2.SegmentAnything2VideoModel",
                "config": {
                    "entrypoint_fcn": "sam2.build_sam.build_sam2_video_predictor",
                    "entrypoint_args": {
                        "model_cfg": "configs/sam2.1/sam2.1_hiera_l.yaml"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "sam2"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segment-anything",
                "torch",
                "zero-shot",
                "video",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "SA-V",
                    "url": "https://ai.meta.com/datasets/segment-anything-video/"
                },
                {
                    "name": "SA-1B",
                    "url": "https://segment-anything.com/dataset/index.html"
                }
            ],
            "date_added": "2024-08-05 14:38:20"
        },
        {
            "base_name": "deeplabv3-resnet50-coco-torch",
            "base_filename": "deeplabv3_resnet50_coco-cd0a2569.pth",
            "version": null,
            "description": "Faster version that quickly identifies and labels objects in images for real-time applications",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Liang-Chieh Chen, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 168312152,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/deeplabv3_resnet50_coco-cd0a2569.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.segmentation.deeplabv3_resnet50",
                    "entrypoint_args": {
                        "weights": "DeepLabV3_ResNet50_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor",
                    "output_processor_args": {
                        "has_softmax_out": false
                    },
                    "mask_targets_path": "{{eta-resources}}/voc-labels.txt",
                    "image_dim": 520,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225]
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segmentation",
                "coco",
                "torch",
                "resnet",
                "deeplabv3",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "densenet121-imagenet-torch",
            "base_filename": "densenet121-a639ec97.pth",
            "version": null,
            "description": "Compact yet powerful classifier that delivers strong results while using minimal computational resources",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Gao Huang, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 32342954,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/densenet121-a639ec97.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.densenet.densenet121",
                    "entrypoint_args": {
                        "weights": "DenseNet121_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "densenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "densenet161-imagenet-torch",
            "base_filename": "densenet161-8d451a50.pth",
            "version": null,
            "description": "Dense network that achieves high accuracy for image classification and adapts well to new tasks",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Gao Huang, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 115730790,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/densenet161-8d451a50.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.densenet.densenet161",
                    "entrypoint_args": {
                        "weights": "DenseNet161_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "densenet"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "densenet169-imagenet-torch",
            "base_filename": "densenet169-b2777c0a.pth",
            "version": null,
            "description": "Deeper variant offering improved accuracy while remaining efficient enough for practical deployment",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Gao Huang, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 57365526,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/densenet169-b2777c0a.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.densenet.densenet169",
                    "entrypoint_args": {
                        "weights": "DenseNet169_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "densenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "densenet201-imagenet-torch",
            "base_filename": "densenet201-c1103571.pth",
            "version": null,
            "description": "Extra-deep model providing the most detailed features for complex image understanding tasks",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Gao Huang, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 81131730,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/densenet201-c1103571.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.densenet.densenet201",
                    "entrypoint_args": {
                        "weights": "DenseNet201_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "densenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "faster-rcnn-resnet50-fpn-coco-torch",
            "base_filename": "fasterrcnn_resnet50_fpn_coco-258fb6c6.pth",
            "version": null,
            "description": "Multi-scale object finder that accurately detects both small and large items in images",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Shaoqing Ren, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 167502836,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.detection.faster_rcnn.fasterrcnn_resnet50_fpn",
                    "entrypoint_args": {
                        "weights": "FasterRCNN_ResNet50_FPN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.DetectorOutputProcessor",
                    "labels_path": "{{eta-resources}}/ms-coco-labels.txt",
                    "confidence_thresh": 0.3
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "faster-rcnn",
                "resnet",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "fcn-resnet101-coco-torch",
            "base_filename": "fcn_resnet101_coco-7ecb50ca.pth",
            "version": null,
            "description": "Creates detailed pixel-level labels for images, identifying and outlining twenty-one different object categories",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Jonathan Long, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 217800805,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.segmentation.fcn_resnet101",
                    "entrypoint_args": {
                        "weights": "FCN_ResNet101_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor",
                    "output_processor_args": {
                        "has_softmax_out": false
                    },
                    "mask_targets_path": "{{eta-resources}}/voc-labels.txt",
                    "image_dim": 520,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225]
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segmentation",
                "coco",
                "torch",
                "fcn",
                "resnet",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "fcn-resnet50-coco-torch",
            "base_filename": "fcn_resnet50_coco-1167a1af.pth",
            "version": null,
            "description": "Fast image labeler that quickly identifies and outlines objects for interactive editing and annotation",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Jonathan Long, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 141567418,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/fcn_resnet50_coco-1167a1af.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.segmentation.fcn_resnet50",
                    "entrypoint_args": {
                        "weights": "FCN_ResNet50_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.SemanticSegmenterOutputProcessor",
                    "output_processor_args": {
                        "has_softmax_out": false
                    },
                    "mask_targets_path": "{{eta-resources}}/voc-labels.txt",
                    "image_dim": 520,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225]
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segmentation",
                "coco",
                "torch",
                "fcn",
                "resnet",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "googlenet-imagenet-torch",
            "base_filename": "googlenet-1378be20.pth",
            "version": null,
            "description": "Classic image classifier providing reliable categorization and features for various computer vision projects.",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Christian Szegedy, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 52147035,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/googlenet-1378be20.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.googlenet.googlenet",
                    "entrypoint_args": {
                        "weights": "GoogLeNet_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["scipy", "torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "googlenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "inception-v3-imagenet-torch",
            "base_filename": "inception_v3_google-1a9a5a14.pth",
            "version": null,
            "description": "Efficient image classifier delivering accurate results with useful features for transfer learning applications",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Christian Szegedy, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 108857766,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.inception.inception_v3",
                    "entrypoint_args": {
                        "weights": "Inception_V3_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_size": [299, 299],
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["scipy", "torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "inception",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "keypoint-rcnn-resnet50-fpn-coco-torch",
            "base_filename": "keypointrcnn_resnet50_fpn_coco-fc266e95.pth",
            "version": null,
            "description": "Finds people in images and maps their body joints for pose estimation and motion analysis",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Kaiming He, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 237034793,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/keypointrcnn_resnet50_fpn_coco-fc266e95.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.detection.keypoint_rcnn.keypointrcnn_resnet50_fpn",
                    "entrypoint_args": {
                        "weights": "KeypointRCNN_ResNet50_FPN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.KeypointDetectorOutputProcessor",
                    "labels_string": "other,person",
                    "confidence_thresh": 0.3,
                    "skeleton": {
                        "labels": [
                            "nose",
                            "left eye",
                            "right eye",
                            "left ear",
                            "right ear",
                            "left shoulder",
                            "right shoulder",
                            "left elbow",
                            "right elbow",
                            "left wrist",
                            "right wrist",
                            "left hip",
                            "right hip",
                            "left knee",
                            "right knee",
                            "left ankle",
                            "right ankle"
                        ],
                        "edges": [
                            [11, 5, 3, 1, 0, 2, 4, 6, 12],
                            [9, 7, 5, 6, 8, 10],
                            [15, 13, 11, 12, 14, 16]
                        ]
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "keypoints",
                "coco",
                "torch",
                "keypoint-rcnn",
                "resnet",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "mask-rcnn-resnet50-fpn-coco-torch",
            "base_filename": "maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth",
            "version": null,
            "description": "Multi-scale object outliner using advanced architecture for accurate segmentation across different object sizes",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Kaiming He, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 178090079,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.detection.mask_rcnn.maskrcnn_resnet50_fpn",
                    "entrypoint_args": {
                        "weights": "MaskRCNN_ResNet50_FPN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.InstanceSegmenterOutputProcessor",
                    "labels_path": "{{eta-resources}}/ms-coco-labels.txt"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "instances",
                "coco",
                "torch",
                "mask-rcnn",
                "resnet",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "mnasnet0.5-imagenet-torch",
            "base_filename": "mnasnet0.5_top1_67.823-3ffadce67e.pth",
            "version": null,
            "description": "Ultra-lightweight image classifier designed by AI for running directly on phones and IoT devices",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Mingxing Tan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 9008489,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/mnasnet0.5_top1_67.823-3ffadce67e.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.mnasnet.mnasnet0_5",
                    "entrypoint_args": {
                        "weights": "MNASNet0_5_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.1"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "mnasnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "mnasnet1.0-imagenet-torch",
            "base_filename": "mnasnet1.0_top1_73.512-f206786ef8.pth",
            "version": null,
            "description": "Mobile-optimized classifier balancing size and accuracy for efficient on-device image recognition",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Mingxing Tan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 17736997,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/mnasnet1.0_top1_73.512-f206786ef8.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.mnasnet.mnasnet1_0",
                    "entrypoint_args": {
                        "weights": "MNASNet1_0_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.1"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "mnasnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "mobilenet-v2-imagenet-torch",
            "base_filename": "mobilenet_v2-b0353104.pth",
            "version": null,
            "description": "Mobile-friendly image classifier optimized for quick training and deployment on resource-limited devices",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Mark Sandler, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 14212972,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/mobilenet_v2-b0353104.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.mobilenet.mobilenet_v2",
                    "entrypoint_args": {
                        "weights": "MobileNet_V2_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.1"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "mobilenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "resnet101-imagenet-torch",
            "base_filename": "resnet101-5d3b4d8f.pth",
            "version": null,
            "description": "Deep image recognition model delivering high accuracy for demanding classification and analysis tasks",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Kaiming He, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 178728960,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/resnet101-5d3b4d8f.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.resnet101",
                    "entrypoint_args": {
                        "weights": "ResNet101_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "resnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "resnet152-imagenet-torch",
            "base_filename": "resnet152-b121ed2d.pth",
            "version": null,
            "description": "Very deep classifier providing the richest visual features for precision-critical image understanding applications",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Kaiming He, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 241530880,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/resnet152-b121ed2d.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.resnet152",
                    "entrypoint_args": {
                        "weights": "ResNet152_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "resnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "resnet18-imagenet-torch",
            "base_filename": "resnet18-5c106cde.pth",
            "version": null,
            "description": "Lightweight image classifier designed for fast recognition on phones and other resource-limited devices",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Kaiming He, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 46827520,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/resnet18-5c106cde.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.resnet18",
                    "entrypoint_args": {
                        "weights": "ResNet18_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "resnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "resnet34-imagenet-torch",
            "base_filename": "resnet34-333f7ec4.pth",
            "version": null,
            "description": "Balanced image classifier offering good accuracy and speed for everyday computer vision needs",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Kaiming He, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 87306240,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/resnet34-333f7ec4.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.resnet34",
                    "entrypoint_args": {
                        "weights": "ResNet34_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "resnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "resnet50-imagenet-torch",
            "base_filename": "resnet50-19c8e357.pth",
            "version": null,
            "description": "Most popular image recognition backbone widely used as starting point for custom vision projects",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Kaiming He, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 102502400,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/resnet50-19c8e357.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.resnet50",
                    "entrypoint_args": {
                        "weights": "ResNet50_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "resnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "resnext101-32x8d-imagenet-torch",
            "base_filename": "resnext101_32x8d-8ba56ff5.pth",
            "version": null,
            "description": "Powerful image classifier with enhanced capacity for handling complex visual recognition challenges effectively",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Saining Xie, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 356082095,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.resnext101_32x8d",
                    "entrypoint_args": {
                        "weights": "ResNeXt101_32X8D_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "resnext",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "resnext50-32x4d-imagenet-torch",
            "base_filename": "resnext50_32x4d-7cdf4587.pth",
            "version": null,
            "description": "Efficient advanced classifier delivering strong accuracy with reasonable computing requirements for practical deployments",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Saining Xie, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 100441675,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.resnext50_32x4d",
                    "entrypoint_args": {
                        "weights": "ResNeXt50_32X4D_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "resnext"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "retinanet-resnet50-fpn-coco-torch",
            "base_filename": "retinanet_resnet50_fpn_coco-eeacb38b.pth",
            "version": null,
            "description": "Fast object detector that quickly finds and boxes eighty common items in any image",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Tsung-Yi Lin, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 136595076,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/retinanet_resnet50_fpn_coco-eeacb38b.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.detection.retinanet.retinanet_resnet50_fpn",
                    "entrypoint_args": {
                        "weights": "RetinaNet_ResNet50_FPN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.DetectorOutputProcessor",
                    "labels_path": "{{eta-resources}}/ms-coco-labels.txt",
                    "confidence_thresh": 0.3
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision>=0.8.0"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "retinanet", "resnet"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "shufflenetv2-0.5x-imagenet-torch",
            "base_filename": "shufflenetv2_x0.5-f707e7126e.pth",
            "version": null,
            "description": "Ultra-small image classifier for tiny devices with very limited power and memory",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Ningning Ma, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 5538128,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/shufflenetv2_x0.5-f707e7126e.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.shufflenetv2.shufflenet_v2_x0_5",
                    "entrypoint_args": {
                        "weights": "ShuffleNet_V2_X0_5_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "shufflenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "shufflenetv2-1.0x-imagenet-torch",
            "base_filename": "shufflenetv2_x1-5666bf0f80.pth",
            "version": null,
            "description": "Mobile image classifier that works efficiently on phones with modest computing resources",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Ningning Ma, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 9218294,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/shufflenetv2_x1-5666bf0f80.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.shufflenetv2.shufflenet_v2_x1_0",
                    "entrypoint_args": {
                        "weights": "ShuffleNet_V2_X1_0_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "shufflenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "squeezenet-1.1-imagenet-torch",
            "base_filename": "squeezenet1_1-f364aa15.pth",
            "version": "1.1",
            "description": "Tiny image classifier that fits in just five megabytes for embedded devices",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Forrest Iandola",
            "license": "BSD 2-Clause",
            "size_bytes": 4966400,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/squeezenet1_1-f364aa15.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.squeezenet.squeezenet1_1",
                    "entrypoint_args": {
                        "weights": "SqueezeNet1_1_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225]
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["classification", "imagenet", "torch", "squeezenet"],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "squeezenet-imagenet-torch",
            "base_filename": "squeezenet1_0-a815701f.pth",
            "version": "1.0",
            "description": "Ultra-compact image classifier perfect for severely resource-constrained hardware and applications",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Forrest Iandola",
            "license": "BSD 2-Clause",
            "size_bytes": 5017600,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/squeezenet1_0-a815701f.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.squeezenet.squeezenet1_0",
                    "entrypoint_args": {
                        "weights": "SqueezeNet1_0_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225]
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "squeezenet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg11-bn-imagenet-torch",
            "base_filename": "vgg11_bn-6002323d.pth",
            "version": null,
            "description": "Classic image classifier with stable training useful for various computer vision projects",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 531503671,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg11_bn-6002323d.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg11_bn",
                    "entrypoint_args": {
                        "weights": "VGG11_BN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg11-imagenet-torch",
            "base_filename": "vgg11-bbd30ac9.pth",
            "version": null,
            "description": "Simple baseline image classifier valuable for research experimentation and learning purposes",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 531456000,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg11-bbd30ac9.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg11",
                    "entrypoint_args": { "weights": "VGG11_Weights.DEFAULT" },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg13-bn-imagenet-torch",
            "base_filename": "vgg13_bn-abd245e5.pth",
            "version": null,
            "description": "Deeper classic classifier providing stable training process and solid accuracy results overall",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 532246301,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg13_bn-abd245e5.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg13_bn",
                    "entrypoint_args": {
                        "weights": "VGG13_BN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg13-imagenet-torch",
            "base_filename": "vgg13-c768596a.pth",
            "version": null,
            "description": "Straightforward image classifier valued for easy experimentation and model compression studies",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 532194478,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg13-c768596a.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg13",
                    "entrypoint_args": { "weights": "VGG13_Weights.DEFAULT" },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg16-bn-imagenet-torch",
            "base_filename": "vgg16_bn-6c64b313.pth",
            "version": null,
            "description": "Popular feature extractor widely used for detection, style transfer, and medical imaging",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 553507836,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg16_bn-6c64b313.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg16_bn",
                    "entrypoint_args": {
                        "weights": "VGG16_BN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg16-imagenet-torch",
            "base_filename": "vgg16-397923af.pth",
            "version": null,
            "description": "PyTorch version of the popular classifier ready for modern deep learning workflows",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 553433881,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg16-397923af.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg16",
                    "entrypoint_args": { "weights": "VGG16_Weights.DEFAULT" },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg19-bn-imagenet-torch",
            "base_filename": "vgg19_bn-c79401a0.pth",
            "version": null,
            "description": "Deep classic model providing rich features for style transfer and interpretability analysis",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 574769405,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg19_bn-c79401a0.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg19_bn",
                    "entrypoint_args": {
                        "weights": "VGG19_BN_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "vgg19-imagenet-torch",
            "base_filename": "vgg19-dcbb9e9d.pth",
            "version": null,
            "description": "Deep image classifier delivering detailed features for creative applications and research projects",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Karen Simonyan, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 574673361,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/vgg19-dcbb9e9d.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.vgg.vgg19",
                    "entrypoint_args": { "weights": "VGG19_Weights.DEFAULT" },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<classifier.6"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "vgg",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "wide-resnet101-2-imagenet-torch",
            "base_filename": "wide_resnet101_2-32ee1156.pth",
            "version": null,
            "description": "Extra-wide deep classifier for high-precision image recognition and advanced transfer learning tasks",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Sergey Zagoruyko, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 254695146,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.wide_resnet101_2",
                    "entrypoint_args": {
                        "weights": "Wide_ResNet101_2_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "wide-resnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "wide-resnet50-2-imagenet-torch",
            "base_filename": "wide_resnet50_2-95faca4d.pth",
            "version": null,
            "description": "Wide classifier offering stronger accuracy and better features for adapting to new tasks",
            "source": "https://pytorch.org/vision/main/models.html",
            "author": "Sergey Zagoruyko, et al.",
            "license": "BSD 3-Clause",
            "size_bytes": 138223492,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.zoo.models.torch.TorchvisionImageModel",
                "config": {
                    "entrypoint_fcn": "torchvision.models.resnet.wide_resnet50_2",
                    "entrypoint_args": {
                        "weights": "Wide_ResNet50_2_Weights.DEFAULT"
                    },
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "labels_path": "{{eta-resources}}/imagenet-labels-no-background.txt",
                    "image_min_dim": 224,
                    "image_max_dim": 2048,
                    "image_mean": [0.485, 0.456, 0.406],
                    "image_std": [0.229, 0.224, 0.225],
                    "embeddings_layer": "<fc"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "embeddings",
                "logits",
                "imagenet",
                "torch",
                "wide-resnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2020-12-11 13:45:51"
        },
        {
            "base_name": "open-clip-torch",
            "base_filename": null,
            "version": null,
            "description": "Connects images with text descriptions enabling search by words and automatic content filtering systems",
            "source": "https://github.com/mlfoundations/open_clip",
            "author": "Gabriel Ilharco, et al.",
            "license": "MIT",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.open_clip.TorchOpenClipModel",
                "config": {
                    "entrypoint_fcn": "",
                    "labels_path": "{{eta-resources}}/voc-labels.txt",
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "image_size": [224, 224],
                    "image_mean": [0.48145466, 0.4578275, 0.40821073],
                    "image_std": [0.26862954, 0.26130258, 0.27577711],
                    "embeddings_layer": "visual",
                    "tokenizer_base_filename": "clip_bpe_simple_vocab_16e6.txt.gz",
                    "tokenizer_base_url": "https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz",
                    "text_prompt": "A photo of"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "open_clip_torch"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "logits",
                "embeddings",
                "torch",
                "clip",
                "zero-shot",
                "transformer"
            ],
            "training_data": [
                { "name": "LAION-2B", "url": "https://laion.ai/blog/laion-5b/" }
            ],
            "date_added": "2023-12-13 14:25:51"
        },
        {
            "base_name": "classification-transformer-torch",
            "base_filename": null,
            "version": null,
            "description": "Vision transformer for image classification and custom fine-tuning on specialized datasets",
            "source": "https://huggingface.co/docs/transformers/tasks/image_classification",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {}
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "official"
            ],
            "training_data": [],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "vit-base-patch16-224-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Modern image classifier that recognizes objects and provides useful features for various computer vision tasks",
            "source": "https://huggingface.co/docs/transformers/tasks/image_classification",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": 346351599,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {}
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "detection-transformer-torch",
            "base_filename": null,
            "version": null,
            "description": "Modern object detector that finds items in images without needing complex post-processing steps",
            "source": "https://huggingface.co/docs/transformers/tasks/object_detection",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {}
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "official"
            ],
            "training_data": [],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "segmentation-transformer-torch",
            "base_filename": null,
            "version": null,
            "description": "Hugging Face Transformers model for semantic segmentation",
            "source": "https://huggingface.co/docs/transformers/tasks/semantic_segmentation",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForSemanticSegmentation",
                "config": {
                    "output_processor_args": {
                        "no_background_cls": true,
                        "has_softmax_out": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segmentation", "torch", "transformers", "official"],
            "training_data": [],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "depth-estimation-transformer-torch",
            "base_filename": null,
            "version": null,
            "description": "Hugging Face Transformers model for monocular depth estimation",
            "source": "https://huggingface.co/docs/transformers/tasks/monocular_depth_estimation",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForDepthEstimation",
                "config": {}
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["depth", "torch", "transformers"],
            "training_data": [],
            "date_added": "2024-02-06 14:25:51"
        },
        {
            "base_name": "zero-shot-classification-transformer-torch",
            "base_filename": null,
            "version": null,
            "description": "Finds any object you name in images without requiring training on those specific items",
            "source": "https://huggingface.co/docs/transformers/tasks/zero_shot_image_classification",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForImageClassification",
                "config": {
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "transforms_args": {
                        "use_fast": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "siglip-base-patch16-224-torch",
            "base_filename": null,
            "version": null,
            "description": "Hugging Face Transformers model for zero-shot image classification",
            "source": "https://huggingface.co/docs/transformers/tasks/zero_shot_image_classification",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": 812762989,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/siglip2-base-patch16-224",
                    "transformers_processor_kwargs": {
                        "padding": "max_length",
                        "max_length": 64
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers>=4.51"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [
                { "name": "WebLI", "url": "https://arxiv.org/abs/2209.06794" }
            ],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "zero-shot-detection-transformer-torch",
            "base_filename": null,
            "version": null,
            "description": "Hugging Face Transformers model for zero-shot object detection",
            "source": "https://huggingface.co/docs/transformers/tasks/zero_shot_object_detection",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForObjectDetection",
                "config": {}
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "group-vit-segmentation-transformer-torch",
            "base_filename": null,
            "version": null,
            "description": "Hugging Face Transformers model for zero-shot semantic segmentation",
            "source": "https://huggingface.co/docs/transformers/en/tasks/mask_generation",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": 223137427,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForSemanticSegmentation",
                "config": {
                    "name_or_path": "nvidia/groupvit-gcc-yfcc",
                    "entrypoint_fcn": "transformers.GroupViTModel.from_pretrained",
                    "entrypoint_args": {
                        "output_segmentation": true
                    },
                    "output_processor_args": {
                        "logits_key": "segmentation_logits",
                        "no_background_cls": true,
                        "has_softmax_out": false
                    },
                    "transforms_args": {
                        "use_fast": false
                    },
                    "transformers_processor_kwargs": {
                        "padding": true,
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "segmentation",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [
                {
                    "name": "YFCC-100M",
                    "url": "https://multimediacommons.wordpress.com/yfcc100m-core-dataset/"
                },
                {
                    "name": "Conceptual Captions 12M",
                    "url": "https://github.com/google-research-datasets/conceptual-12m"
                }
            ],
            "date_added": "2025-05-15 15:20:35"
        },
        {
            "base_name": "owlvit-base-patch16-torch",
            "base_filename": null,
            "version": null,
            "description": "Finds any object you name in pictures using 16x16 image patches without needing specific training for those items",
            "source": "https://huggingface.co/docs/transformers/tasks/zero_shot_object_detection",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForObjectDetection",
                "config": {
                    "name_or_path": "google/owlvit-base-patch16"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [
                { "name": "WebLI", "url": "https://arxiv.org/abs/2209.06794" }
            ],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "owlvit-base-patch32-torch",
            "base_filename": null,
            "version": null,
            "description": "Finds any object you name in pictures using efficient 32x32 image patches without needing specific training",
            "source": "https://huggingface.co/docs/transformers/tasks/zero_shot_object_detection",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": 1229149172,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForObjectDetection",
                "config": {
                    "name_or_path": "google/owlvit-base-patch32"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [
                { "name": "WebLI", "url": "https://arxiv.org/abs/2209.06794" }
            ],
            "date_added": "2025-07-15 13:49:00"
        },
        {
            "base_name": "owlvit-large-patch14-torch",
            "base_filename": null,
            "version": null,
            "description": "Large OWL-ViT zero-shot object detector with ViT-L/14 backbone. Achieves higher accuracy than base models, especially for smaller objects.",
            "source": "https://huggingface.co/google/owlvit-large-patch14",
            "author": "Matthias Minderer, et al.",
            "license": "Apache 2.0",
            "size_bytes": 1735754597,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForObjectDetection",
                "config": {
                    "name_or_path": "google/owlvit-large-patch14"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [
                { "name": "WebLI", "url": "https://arxiv.org/abs/2209.06794" }
            ],
            "date_added": "2025-07-28 15:25:00"
        },
        {
            "base_name": "omdet-turbo-swin-tiny-torch",
            "base_filename": null,
            "version": null,
            "description": "Real-time detector that finds any object you describe in words, perfect for live video analysis",
            "source": "https://huggingface.co/docs/transformers/tasks/zero_shot_object_detection",
            "author": "Thomas Wolf, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneZeroShotTransformerForObjectDetection",
                "config": {
                    "name_or_path": "omlab/omdet-turbo-swin-tiny-hf"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers>=4.51"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "logits",
                "embeddings",
                "torch",
                "transformers",
                "zero-shot",
                "official"
            ],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "OpenImages V6",
                    "url": "https://storage.googleapis.com/openimages/web/index.html"
                },
                {
                    "name": "MS COCO 2017",
                    "url": "https://cocodataset.org"
                }
            ],
            "date_added": "2024-01-17 14:25:51"
        },
        {
            "base_name": "clip-vit-base32-torch",
            "base_filename": "CLIP-ViT-B-32.pt",
            "version": null,
            "description": "Understands both images and text together, enabling search and classification using natural language descriptions",
            "source": "https://github.com/openai/CLIP",
            "author": "Alec Radford, et al.",
            "license": "MIT",
            "size_bytes": 353976522,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.clip.TorchCLIPModel",
                "config": {
                    "entrypoint_fcn": "",
                    "labels_path": "{{eta-resources}}/voc-labels.txt",
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "image_size": [224, 224],
                    "image_mean": [0.48145466, 0.4578275, 0.40821073],
                    "image_std": [0.26862954, 0.26130258, 0.27577711],
                    "embeddings_layer": "visual",
                    "tokenizer_base_filename": "clip_bpe_simple_vocab_16e6.txt.gz",
                    "tokenizer_base_url": "https://github.com/openai/CLIP/raw/main/clip/bpe_simple_vocab_16e6.txt.gz",
                    "context_length": 77,
                    "text_prompt": "A photo of"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "logits",
                "embeddings",
                "torch",
                "clip",
                "zero-shot",
                "transformer",
                "official"
            ],
            "training_data": [
                { "name": "WIT-400M", "url": "https://github.com/openai/CLIP" }
            ],
            "date_added": "2022-04-12 17:49:51"
        },
        {
            "base_name": "dinov2-vits14-torch",
            "description": "Small model enabling image search and similarity matching directly on mobile devices",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 88283115,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vits14"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-07-31 10:17:51"
        },
        {
            "base_name": "dinov2-vitb14-torch",
            "description": "Creates searchable image fingerprints for finding similar pictures and organizing large photo collections",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 346378731,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vitb14"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-07-31 10:17:51"
        },
        {
            "base_name": "dinov2-vitl14-torch",
            "description": "Large model that creates detailed image fingerprints for advanced search and automatic grouping",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 1217586395,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vitl14"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-07-31 10:17:51"
        },
        {
            "base_name": "dinov2-vitg14-torch",
            "description": "Powerful image search engine that handles enormous photo collections with rich detail extraction",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 4546108579,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vitg14"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-07-31 10:17:51"
        },
        {
            "base_name": "dinov2-vits14-reg-torch",
            "description": "Compact stable model for image search that runs efficiently on phones and edge devices",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 88291785,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vits14_reg"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-12-02 16:42:00"
        },
        {
            "base_name": "dinov2-vitb14-reg-torch",
            "description": "Enhanced image search model that resists noise and errors for more reliable similarity matching",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 346393545,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vitb14_reg"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-12-02 16:42:00"
        },
        {
            "base_name": "dinov2-vitl14-reg-torch",
            "description": "Large stable model for finding and grouping similar images across big databases reliably",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 1217607321,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vitl14_reg"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-12-02 16:42:00"
        },
        {
            "base_name": "dinov2-vitg14-reg-torch",
            "description": "Highest-capacity dinov2 search model with maximum stability for finding images across massive diverse datasets",
            "source": "https://github.com/facebookresearch/dinov2",
            "author": "Maxime Oquab, et al.",
            "license": "Apache 2.0",
            "size_bytes": 4546140349,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.torch.TorchImageModel",
                "config": {
                    "entrypoint_fcn": "fiftyone.utils.torch.load_torch_hub_raw_model",
                    "entrypoint_args": {
                        "repo_or_dir": "facebookresearch/dinov2",
                        "model": "dinov2_vitg14_reg"
                    },
                    "image_patch_size": 14,
                    "image_min_dim": 14,
                    "embeddings_layer": "head"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "embeddings",
                "torch",
                "dinov2",
                "transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "LVD-142M",
                    "url": "https://arxiv.org/abs/2304.07193"
                }
            ],
            "date_added": "2023-12-02 16:42:00"
        },
        {
            "base_name": "yolov5n-coco-torch",
            "base_filename": "yolov5n-coco.pt",
            "description": "Lightweight detector for edge devices needing basic object detection capabilities",
            "source": "https://pytorch.org/hub/ultralytics_yolov5",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 5562759,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5nu.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov5nu.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-05-22 14:14:45"
        },
        {
            "base_name": "yolov5s-coco-torch",
            "base_filename": "yolov5s-coco.pt",
            "description": "Real-time detector delivering good results with minimal computational requirements",
            "source": "https://pytorch.org/hub/ultralytics_yolov5",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 18581255,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5su.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov5su.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-05-22 14:14:45"
        },
        {
            "base_name": "yolov5m-coco-torch",
            "base_filename": "yolov5m-coco.pt",
            "description": "Real-time detector balancing good accuracy with fast processing speeds",
            "source": "https://pytorch.org/hub/ultralytics_yolov5",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 50589507,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5mu.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov5mu.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-05-22 14:14:45"
        },
        {
            "base_name": "yolov5l-coco-torch",
            "base_filename": "yolov5l-coco.pt",
            "description": "Real-time detector producing accurate results quickly for demanding vision applications",
            "source": "https://pytorch.org/hub/ultralytics_yolov5",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 106913919,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5lu.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov5lu.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-05-22 14:14:45"
        },
        {
            "base_name": "yolov8n-coco-torch",
            "base_filename": "yolov8n-coco.pt",
            "description": "Edge-optimized detector recognizing common objects on resource-limited devices effectively",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 6534387,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8n.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8s-coco-torch",
            "base_filename": "yolov8s-coco.pt",
            "description": "Detector offering fast performance on mid-range graphics cards and processors",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 22573363,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8s.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8m-coco-torch",
            "base_filename": "yolov8m-coco.pt",
            "description": "Detector balancing speed and accuracy for everyday object detection needs",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 52117635,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8m.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8l-coco-torch",
            "base_filename": "yolov8l-coco.pt",
            "description": "Real-time detector with advanced architecture for improved object finding in complex scenes",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 87769683,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8l.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8l.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8x-coco-torch",
            "base_filename": "yolov8x-coco.pt",
            "description": "High-accuracy detector for critical applications where precision matters most",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 136867539,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8x.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8x.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8n-seg-coco-torch",
            "base_filename": "yolov8n-seg-coco.pt",
            "description": "Edge-optimized model producing object outlines on devices with limited resources.",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 7054355,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8n-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-04-02 19:22:51"
        },
        {
            "base_name": "yolov8s-seg-coco-torch",
            "base_filename": "yolov8s-seg-coco.pt",
            "description": "Fast model creating object masks for real-time image segmentation needs",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 23897299,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8s-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-04-02 19:22:51"
        },
        {
            "base_name": "yolov8m-seg-coco-torch",
            "base_filename": "yolov8m-seg-coco.pt",
            "description": "Generates object masks with good balance of speed and quality",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 54899779,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8m-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-04-02 19:22:51"
        },
        {
            "base_name": "yolov8l-seg-coco-torch",
            "base_filename": "yolov8l-seg-coco.pt",
            "description": "Creates precise object outlines for detailed image editing and analysis tasks",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 92391859,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8l-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8l-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-04-02 19:22:51"
        },
        {
            "base_name": "yolov8x-seg-coco-torch",
            "base_filename": "yolov8x-seg-coco.pt",
            "description": "High-accuracy model generating detailed object outlines for demanding professional applications",
            "source": "https://docs.ultralytics.com/models/yolov8/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 144076467,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8x-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8x-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-04-02 19:22:51"
        },
        {
            "base_name": "yolov9c-coco-torch",
            "base_filename": "yolov9c-coco.pt",
            "description": "Detector enhanced with transformer technology for improved object finding capabilities",
            "source": "https://docs.ultralytics.com/models/yolov9/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 51802599,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov9c.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov9c.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov9e-coco-torch",
            "base_filename": "yolov9e-coco.pt",
            "description": "Advanced detector with transformer backbone delivering superior accuracy for complex scenes",
            "source": "https://docs.ultralytics.com/models/yolov9/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 117530476,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov9e.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov9e.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov9c-seg-coco-torch",
            "base_filename": "yolov9c-seg-coco.pt",
            "description": "Compact model producing both masks and boxes with transformer-enhanced accuracy",
            "source": "https://docs.ultralytics.com/models/yolov9/#__tabbed_1_2",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 56474024,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov9c-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov9c-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.42"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-04-02 19:22:51"
        },
        {
            "base_name": "yolov9e-seg-coco-torch",
            "base_filename": "yolov9e-seg-coco.pt",
            "description": "Advanced model creating precise object outlines using enhanced transformer architecture",
            "source": "https://docs.ultralytics.com/models/yolov9/#__tabbed_1_2",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 122214886,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov9e-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov9e-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.42"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-04-02 19:22:51"
        },
        {
            "base_name": "yolov10n-coco-torch",
            "base_filename": "yolov10n-coco.pt",
            "description": "Edge-optimized detector for devices with minimal computing resources available",
            "source": "https://docs.ultralytics.com/models/yolov10/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 5860383,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov10n.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov10n.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.2.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-07-01 19:22:51"
        },
        {
            "base_name": "yolov10s-coco-torch",
            "base_filename": "yolov10s-coco.pt",
            "description": "Fast lightweight detector suitable for systems with limited GPU capabilities and memory",
            "source": "https://docs.ultralytics.com/models/yolov10/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 16623111,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov10s.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov10s.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.2.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-07-01 19:22:51"
        },
        {
            "base_name": "yolov10m-coco-torch",
            "base_filename": "yolov10m-coco.pt",
            "description": "Balanced detector providing good accuracy and speed for general-purpose object detection tasks",
            "source": "https://docs.ultralytics.com/models/yolov10/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 33643667,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov10m.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov10m.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.2.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-07-01 19:22:51"
        },
        {
            "base_name": "yolov10l-coco-torch",
            "base_filename": "yolov10l-coco.pt",
            "description": "Object detector with special optimizations for even faster inference on modern hardware",
            "source": "https://docs.ultralytics.com/models/yolov10/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 52425230,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov10l.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov10l.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.2.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-07-01 19:22:51"
        },
        {
            "base_name": "yolov10x-coco-torch",
            "base_filename": "yolov10x-coco.pt",
            "description": "High-accuracy detector for demanding object detection applications and research",
            "source": "https://docs.ultralytics.com/models/yolov10/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 64395854,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov10x.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov10x.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.2.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-07-01 19:22:51"
        },
        {
            "base_name": "yolo11n-coco-torch",
            "base_filename": "yolo11n-coco.pt",
            "description": "Object detector designed specifically for phones and other edge computing devices",
            "source": "https://docs.ultralytics.com/models/yolov11/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 5613764,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11n.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11s-coco-torch",
            "base_filename": "yolo11s-coco.pt",
            "description": "Fast object detector ideal for systems with limited graphics processing power",
            "source": "https://docs.ultralytics.com/models/yolov11/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 19313732,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11s.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11m-coco-torch",
            "base_filename": "yolo11m-coco.pt",
            "description": "Object detector offering good balance between speed and accuracy for most applications",
            "source": "https://docs.ultralytics.com/models/yolov11/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 40684120,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11m.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11l-coco-torch",
            "base_filename": "yolo11l-coco.pt",
            "description": "Real-time object detector balancing high accuracy with fast processing speeds effectively",
            "source": "https://docs.ultralytics.com/models/yolov11/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 51387343,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11l.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11x-coco-torch",
            "base_filename": "yolo11x-coco.pt",
            "description": "Object detector prioritizing accuracy over processing speed for critical applications",
            "source": "https://docs.ultralytics.com/models/yolov11/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 114636239,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11x.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11n-seg-coco-torch",
            "base_filename": "yolo11n-seg-coco.pt",
            "description": "Edge model producing object outlines directly on phones and edge devices",
            "source": "https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 6182636,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11n-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11s-seg-coco-torch",
            "base_filename": "yolo11s-seg-coco.pt",
            "description": "Model creating object masks quickly for real-time segmentation applications",
            "source": "https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 20669228,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11s-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11m-seg-coco-torch",
            "base_filename": "yolo11m-seg-coco.pt",
            "description": "Model generating object masks efficiently for everyday segmentation tasks",
            "source": "https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 45400152,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11m-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11l-seg-coco-torch",
            "base_filename": "yolo11l-seg-coco.pt",
            "description": "Model creating detailed object outlines for precise image editing and analysis",
            "source": "https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 56096965,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11l-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yolo11x-seg-coco-torch",
            "base_filename": "yolo11x-seg-coco.pt",
            "description": "Model delivering high-quality object outlines for professional workflows",
            "source": "https://docs.ultralytics.com/models/yolo11/#__tabbed_1_2",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 125090821,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolo11x-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-10-05 19:22:51"
        },
        {
            "base_name": "yoloe11s-seg-torch",
            "base_filename": "yoloe-11s-seg.pt",
            "description": "Segments specified classes, generating object outlines and boxes for real-time applications",
            "source": "https://docs.ultralytics.com/models/yoloe",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 27803986,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11s-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yoloe-11s-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.99"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                },
                {
                    "name": "CC3M-Lite",
                    "url": null
                }
            ],
            "date_added": "2025-04-10 12:24:41"
        },
        {
            "base_name": "yoloe11m-seg-torch",
            "base_filename": "yoloe-11m-seg.pt",
            "description": "Model producing masks and boxes for objects described in natural language",
            "source": "https://docs.ultralytics.com/models/yoloe",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 60276714,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11m-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yoloe-11m-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.99"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                },
                {
                    "name": "CC3M-Lite",
                    "url": null
                }
            ],
            "date_added": "2025-04-10 12:24:41"
        },
        {
            "base_name": "yoloe11l-seg-torch",
            "base_filename": "yoloe-11l-seg.pt",
            "description": "Real-time model creating both object outlines and boxes for any described item",
            "source": "https://docs.ultralytics.com/models/yoloe",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 70982416,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-11l-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yoloe-11l-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.99"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                },
                {
                    "name": "CC3M-Lite",
                    "url": null
                }
            ],
            "date_added": "2025-04-10 12:24:41"
        },
        {
            "base_name": "yoloev8s-seg-torch",
            "base_filename": "yoloe-v8s-seg.pt",
            "description": "Compact model producing outlines for objects described in words on edge devices",
            "source": "https://docs.ultralytics.com/models/yoloe",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 31135890,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8s-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yoloe-v8s-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.99"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                },
                {
                    "name": "CC3M-Lite",
                    "url": null
                }
            ],
            "date_added": "2025-04-10 12:24:41"
        },
        {
            "base_name": "yoloev8m-seg-torch",
            "base_filename": "yoloe-v8m-seg.pt",
            "description": "Model creating masks for any object type you name in text",
            "source": "https://docs.ultralytics.com/models/yoloe",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 65800690,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8m-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yoloe-v8m-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.99"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                },
                {
                    "name": "CC3M-Lite",
                    "url": null
                }
            ],
            "date_added": "2025-04-10 12:24:41"
        },
        {
            "base_name": "yoloev8l-seg-torch",
            "base_filename": "yoloe-v8l-seg.pt",
            "description": "Model outlining and boxing any object you describe without specific training",
            "source": "https://docs.ultralytics.com/models/yoloe",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 107405586,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yoloe-v8l-seg.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yoloe-v8l-seg.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsSegmentationOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.3.99"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["instances", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                },
                {
                    "name": "CC3M-Lite",
                    "url": null
                }
            ],
            "date_added": "2025-04-10 12:24:41"
        },
        {
            "base_name": "rtdetr-l-coco-torch",
            "base_filename": "rtdetr-l-coco.pt",
            "description": "Modern real-time object detector that finds items without complex post-processing for responsive applications",
            "source": "https://docs.ultralytics.com/models/rtdetr/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 66511432,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/rtdetr-l.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneRTDETRModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.RTDETR",
                    "entrypoint_args": {
                        "model": "rtdetr-l.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.2.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformer",
                "rtdetr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-07-01 19:22:51"
        },
        {
            "base_name": "rtdetr-x-coco-torch",
            "base_filename": "rtdetr-x-coco.pt",
            "description": "High-capacity object detector delivering very precise results at speeds suitable for production use",
            "source": "https://docs.ultralytics.com/models/rtdetr/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 135755662,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/rtdetr-x.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneRTDETRModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.RTDETR",
                    "entrypoint_args": {
                        "model": "rtdetr-x.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.2.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformer",
                "rtdetr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2024-07-01 19:22:51"
        },
        {
            "base_name": "yolov8s-world-torch",
            "base_filename": "yolov8s-world.pt",
            "description": "Lightweight detector finding objects based on text descriptions for edge applications",
            "source": "https://docs.ultralytics.com/models/yolo-world/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 27166882,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s-world.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8s-world.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8m-world-torch",
            "base_filename": "yolov8m-world.pt",
            "description": "Detector understanding text descriptions to find matching objects in images",
            "source": "https://docs.ultralytics.com/models/yolo-world/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 58607810,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m-world.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8m-world.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8l-world-torch",
            "base_filename": "yolov8l-world.pt",
            "description": "Finds and boxes any object you describe using natural language prompts",
            "source": "https://docs.ultralytics.com/models/yolo-world/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 95664610,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8l-world.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8l-world.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8x-world-torch",
            "base_filename": "yolov8x-world.pt",
            "description": "Open-vocabulary detector with high accuracy for text-based object finding",
            "source": "https://docs.ultralytics.com/models/yolo-world/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 147959522,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8x-world.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8x-world.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "torch", "yolo", "zero-shot", "official"],
            "training_data": [
                {
                    "name": "Objects365v1",
                    "url": "https://www.objects365.org"
                },
                {
                    "name": "GQA",
                    "url": "https://cs.stanford.edu/people/dorarad/gqa/"
                },
                {
                    "name": "Flickr30k",
                    "url": "https://shannon.cs.illinois.edu/DenotationGraph/"
                }
            ],
            "date_added": "2024-03-11 19:22:51"
        },
        {
            "base_name": "yolov8n-obb-dotav1-torch",
            "base_filename": "yolov8n-obb.pt",
            "description": "Lightweight detector for finding rotated objects in aerial imagery on edge hardware",
            "source": "https://docs.ultralytics.com/tasks/obb/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 6548034,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n-obb.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8n-obb.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsOBBOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "torch",
                "yolo",
                "polylines",
                "obb",
                "official"
            ],
            "training_data": [
                {
                    "name": "DOTA-v1.0",
                    "url": "https://captain-whu.github.io/DOTA/"
                }
            ],
            "date_added": "2024-04-05 19:22:51"
        },
        {
            "base_name": "yolov8s-obb-dotav1-torch",
            "base_filename": "yolov8s-obb.pt",
            "description": "Efficiently finds rotated objects in aerial photos for mapping and analysis tasks",
            "source": "https://docs.ultralytics.com/tasks/obb/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 23245186,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8s-obb.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8s-obb.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsOBBOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "torch",
                "yolo",
                "polylines",
                "obb",
                "official"
            ],
            "training_data": [
                {
                    "name": "DOTA-v1.0",
                    "url": "https://captain-whu.github.io/DOTA/"
                }
            ],
            "date_added": "2024-04-05 19:22:51"
        },
        {
            "base_name": "yolov8m-obb-dotav1-torch",
            "base_filename": "yolov8m-obb.pt",
            "description": "Finds rotated bounding boxes in aerial images for mapping and surveillance applications",
            "source": "https://docs.ultralytics.com/tasks/obb/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 53304682,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8m-obb.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8m-obb.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsOBBOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "torch",
                "yolo",
                "polylines",
                "obb",
                "official"
            ],
            "training_data": [
                {
                    "name": "DOTA-v1.0",
                    "url": "https://captain-whu.github.io/DOTA/"
                }
            ],
            "date_added": "2024-04-05 19:22:51"
        },
        {
            "base_name": "yolov8l-obb-dotav1-torch",
            "base_filename": "yolov8l-obb.pt",
            "description": "Specialized detector that finds rotated objects in aerial and satellite imagery accurately",
            "source": "https://docs.ultralytics.com/tasks/obb/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 89504274,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8l-obb.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8l-obb.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsOBBOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "torch",
                "yolo",
                "polylines",
                "obb",
                "official"
            ],
            "training_data": [
                {
                    "name": "DOTA-v1.0",
                    "url": "https://captain-whu.github.io/DOTA/"
                }
            ],
            "date_added": "2024-04-05 19:22:51"
        },
        {
            "base_name": "yolov8x-obb-dotav1-torch",
            "base_filename": "yolov8x-obb.pt",
            "description": "High-precision detector for rotated objects in aerial and satellite imagery analysis",
            "source": "https://docs.ultralytics.com/tasks/obb/",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 139533138,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8x-obb.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8x-obb.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsOBBOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics>=8.1.0"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "torch",
                "yolo",
                "polylines",
                "obb",
                "official"
            ],
            "training_data": [
                {
                    "name": "DOTA-v1.0",
                    "url": "https://captain-whu.github.io/DOTA/"
                }
            ],
            "date_added": "2024-04-05 19:22:51"
        },
        {
            "base_name": "yolov8n-oiv7-torch",
            "base_filename": "yolov8n-oiv7.pt",
            "description": "Edge-friendly detector recognizing hundreds of object categories on resource-limited devices effectively",
            "source": "https://docs.ultralytics.com/datasets/detect/open-images-v7",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 7226939,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8n-oiv7.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8n-oiv7.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "oiv7", "torch", "yolo", "official"],
            "training_data": [
                {
                    "name": "Open Images V7",
                    "url": "https://storage.googleapis.com/openimages/web/index.html"
                }
            ],
            "date_added": "2024-05-20 19:22:51"
        },
        {
            "base_name": "yolov8s-oiv7-torch",
            "base_filename": "yolov8s-oiv7.pt",
            "description": "Compact detector recognizing diverse object types across many different image categories",
            "source": "https://docs.ultralytics.com/datasets/detect/open-images-v7",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 22986811,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8s-oiv7.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8s-oiv7.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "oiv7", "torch", "yolo", "official"],
            "training_data": [
                {
                    "name": "Open Images V7",
                    "url": "https://storage.googleapis.com/openimages/web/index.html"
                }
            ],
            "date_added": "2024-05-20 19:22:51"
        },
        {
            "base_name": "yolov8m-oiv7-torch",
            "base_filename": "yolov8m-oiv7.pt",
            "description": "Versatile detector recognizing hundreds of different object types across varied image domains",
            "source": "https://docs.ultralytics.com/datasets/detect/open-images-v7",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 52732835,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8m-oiv7.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8m-oiv7.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "oiv7", "torch", "yolo", "official"],
            "training_data": [
                {
                    "name": "Open Images V7",
                    "url": "https://storage.googleapis.com/openimages/web/index.html"
                }
            ],
            "date_added": "2024-05-20 19:22:51"
        },
        {
            "base_name": "yolov8l-oiv7-torch",
            "base_filename": "yolov8l-oiv7.pt",
            "description": "General-purpose detector trained on diverse images recognizing over six hundred object categories",
            "source": "https://docs.ultralytics.com/datasets/detect/open-images-v7",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 87769683,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8l-oiv7.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8l-oiv7.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "oiv7", "torch", "yolo", "official"],
            "training_data": [
                {
                    "name": "Open Images V7",
                    "url": "https://storage.googleapis.com/openimages/web/index.html"
                }
            ],
            "date_added": "2024-05-20 19:22:51"
        },
        {
            "base_name": "yolov8x-oiv7-torch",
            "base_filename": "yolov8x-oiv7.pt",
            "description": "Accurate general detector recognizing over six hundred different object types",
            "source": "https://docs.ultralytics.com/datasets/detect/open-images-v7",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 136867539,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.2.0/yolov8x-oiv7.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov8x-oiv7.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "oiv7", "torch", "yolo", "official"],
            "training_data": [
                {
                    "name": "Open Images V7",
                    "url": "https://storage.googleapis.com/openimages/web/index.html"
                }
            ],
            "date_added": "2024-05-20 19:22:51"
        },
        {
            "base_name": "yolo-nas-torch",
            "base_filename": null,
            "version": null,
            "description": "AI-designed detector family offering three model variants for diverse deployment scenarios",
            "source": "https://github.com/Deci-AI/super-gradients",
            "author": "Shay Aharon, et al.",
            "license": "Apache 2.0",
            "size_bytes": null,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.super_gradients.TorchYoloNasModel",
                "config": {
                    "labels_path": "{{eta-resources}}/ms-coco-labels.txt",
                    "output_processor_cls": "fiftyone.utils.torch.ClassifierOutputProcessor",
                    "raw_inputs": true
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "super-gradients"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "torch", "yolo", "official"],
            "training_data": [
                {
                    "name": "MS COCO 2017",
                    "url": "https://cocodataset.org"
                },
                {
                    "name": "Objects365",
                    "url": "https://www.objects365.org"
                }
            ],
            "date_added": "2024-01-06 08:51:14"
        },
        {
            "base_name": "yolov5x-coco-torch",
            "base_filename": "yolov5x-coco.pt",
            "description": "High-accuracy detector offering top precision for applications where quality is critical",
            "source": "https://pytorch.org/hub/ultralytics_yolov5",
            "author": "Glenn Jocher, et al.",
            "license": "AGPL-3.0",
            "size_bytes": 195132283,
            "manager": {
                "type": "fiftyone.core.models.ModelManager",
                "config": {
                    "url": "https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov5xu.pt"
                }
            },
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.ultralytics.FiftyOneYOLOModel",
                "config": {
                    "entrypoint_fcn": "ultralytics.YOLO",
                    "entrypoint_args": {
                        "model": "yolov5xu.pt"
                    },
                    "output_processor_cls": "fiftyone.utils.ultralytics.UltralyticsDetectionOutputProcessor"
                }
            },
            "requirements": {
                "packages": [
                    "torch>=1.7.0",
                    "torchvision>=0.8.1",
                    "ultralytics"
                ],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "yolo", "official"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-05-22 14:14:45"
        },
        {
            "base_name": "convnext-tiny-224-torch",
            "base_filename": null,
            "version": null,
            "description": "Tiny modern CNN bridging traditional convolutions with transformer-inspired improvements",
            "source": "https://huggingface.co/facebook/convnext-tiny-224",
            "author": "Zhuang Liu, et al.",
            "license": "Apache 2.0",
            "size_bytes": 457742046,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "facebook/convnext-tiny-224",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "convnext"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "convnext-small-224-torch",
            "base_filename": null,
            "version": null,
            "description": "Small modernized CNN delivering strong accuracy through architectural innovations",
            "source": "https://huggingface.co/facebook/convnext-small-224",
            "author": "Zhuang Liu, et al.",
            "license": "Apache 2.0",
            "size_bytes": 402011374,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "facebook/convnext-small-224",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "convnext",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "convnext-base-224-torch",
            "base_filename": null,
            "version": null,
            "description": "Base modern CNN with transformer elements for robust visual understanding",
            "source": "https://huggingface.co/facebook/convnext-base-224",
            "author": "Zhuang Liu, et al.",
            "license": "Apache 2.0",
            "size_bytes": 1417939660,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "facebook/convnext-base-224",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "convnext",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "convnext-large-224-torch",
            "base_filename": null,
            "version": null,
            "description": "Large modern CNN demonstrating competitive performance with vision transformers",
            "source": "https://huggingface.co/facebook/convnext-large-224",
            "author": "Zhuang Liu, et al.",
            "license": "Apache 2.0",
            "size_bytes": 3164754044,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "facebook/convnext-large-224",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "convnext"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "convnext-xlarge-224-torch",
            "base_filename": null,
            "version": null,
            "description": "Extra-large modern CNN maximizing architectural improvements for top accuracy",
            "source": "https://huggingface.co/facebook/convnext-xlarge-224-22k-1k",
            "author": "Zhuang Liu, et al.",
            "license": "Apache 2.0",
            "size_bytes": 1400917137,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "facebook/convnext-xlarge-224-22k-1k",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "convnext",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-22K",
                    "url": "https://www.image-net.org/download.php"
                },
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b0-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Efficient image classifier optimized for mobile devices with excellent accuracy-efficiency tradeoff",
            "source": "https://huggingface.co/google/efficientnet-b0",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 85768446,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b0",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b1-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Scaled efficient classifier with improved accuracy for slightly larger computational budgets",
            "source": "https://huggingface.co/google/efficientnet-b1",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 126318898,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b1",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b2-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Balanced efficient model providing stronger performance while maintaining reasonable resource usage",
            "source": "https://huggingface.co/google/efficientnet-b2",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 147458738,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b2",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b3-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Mid-scale efficient classifier delivering high accuracy for versatile deployment scenarios",
            "source": "https://huggingface.co/google/efficientnet-b3",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 197807644,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b3",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b4-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Large efficient model with enhanced features for transfer learning applications",
            "source": "https://huggingface.co/google/efficientnet-b4",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 312276324,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b4",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b5-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "High-capacity efficient classifier prioritizing accuracy with available compute resources",
            "source": "https://huggingface.co/google/efficientnet-b5",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 489948742,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b5",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b6-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Extended efficient model approaching state-of-the-art accuracy on challenging datasets",
            "source": "https://huggingface.co/google/efficientnet-b6",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 693311678,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b6",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "efficientnet-b7-imagenet-torch",
            "base_filename": null,
            "version": null,
            "description": "Maximum efficient classifier pushing performance boundaries while preserving efficiency principles",
            "source": "https://huggingface.co/google/efficientnet-b7",
            "author": "Mingxing Tan and Quoc V. Le",
            "license": "Apache 2.0",
            "size_bytes": 1067822754,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "google/efficientnet-b7",
                    "channels_last": false,
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "efficientnet",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-06-30 12:00:00"
        },
        {
            "base_name": "swin-v2-tiny-torch",
            "base_filename": null,
            "version": null,
            "description": "Tiny hierarchical transformer for efficient visual recognition on edge devices",
            "source": "https://huggingface.co/microsoft/swinv2-tiny-patch4-window8-256",
            "author": "Ze Liu, et al.",
            "license": "MIT",
            "size_bytes": 453919540,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "microsoft/swinv2-tiny-patch4-window8-256",
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "swin-transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-07-02 12:00:00"
        },
        {
            "base_name": "swin-v2-small-torch",
            "base_filename": null,
            "version": null,
            "description": "Small hierarchical transformer balancing efficiency and performance for practical use",
            "source": "https://huggingface.co/microsoft/swinv2-small-patch4-window8-256",
            "author": "Ze Liu, et al.",
            "license": "MIT",
            "size_bytes": 796233910,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "microsoft/swinv2-small-patch4-window8-256",
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "swin-transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-07-02 12:00:00"
        },
        {
            "base_name": "swin-v2-base-torch",
            "base_filename": null,
            "version": null,
            "description": "Base hierarchical transformer delivering strong results across vision tasks",
            "source": "https://huggingface.co/microsoft/swinv2-base-patch4-window8-256",
            "author": "Ze Liu, et al.",
            "license": "MIT",
            "size_bytes": 1407282936,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "microsoft/swinv2-base-patch4-window8-256",
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "swin-transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-07-02 12:00:00"
        },
        {
            "base_name": "swin-v2-large-torch",
            "base_filename": null,
            "version": null,
            "description": "Large hierarchical transformer with enhanced capacity for demanding applications",
            "source": "https://huggingface.co/microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft",
            "author": "Ze Liu, et al.",
            "license": "MIT",
            "size_bytes": 3148420002,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForImageClassification",
                "config": {
                    "name_or_path": "microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft",
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "classification",
                "imagenet",
                "torch",
                "transformers",
                "swin-transformer",
                "official"
            ],
            "training_data": [
                {
                    "name": "ImageNet-1K",
                    "url": "https://www.image-net.org/download.php"
                }
            ],
            "date_added": "2025-07-02 12:00:00"
        },
        {
            "base_name": "rtdetr-v2-s-coco-torch",
            "base_filename": null,
            "version": null,
            "description": "Lightweight real-time object detector optimized for speed on edge devices",
            "source": "https://huggingface.co/PekingU/rtdetr_v2_r18vd",
            "author": "Wenyu Lv, et al.",
            "license": "Apache 2.0",
            "size_bytes": 161820194,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {
                    "name_or_path": "PekingU/rtdetr_v2_r18vd",
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["detection", "coco", "torch", "transformers", "rtdetr"],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-07-03 12:00:00"
        },
        {
            "base_name": "rtdetr-v2-m-coco-torch",
            "base_filename": null,
            "version": null,
            "description": "Balanced real-time object detector offering improved accuracy for production use",
            "source": "https://huggingface.co/PekingU/rtdetr_v2_r50vd",
            "author": "Wenyu Lv, et al.",
            "license": "Apache 2.0",
            "size_bytes": 344364318,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {
                    "name_or_path": "PekingU/rtdetr_v2_r50vd",
                    "transformers_processor_kwargs": {
                        "return_tensors": "pt"
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformers",
                "rtdetr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-07-03 12:00:00"
        },
        {
            "base_name": "dfine-nano-coco-torch",
            "base_filename": null,
            "version": null,
            "description": "D-FINE Nano from `D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement` trained on COCO. Ultra-lightweight real-time object detector.",
            "source": "https://github.com/Peterande/D-FINE",
            "author": "Yansong Peng, et al.",
            "license": "Apache 2.0",
            "size_bytes": 14680064,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {
                    "name_or_path": "ustc-community/dfine-nano-coco"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformers",
                "detr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-07-15 23:00:00"
        },
        {
            "base_name": "dfine-small-coco-torch",
            "base_filename": null,
            "version": null,
            "description": "D-FINE Small from `D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement` trained on COCO. Balanced real-time object detector.",
            "source": "https://github.com/Peterande/D-FINE",
            "author": "Yansong Peng, et al.",
            "license": "Apache 2.0",
            "size_bytes": 25165824,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {
                    "name_or_path": "ustc-community/dfine-small-coco"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformers",
                "detr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-07-15 23:00:00"
        },
        {
            "base_name": "dfine-medium-coco-torch",
            "base_filename": null,
            "version": null,
            "description": "D-FINE Medium from `D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement` trained on COCO. Mid-size real-time object detector.",
            "source": "https://github.com/Peterande/D-FINE",
            "author": "Yansong Peng, et al.",
            "license": "Apache 2.0",
            "size_bytes": 31457280,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {
                    "name_or_path": "ustc-community/dfine-medium-coco"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformers",
                "detr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-07-15 23:00:00"
        },
        {
            "base_name": "dfine-large-coco-torch",
            "base_filename": null,
            "version": null,
            "description": "D-FINE Large from `D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement` trained on COCO. Achieves 54.0% AP at 124 FPS on T4 GPU.",
            "source": "https://github.com/Peterande/D-FINE",
            "author": "Yansong Peng, et al.",
            "license": "Apache 2.0",
            "size_bytes": 52428800,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {
                    "name_or_path": "ustc-community/dfine-large-coco"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformers",
                "detr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-07-15 23:00:00"
        },
        {
            "base_name": "dfine-xlarge-coco-torch",
            "base_filename": null,
            "version": null,
            "description": "D-FINE XLarge from `D-FINE: Redefine Regression Task in DETRs as Fine-grained Distribution Refinement` trained on COCO. Achieves 55.8% AP at 78 FPS on T4 GPU.",
            "source": "https://github.com/Peterande/D-FINE",
            "author": "Yansong Peng, et al.",
            "license": "Apache 2.0",
            "size_bytes": 128974848,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForObjectDetection",
                "config": {
                    "name_or_path": "ustc-community/dfine-xlarge-coco"
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": [
                "detection",
                "coco",
                "torch",
                "transformers",
                "detr",
                "official"
            ],
            "training_data": [
                { "name": "MS COCO 2017", "url": "https://cocodataset.org" }
            ],
            "date_added": "2025-07-15 23:00:00"
        },
        {
            "base_name": "segformer-b0-ade20k-torch",
            "base_filename": null,
            "version": null,
            "description": "Efficient transformer-based semantic segmentation model for scene parsing with 150 classes",
            "source": "https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512",
            "author": "Enze Xie, et al.",
            "license": "MIT",
            "size_bytes": 15012832,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForSemanticSegmentation",
                "config": {
                    "name_or_path": "nvidia/segformer-b0-finetuned-ade-512-512",
                    "output_processor_args": {
                        "no_background_cls": true,
                        "has_softmax_out": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segmentation", "torch", "segformer", "official"],
            "training_data": [
                {
                    "name": "ADE20K",
                    "url": "https://github.com/CSAILVision/ADE20K"
                }
            ],
            "date_added": "2025-08-04 12:34:17"
        },
        {
            "base_name": "segformer-b1-ade20k-torch",
            "base_filename": null,
            "version": null,
            "description": "Balanced SegFormer model providing good accuracy-efficiency tradeoff for scene understanding",
            "source": "https://huggingface.co/nvidia/segformer-b1-finetuned-ade-512-512",
            "author": "Enze Xie, et al.",
            "license": "MIT",
            "size_bytes": 54865248,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForSemanticSegmentation",
                "config": {
                    "name_or_path": "nvidia/segformer-b1-finetuned-ade-512-512",
                    "output_processor_args": {
                        "no_background_cls": true,
                        "has_softmax_out": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segmentation", "torch", "segformer", "official"],
            "training_data": [
                {
                    "name": "ADE20K",
                    "url": "https://github.com/CSAILVision/ADE20K"
                }
            ],
            "date_added": "2025-08-04 12:34:20"
        },
        {
            "base_name": "segformer-b2-ade20k-torch",
            "base_filename": null,
            "version": null,
            "description": "Medium-sized SegFormer delivering enhanced segmentation quality for complex scenes",
            "source": "https://huggingface.co/nvidia/segformer-b2-finetuned-ade-512-512",
            "author": "Enze Xie, et al.",
            "license": "MIT",
            "size_bytes": 109854048,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForSemanticSegmentation",
                "config": {
                    "name_or_path": "nvidia/segformer-b2-finetuned-ade-512-512",
                    "output_processor_args": {
                        "no_background_cls": true,
                        "has_softmax_out": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segmentation", "torch", "segformer", "official"],
            "training_data": [
                {
                    "name": "ADE20K",
                    "url": "https://github.com/CSAILVision/ADE20K"
                }
            ],
            "date_added": "2025-08-04 12:34:22"
        },
        {
            "base_name": "segformer-b3-ade20k-torch",
            "base_filename": null,
            "version": null,
            "description": "Larger SegFormer model with improved accuracy for detailed semantic segmentation",
            "source": "https://huggingface.co/nvidia/segformer-b3-finetuned-ade-512-512",
            "author": "Enze Xie, et al.",
            "license": "MIT",
            "size_bytes": 189357408,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForSemanticSegmentation",
                "config": {
                    "name_or_path": "nvidia/segformer-b3-finetuned-ade-512-512",
                    "output_processor_args": {
                        "no_background_cls": true,
                        "has_softmax_out": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segmentation", "torch", "segformer", "official"],
            "training_data": [
                {
                    "name": "ADE20K",
                    "url": "https://github.com/CSAILVision/ADE20K"
                }
            ],
            "date_added": "2025-08-04 12:34:24"
        },
        {
            "base_name": "segformer-b4-ade20k-torch",
            "base_filename": null,
            "version": null,
            "description": "High-capacity SegFormer achieving excellent results on challenging segmentation tasks",
            "source": "https://huggingface.co/nvidia/segformer-b4-finetuned-ade-512-512",
            "author": "Enze Xie, et al.",
            "license": "MIT",
            "size_bytes": 256439648,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForSemanticSegmentation",
                "config": {
                    "name_or_path": "nvidia/segformer-b4-finetuned-ade-512-512",
                    "output_processor_args": {
                        "no_background_cls": true,
                        "has_softmax_out": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segmentation", "torch", "segformer", "official"],
            "training_data": [
                {
                    "name": "ADE20K",
                    "url": "https://github.com/CSAILVision/ADE20K"
                }
            ],
            "date_added": "2025-08-04 12:34:25"
        },
        {
            "base_name": "segformer-b5-ade20k-torch",
            "base_filename": null,
            "version": null,
            "description": "Largest SegFormer model delivering the best semantic segmentation performance in its family",
            "source": "https://huggingface.co/nvidia/segformer-b5-finetuned-ade-640-640",
            "author": "Enze Xie, et al.",
            "license": "MIT",
            "size_bytes": 338840928,
            "default_deployment_config_dict": {
                "type": "fiftyone.utils.transformers.FiftyOneTransformerForSemanticSegmentation",
                "config": {
                    "name_or_path": "nvidia/segformer-b5-finetuned-ade-640-640",
                    "output_processor_args": {
                        "no_background_cls": true,
                        "has_softmax_out": false
                    }
                }
            },
            "requirements": {
                "packages": ["torch", "torchvision", "transformers"],
                "cpu": {
                    "support": true
                },
                "gpu": {
                    "support": true
                }
            },
            "tags": ["segmentation", "torch", "segformer", "official"],
            "training_data": [
                {
                    "name": "ADE20K",
                    "url": "https://github.com/CSAILVision/ADE20K"
                }
            ],
            "date_added": "2025-08-04 12:34:27"
        }
    ]
}
