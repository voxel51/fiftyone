{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup Data Splits\n",
    "\n",
    "Before iterating on annotations, you need proper data splits. Without them, you'll contaminate your evaluation and build a model that only looks good on paper.\n",
    "\n",
    "**This step creates:**\n",
    "- **Test set (15%)** - Frozen. Never used for selection or training. Final evaluation only.\n",
    "- **Validation set (15%)** - For iteration decisions. Used to evaluate between training rounds.\n",
    "- **Golden QA set (5%)** - Small, heavily reviewed. Detects label drift.\n",
    "- **Pool (65%)** - Active learning pool. All new labels come from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import random\n",
    "\n",
    "DATASET_NAME = \"annotation_tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create the dataset (idempotent - safe to rerun)\n",
    "if DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Loading existing dataset: {DATASET_NAME}\")\n",
    "    dataset = fo.load_dataset(DATASET_NAME)\n",
    "    \n",
    "    # Check if splits already exist\n",
    "    existing_views = dataset.list_saved_views()\n",
    "    if \"pool\" in existing_views:\n",
    "        print(\"Splits already exist. Skipping creation.\")\n",
    "        SPLITS_EXIST = True\n",
    "    else:\n",
    "        SPLITS_EXIST = False\n",
    "else:\n",
    "    print(f\"Creating new dataset: {DATASET_NAME}\")\n",
    "    dataset = foz.load_zoo_dataset(\"quickstart\", dataset_name=DATASET_NAME)\n",
    "    dataset.persistent = True\n",
    "    SPLITS_EXIST = False\n",
    "\n",
    "print(f\"\\nDataset: {dataset.name}\")\n",
    "print(f\"Samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits (only if they don't exist)\n",
    "if not SPLITS_EXIST:\n",
    "    random.seed(42)\n",
    "    sample_ids = list(dataset.values(\"id\"))\n",
    "    random.shuffle(sample_ids)\n",
    "\n",
    "    n = len(sample_ids)\n",
    "    n_test = int(0.15 * n)      # 15% frozen test\n",
    "    n_val = int(0.15 * n)       # 15% validation\n",
    "    n_golden = int(0.05 * n)    # 5% golden QA\n",
    "    # Remainder is pool\n",
    "\n",
    "    test_ids = sample_ids[:n_test]\n",
    "    val_ids = sample_ids[n_test:n_test + n_val]\n",
    "    golden_ids = sample_ids[n_test + n_val:n_test + n_val + n_golden]\n",
    "    pool_ids = sample_ids[n_test + n_val + n_golden:]\n",
    "\n",
    "    print(f\"Creating splits:\")\n",
    "    print(f\"  Test:   {len(test_ids)} ({100*len(test_ids)/n:.0f}%)\")\n",
    "    print(f\"  Val:    {len(val_ids)} ({100*len(val_ids)/n:.0f}%)\")\n",
    "    print(f\"  Golden: {len(golden_ids)} ({100*len(golden_ids)/n:.0f}%)\")\n",
    "    print(f\"  Pool:   {len(pool_ids)} ({100*len(pool_ids)/n:.0f}%)\")\n",
    "\n",
    "    # Tag samples by split\n",
    "    dataset.select(test_ids).tag_samples(\"split:test\")\n",
    "    dataset.select(val_ids).tag_samples(\"split:val\")\n",
    "    dataset.select(golden_ids).tag_samples(\"split:golden\")\n",
    "    dataset.select(pool_ids).tag_samples(\"split:pool\")\n",
    "\n",
    "    # Save views for easy access\n",
    "    dataset.save_view(\"test_set\", dataset.match_tags(\"split:test\"))\n",
    "    dataset.save_view(\"val_set\", dataset.match_tags(\"split:val\"))\n",
    "    dataset.save_view(\"golden_qa\", dataset.match_tags(\"split:golden\"))\n",
    "    dataset.save_view(\"pool\", dataset.match_tags(\"split:pool\"))\n",
    "\n",
    "    print(\"\\nSplits created and saved as views.\")\n",
    "else:\n",
    "    print(\"Using existing splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add annotation tracking field (idempotent)\n",
    "if \"annotation_status\" not in dataset.get_field_schema():\n",
    "    dataset.add_sample_field(\"annotation_status\", fo.StringField)\n",
    "    dataset.set_values(\"annotation_status\", [\"unlabeled\"] * len(dataset))\n",
    "    print(\"Added annotation_status field (all samples start as 'unlabeled')\")\n",
    "else:\n",
    "    print(\"annotation_status field already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why These Splits Matter\n",
    "\n",
    "| Split | Purpose | Rules |\n",
    "|-------|---------|-------|\n",
    "| **Test** | Final evaluation | Never touch. Never use for selection. Check only at the end. |\n",
    "| **Val** | Iteration decisions | Evaluate after each training round. Guides what to label next. |\n",
    "| **Golden** | Label quality check | Heavily reviewed. Re-check after each annotation round for drift. |\n",
    "| **Pool** | Labeling source | All new annotations come from here. |\n",
    "\n",
    "**If you skip this:** Your metrics become meaningless. You'll overfit to your own selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup\n",
    "print(\"Saved views:\", dataset.list_saved_views())\n",
    "print(f\"\\nTest set: {len(dataset.load_saved_view('test_set'))} samples\")\n",
    "print(f\"Val set: {len(dataset.load_saved_view('val_set'))} samples\")\n",
    "print(f\"Golden QA: {len(dataset.load_saved_view('golden_qa'))} samples\")\n",
    "print(f\"Pool: {len(dataset.load_saved_view('pool'))} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You created four data splits with clear purposes:\n",
    "- Test (frozen), Val (iteration), Golden (QA), Pool (labeling source)\n",
    "\n",
    "**Next:** Step 3 - Smart Sample Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
