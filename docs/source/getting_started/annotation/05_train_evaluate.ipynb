{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train + Evaluate\n",
    "\n",
    "Train a detector on your labeled data and evaluate it properly. Understanding **where** the model fails tells you what to label next.\n",
    "\n",
    "> **Note:** We train on `human_labels` and evaluate against `human_labels` on the val set. For this tutorial, we copy ground_truth to human_labels for val samples (filtered to schema classes) so we have labels to evaluate against. In production, you'd label the val set too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import os\n",
    "\n",
    "LABEL_FIELD = \"human_labels\"\n",
    "\n",
    "dataset = fo.load_dataset(\"annotation_tutorial\")\n",
    "\n",
    "# Get schema classes from dataset info\n",
    "if \"annotation_schema\" in dataset.info:\n",
    "    SCHEMA_CLASSES = set(dataset.info[\"annotation_schema\"][\"classes\"])\n",
    "else:\n",
    "    # Fallback\n",
    "    SCHEMA_CLASSES = {\n",
    "        \"person\", \"car\", \"truck\", \"bus\", \"motorcycle\", \"bicycle\",\n",
    "        \"dog\", \"cat\", \"bird\", \"horse\",\n",
    "        \"chair\", \"couch\", \"dining table\", \"tv\",\n",
    "        \"bottle\", \"cup\", \"bowl\", \"other\"\n",
    "    }\n",
    "\n",
    "print(f\"Schema classes: {len(SCHEMA_CLASSES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Data\n",
    "\n",
    "**Important:** We only train on samples that actually have labels, not just samples tagged as \"annotated\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: annotated samples WITH actual labels\n",
    "annotated = dataset.match_tags(\"annotated:v0\")\n",
    "\n",
    "# Filter to only samples with detections\n",
    "train_view = annotated.match(F(f\"{LABEL_FIELD}.detections\").length() > 0)\n",
    "\n",
    "# Validation data\n",
    "val_view = dataset.load_saved_view(\"val_set\")\n",
    "\n",
    "print(f\"Annotated samples: {len(annotated)}\")\n",
    "print(f\"Training samples (with labels): {len(train_view)}\")\n",
    "print(f\"Validation samples: {len(val_view)}\")\n",
    "\n",
    "if len(train_view) == 0:\n",
    "    print(\"\\n>>> No training samples with labels. Complete Step 4 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation, we need human_labels on val set\n",
    "# In production, you'd label these. For tutorial, we copy ground_truth\n",
    "# FILTERED to schema classes for consistency\n",
    "\n",
    "copied_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for sample in val_view:\n",
    "    if sample.ground_truth and not sample[LABEL_FIELD]:\n",
    "        filtered_dets = []\n",
    "        for d in sample.ground_truth.detections:\n",
    "            if d.label in SCHEMA_CLASSES:\n",
    "                filtered_dets.append(fo.Detection(\n",
    "                    label=d.label,\n",
    "                    bounding_box=d.bounding_box\n",
    "                ))\n",
    "                copied_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        sample[LABEL_FIELD] = fo.Detections(detections=filtered_dets)\n",
    "        sample.save()\n",
    "\n",
    "print(f\"Val set prepared: {copied_count} detections copied, {skipped_count} skipped (not in schema)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_view) == 0:\n",
    "    raise ValueError(\"No training samples. Complete Step 4 first.\")\n",
    "\n",
    "# Get classes from training data\n",
    "classes = train_view.distinct(f\"{LABEL_FIELD}.detections.label\")\n",
    "print(f\"Classes in training data: {classes}\")\n",
    "\n",
    "export_dir = \"/tmp/annotation_tutorial_yolo\"\n",
    "os.makedirs(export_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training data\n",
    "train_view.export(\n",
    "    export_dir=os.path.join(export_dir, \"train\"),\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=LABEL_FIELD,\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "# Export validation data\n",
    "val_view.export(\n",
    "    export_dir=os.path.join(export_dir, \"val\"),\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=LABEL_FIELD,\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "print(f\"Exported {len(train_view)} train, {len(val_view)} val samples to {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YAML config\n",
    "yaml_content = f\"\"\"path: {export_dir}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "names:\n",
    "\"\"\"\n",
    "for i, cls in enumerate(classes):\n",
    "    yaml_content += f\"  {i}: {cls}\\n\"\n",
    "\n",
    "yaml_path = os.path.join(export_dir, \"dataset.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"Created {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Train (small model, few epochs for demo)\n",
    "model = YOLO('yolov8n.pt')\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=10,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    name='tutorial_v0',\n",
    "    project='/tmp/yolo_tutorial'\n",
    ")\n",
    "\n",
    "model_path = '/tmp/yolo_tutorial/tutorial_v0/weights/best.pt'\n",
    "print(f\"Model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Run inference on val set\n",
    "for sample in val_view:\n",
    "    results = model(sample.filepath, verbose=False)[0]\n",
    "    \n",
    "    detections = []\n",
    "    if results.boxes is not None:\n",
    "        for box in results.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxyn[0].tolist()\n",
    "            conf = box.conf[0].item()\n",
    "            cls_idx = int(box.cls[0].item())\n",
    "            label = classes[cls_idx] if cls_idx < len(classes) else f\"class_{cls_idx}\"\n",
    "            \n",
    "            detections.append(fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[x1, y1, x2-x1, y2-y1],\n",
    "                confidence=conf\n",
    "            ))\n",
    "    \n",
    "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    sample.save()\n",
    "\n",
    "print(f\"Added predictions to {len(val_view)} val samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = val_view.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=LABEL_FIELD,\n",
    "    eval_key=\"eval_v0\",\n",
    "    compute_mAP=True\n",
    ")\n",
    "\n",
    "print(f\"mAP: {results.mAP():.3f}\")\n",
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Failures\n",
    "\n",
    "Understanding failures is more important than the mAP number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-FN samples (model missed objects)\n",
    "high_fn = val_view.sort_by(\"eval_v0_fn\", reverse=True).limit(10)\n",
    "high_fn.tag_samples(\"failure:high_fn\")\n",
    "\n",
    "# Find high-FP samples (model hallucinated)\n",
    "high_fp = val_view.sort_by(\"eval_v0_fp\", reverse=True).limit(10)\n",
    "high_fp.tag_samples(\"failure:high_fp\")\n",
    "\n",
    "print(f\"Tagged {len(high_fn)} high-FN samples\")\n",
    "print(f\"Tagged {len(high_fp)} high-FP samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View failures in App\n",
    "session = fo.launch_app(val_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the App:\n",
    "1. Filter by `failure:high_fn` to see samples where model missed objects\n",
    "2. Filter by `failure:high_fp` to see samples where model hallucinated\n",
    "3. Look for patterns: specific classes? lighting conditions? object sizes?\n",
    "\n",
    "These patterns tell you what to label next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation info\n",
    "dataset.info[\"eval_v0\"] = {\n",
    "    \"mAP\": results.mAP(),\n",
    "    \"train_samples\": len(train_view),\n",
    "    \"val_samples\": len(val_view),\n",
    "    \"model_path\": model_path\n",
    "}\n",
    "dataset.save()\n",
    "\n",
    "# Save failure view\n",
    "failures = val_view.match_tags([\"failure:high_fn\", \"failure:high_fp\"])\n",
    "dataset.save_view(\"eval_v0_failures\", failures)\n",
    "\n",
    "print(f\"Saved {len(failures)} failure samples to view 'eval_v0_failures'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You trained and evaluated a model:\n",
    "- Filtered to samples with actual labels (not just tagged as annotated)\n",
    "- Exported in YOLO format\n",
    "- Trained YOLOv8n for 10 epochs\n",
    "- Evaluated with FiftyOne: mAP + per-sample FP/FN\n",
    "- Tagged failure cases for next iteration\n",
    "\n",
    "**Key insight:** The failure tags tell you what to label next.\n",
    "\n",
    "**Next:** Step 6 - Iteration Loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
