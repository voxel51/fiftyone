========================================
HUMAN ANNOTATION TUTORIAL - FULL CONTENT
========================================

=== INDEX.RST ===
.. _human_annotation_guide:

Human Annotation Guide
======================

.. default-role:: code

**In-App Labeling for Detection Datasets**

FiftyOne's in-app annotation lets you create and edit labels directly in the App—no external tools required. This guide offers two tracks depending on your goals.

.. _human_annotation-tracks:

Choose Your Track
-----------------

.. list-table::
   :widths: 20 15 25 40
   :header-rows: 1

   * - Track
     - Level
     - Time
     - Best For
   * - **Quickstart**
     - Beginner
     - 10-15 min
     - "Does this work for me?" Try in-app labeling immediately.
   * - **Full Loop**
     - Intermediate
     - 60-90 min
     - Build a complete curate -> annotate -> train -> evaluate pipeline.

.. note::

   **These tracks are independent.** Quickstart uses dataset ``my_annotation_project``, Full Loop uses ``annotation_tutorial``. You can do both without conflict.

.. _human_annotation-quickstart:

Quickstart Track
----------------

**Level:** Beginner | **Time:** 10-15 minutes

Jump straight to labeling:

1. :doc:`01_quickstart` - Load data, enter annotate mode, draw boxes, verify labels saved

If in-app annotation fits your needs, check out the Full Loop for production workflows.

.. _human_annotation-full-loop:

Full Loop Track
---------------

**Level:** Intermediate | **Time:** 60-90 minutes

A complete data-centric detection workflow. You should be comfortable with:

- Basic Python and Jupyter notebooks
- Train/val/test split concepts
- What embeddings represent (conceptually)
- Running a training loop (we use YOLOv8)

**Steps:**

2. :doc:`02_setup_splits` - Create frozen test, golden QA, and active pool splits
3. :doc:`03_smart_selection` - Use diversity sampling to pick high-value samples
4. :doc:`04_annotation_qa` - Annotate in the App with QA discipline
5. :doc:`05_train_evaluate` - Train YOLOv8, evaluate, analyze failure modes
6. :doc:`06_iteration` - Hybrid acquisition loop: coverage + targeted failure mining

.. _human_annotation-what-you-learn:

What You'll Learn
-----------------

**Quickstart Track:**

- How to enter Annotate mode in the FiftyOne App
- Creating detection bounding boxes and classifications
- Verifying annotations saved correctly
- Exporting labeled data for training

**Full Loop Track (adds):**

- Split discipline: frozen test set, golden QA, active pool
- Diversity-based sample selection for efficient labeling
- QA workflows to catch label errors before training
- Failure analysis to drive the next labeling batch
- Iterative improvement without test set contamination

.. _human_annotation-when-to-use:

When to Use In-App Annotation
-----------------------------

**Good fit:**

- Small to medium annotation tasks (tens to hundreds of samples)
- Quick corrections and QA passes
- Prototyping label schemas before scaling
- Single annotator or small team workflows
- Tight feedback loops between labeling and model evaluation

**Consider external tools (CVAT, Label Studio) when:**

- High-volume annotation with multiple annotators
- Complex role-based review and approval workflows
- Annotation task management and assignment
- You need audit trails and annotator agreement metrics

FiftyOne integrates with external annotation tools via the :ref:`annotation API <fiftyone-annotation>`.

.. _human_annotation-dataset:

Dataset
-------

Both tracks use FiftyOne's `quickstart` dataset (200 images from COCO with detection annotations). It downloads automatically when you run the notebooks.

.. _human_annotation-start:

Ready to Begin?
---------------

Click **Next** to start with the Quickstart track, or jump directly to :doc:`02_setup_splits` for the Full Loop.

.. toctree::
   :maxdepth: 1
   :hidden:

   Quickstart: In-App Labeling <01_quickstart.ipynb>
   Setup: Data Splits <02_setup_splits.ipynb>
   Smart Sample Selection <03_smart_selection.ipynb>
   Annotation + QA <04_annotation_qa.ipynb>
   Train + Evaluate <05_train_evaluate.ipynb>
   Iteration Loop <06_iteration.ipynb>
   Guide Summary <summary>

=== 01_QUICKSTART.IPYNB ===
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart: In-App Labeling in 10 Minutes\n",
    "\n",
    "This quickstart shows you how to create and edit labels directly in the FiftyOne App. By the end, you'll know:\n",
    "\n",
    "1. How to enter Annotate mode\n",
    "2. How to draw bounding boxes and set classes\n",
    "3. Where your labels are stored\n",
    "4. How to export labeled data\n",
    "\n",
    "Let's go.\n",
    "\n",
    "> **Note:** This track is standalone. The Full Loop track uses a separate dataset, so you can do both independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, define the label field name we'll use throughout this tutorial. **Use this exact name** when creating your field in the App."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: Use this exact field name when labeling in the App\n",
    "LABEL_FIELD = \"my_labels\"\n",
    "DATASET_NAME = \"my_annotation_project\"\n",
    "\n",
    "print(f\"Dataset name: {DATASET_NAME}\")\n",
    "print(f\"Label field: {LABEL_FIELD}\")\n",
    "print(f\"\\nWhen you create a label field in the App, name it exactly: {LABEL_FIELD}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Clone the Dataset\n",
    "\n",
    "We'll use the `quickstart` dataset (200 images from COCO). We clone it so your annotations are saved separately from the original zoo dataset.\n",
    "\n",
    "This cell is **idempotent** - safe to rerun. If the dataset already exists, it loads it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create the dataset (safe to rerun)\n",
    "if DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Loading existing dataset: {DATASET_NAME}\")\n",
    "    dataset = fo.load_dataset(DATASET_NAME)\n",
    "else:\n",
    "    print(f\"Creating new dataset: {DATASET_NAME}\")\n",
    "    source = foz.load_zoo_dataset(\"quickstart\")\n",
    "    dataset = source.clone(DATASET_NAME)\n",
    "    dataset.persistent = True\n",
    "\n",
    "print(f\"\\nDataset: {dataset.name}\")\n",
    "print(f\"Samples: {len(dataset)}\")\n",
    "print(f\"Persistent: {dataset.persistent}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launch the App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enter Annotate Mode\n",
    "\n",
    "In the FiftyOne App:\n",
    "\n",
    "1. **Click on any sample** to open the sample modal (expanded view)\n",
    "2. **Look at the left sidebar** - you'll see tabs at the top\n",
    "3. **Click the \"Annotate\" tab** (pencil icon) to enter annotation mode\n",
    "\n",
    "When you enter Annotate mode, you'll see:\n",
    "- **Action buttons** at the top: Classification, Detection (bounding box), Undo, Redo\n",
    "- **Schema button**: Configure which fields and classes are available\n",
    "- **Label list**: Shows existing labels on this sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Label Field\n",
    "\n",
    "Before drawing boxes, create a field to store them:\n",
    "\n",
    "1. In Annotate mode, click the **Schema** button\n",
    "2. Click **\"+ Add Field\"**\n",
    "3. **Enter the exact name:** `my_labels` (must match `LABEL_FIELD` above)\n",
    "4. Select the field type: **Detections** for bounding boxes\n",
    "5. Define your classes (e.g., `car`, `person`, `bicycle`)\n",
    "6. Click **Save**\n",
    "\n",
    "Your new field is now available for annotation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Draw Bounding Boxes\n",
    "\n",
    "Now let's draw some boxes:\n",
    "\n",
    "1. **Click the Detection button** (square icon) in the action bar\n",
    "2. **Click and drag** on the image to draw a bounding box\n",
    "3. **Release** to complete the box\n",
    "4. **Select a class** from the dropdown that appears\n",
    "5. Your label is saved automatically!\n",
    "\n",
    "### Tips:\n",
    "- **Resize**: Drag the corners or edges of a box\n",
    "- **Move**: Click inside the box and drag\n",
    "- **Delete**: Select the box and press `Delete` or `Backspace`\n",
    "- **Undo/Redo**: Use the buttons or keyboard shortcuts\n",
    "\n",
    "### Try it now:\n",
    "1. Go back to the App\n",
    "2. Draw 3-5 bounding boxes on different objects\n",
    "3. Assign classes to each\n",
    "4. Move to a few more samples and label those too"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Your Labels Saved\n",
    "\n",
    "**Run this cell after labeling to confirm your annotations were saved correctly.**\n",
    "\n",
    "If the counts below are 0, either:\n",
    "- You haven't labeled any samples yet, or\n",
    "- You used a different field name (must be exactly `my_labels`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload to see your changes\n",
    "dataset.reload()\n",
    "\n",
    "# Count samples with labels in your field\n",
    "if LABEL_FIELD in dataset.get_field_schema():\n",
    "    labeled = dataset.match(F(f\"{LABEL_FIELD}.detections\").length() > 0)\n",
    "    total_dets = sum(\n",
    "        len(s[LABEL_FIELD].detections) \n",
    "        for s in labeled \n",
    "        if s[LABEL_FIELD] is not None\n",
    "    )\n",
    "    \n",
    "    print(f\"Samples with labels in '{LABEL_FIELD}': {len(labeled)}\")\n",
    "    print(f\"Total detections: {total_dets}\")\n",
    "    \n",
    "    if len(labeled) > 0:\n",
    "        print(f\"\\n--- Example from first labeled sample ---\")\n",
    "        sample = labeled.first()\n",
    "        print(f\"File: {sample.filepath.split('/')[-1]}\")\n",
    "        for det in sample[LABEL_FIELD].detections:\n",
    "            print(f\"  {det.label}: {[round(x, 3) for x in det.bounding_box]}\")\n",
    "    else:\n",
    "        print(\"\\n>>> No labels found. Did you draw boxes in the App?\")\n",
    "        print(f\">>> Make sure your field is named exactly: {LABEL_FIELD}\")\n",
    "else:\n",
    "    print(f\"Field '{LABEL_FIELD}' not found in dataset.\")\n",
    "    print(f\"\\nAvailable fields: {list(dataset.get_field_schema().keys())}\")\n",
    "    print(f\"\\n>>> Create a field named exactly '{LABEL_FIELD}' in the App.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classifications (Optional)\n",
    "\n",
    "You can also add image-level classifications:\n",
    "\n",
    "1. **Click the Classification button** (tag icon)\n",
    "2. **Select a class** from the dropdown\n",
    "3. The classification is added to the sample\n",
    "\n",
    "Use classifications for:\n",
    "- Image-level attributes (\"indoor\", \"outdoor\", \"daytime\", \"night\")\n",
    "- Quality flags (\"blurry\", \"good\", \"needs_review\")\n",
    "- Dataset splits (\"train\", \"val\", \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit Existing Labels\n",
    "\n",
    "To edit labels that already exist on a sample:\n",
    "\n",
    "1. **Click on a label** in the image or in the sidebar list\n",
    "2. The label becomes selected (highlighted)\n",
    "3. **Drag** to reposition or resize\n",
    "4. **Click the class name** to change it\n",
    "5. **Press Delete** to remove it\n",
    "\n",
    "Changes are saved automatically—no \"Save\" button needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Your Labeled Data\n",
    "\n",
    "Once you've annotated samples, export them for training. We filter to only samples that have labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only samples with labels\n",
    "if LABEL_FIELD in dataset.get_field_schema():\n",
    "    labeled_samples = dataset.match(F(f\"{LABEL_FIELD}.detections\").length() > 0)\n",
    "else:\n",
    "    labeled_samples = dataset.limit(0)\n",
    "\n",
    "if len(labeled_samples) == 0:\n",
    "    print(\"No labeled samples to export. Label some samples first!\")\n",
    "else:\n",
    "    # Export in COCO format\n",
    "    labeled_samples.export(\n",
    "        export_dir=\"/tmp/my_labeled_data\",\n",
    "        dataset_type=fo.types.COCODetectionDataset,\n",
    "        label_field=LABEL_FIELD,\n",
    "    )\n",
    "    print(f\"Exported {len(labeled_samples)} samples to /tmp/my_labeled_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or export in YOLO format for YOLOv8 training\n",
    "if len(labeled_samples) > 0:\n",
    "    labeled_samples.export(\n",
    "        export_dir=\"/tmp/my_labeled_data_yolo\",\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        label_field=LABEL_FIELD,\n",
    "    )\n",
    "    print(f\"Exported {len(labeled_samples)} samples to /tmp/my_labeled_data_yolo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Behaviors\n",
    "\n",
    "### Auto-save\n",
    "Labels save automatically as you create/edit them. There's no \"Save\" button.\n",
    "\n",
    "### Schema Enforcement\n",
    "If you define a schema with specific classes, only those classes are available when labeling. This prevents typos and maintains consistency.\n",
    "\n",
    "### Persistence\n",
    "Because we set `dataset.persistent = True`, your dataset and labels persist between Python sessions. Run `fo.load_dataset(\"my_annotation_project\")` to reload it later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You learned how to:\n",
    "\n",
    "1. Create a persistent dataset for annotation\n",
    "2. Enter Annotate mode (click sample -> Annotate tab)\n",
    "3. Create a label field with a schema\n",
    "4. Draw bounding boxes and assign classes\n",
    "5. Verify labels saved correctly\n",
    "6. Export for training\n",
    "\n",
    "**That's in-app annotation in FiftyOne.** You now have everything you need to label small datasets or make quick corrections.\n",
    "\n",
    "---\n",
    "\n",
    "## What's Next?\n",
    "\n",
    "For production annotation workflows, continue to the **Full Loop Track**:\n",
    "\n",
    "- **Step 2: Setup Splits** - Create proper train/val/test splits\n",
    "- **Step 3: Smart Selection** - Use diversity sampling to pick high-value samples\n",
    "- **Step 4: Annotation + QA** - Disciplined labeling with quality checks\n",
    "- **Step 5: Train + Evaluate** - Train a model and analyze failures\n",
    "- **Step 6: Iteration** - Use failures to drive the next labeling batch\n",
    "\n",
    "The Full Loop teaches you to label **smarter, not harder**.\n",
    "\n",
    "> **Note:** The Full Loop uses a separate dataset (`annotation_tutorial`) so you can do both tracks independently."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

=== 02_SETUP_SPLITS.IPYNB ===
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Setup Data Splits\n",
    "\n",
    "Before iterating on annotations, you need proper data splits. Without them, you'll contaminate your evaluation and build a model that only looks good on paper.\n",
    "\n",
    "**This step creates:**\n",
    "- **Test set (15%)** - Frozen. Never used for selection or training. Final evaluation only.\n",
    "- **Validation set (15%)** - For iteration decisions. Used to evaluate between training rounds.\n",
    "- **Golden QA set (5%)** - Small, heavily reviewed. Detects label drift.\n",
    "- **Pool (65%)** - Active learning pool. All new labels come from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import random\n",
    "\n",
    "DATASET_NAME = \"annotation_tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load or create the dataset (idempotent - safe to rerun)\n",
    "if DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Loading existing dataset: {DATASET_NAME}\")\n",
    "    dataset = fo.load_dataset(DATASET_NAME)\n",
    "    \n",
    "    # Check if splits already exist\n",
    "    existing_views = dataset.list_saved_views()\n",
    "    if \"pool\" in existing_views:\n",
    "        print(\"Splits already exist. Skipping creation.\")\n",
    "        SPLITS_EXIST = True\n",
    "    else:\n",
    "        SPLITS_EXIST = False\n",
    "else:\n",
    "    print(f\"Creating new dataset: {DATASET_NAME}\")\n",
    "    dataset = foz.load_zoo_dataset(\"quickstart\", dataset_name=DATASET_NAME)\n",
    "    dataset.persistent = True\n",
    "    SPLITS_EXIST = False\n",
    "\n",
    "print(f\"\\nDataset: {dataset.name}\")\n",
    "print(f\"Samples: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create splits (only if they don't exist)\n",
    "if not SPLITS_EXIST:\n",
    "    random.seed(42)\n",
    "    sample_ids = list(dataset.values(\"id\"))\n",
    "    random.shuffle(sample_ids)\n",
    "\n",
    "    n = len(sample_ids)\n",
    "    n_test = int(0.15 * n)      # 15% frozen test\n",
    "    n_val = int(0.15 * n)       # 15% validation\n",
    "    n_golden = int(0.05 * n)    # 5% golden QA\n",
    "    # Remainder is pool\n",
    "\n",
    "    test_ids = sample_ids[:n_test]\n",
    "    val_ids = sample_ids[n_test:n_test + n_val]\n",
    "    golden_ids = sample_ids[n_test + n_val:n_test + n_val + n_golden]\n",
    "    pool_ids = sample_ids[n_test + n_val + n_golden:]\n",
    "\n",
    "    print(f\"Creating splits:\")\n",
    "    print(f\"  Test:   {len(test_ids)} ({100*len(test_ids)/n:.0f}%)\")\n",
    "    print(f\"  Val:    {len(val_ids)} ({100*len(val_ids)/n:.0f}%)\")\n",
    "    print(f\"  Golden: {len(golden_ids)} ({100*len(golden_ids)/n:.0f}%)\")\n",
    "    print(f\"  Pool:   {len(pool_ids)} ({100*len(pool_ids)/n:.0f}%)\")\n",
    "\n",
    "    # Tag samples by split\n",
    "    dataset.select(test_ids).tag_samples(\"split:test\")\n",
    "    dataset.select(val_ids).tag_samples(\"split:val\")\n",
    "    dataset.select(golden_ids).tag_samples(\"split:golden\")\n",
    "    dataset.select(pool_ids).tag_samples(\"split:pool\")\n",
    "\n",
    "    # Save views for easy access\n",
    "    dataset.save_view(\"test_set\", dataset.match_tags(\"split:test\"))\n",
    "    dataset.save_view(\"val_set\", dataset.match_tags(\"split:val\"))\n",
    "    dataset.save_view(\"golden_qa\", dataset.match_tags(\"split:golden\"))\n",
    "    dataset.save_view(\"pool\", dataset.match_tags(\"split:pool\"))\n",
    "\n",
    "    print(\"\\nSplits created and saved as views.\")\n",
    "else:\n",
    "    print(\"Using existing splits.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add annotation tracking field (idempotent)\n",
    "if \"annotation_status\" not in dataset.get_field_schema():\n",
    "    dataset.add_sample_field(\"annotation_status\", fo.StringField)\n",
    "    dataset.set_values(\"annotation_status\", [\"unlabeled\"] * len(dataset))\n",
    "    print(\"Added annotation_status field (all samples start as 'unlabeled')\")\n",
    "else:\n",
    "    print(\"annotation_status field already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why These Splits Matter\n",
    "\n",
    "| Split | Purpose | Rules |\n",
    "|-------|---------|-------|\n",
    "| **Test** | Final evaluation | Never touch. Never use for selection. Check only at the end. |\n",
    "| **Val** | Iteration decisions | Evaluate after each training round. Guides what to label next. |\n",
    "| **Golden** | Label quality check | Heavily reviewed. Re-check after each annotation round for drift. |\n",
    "| **Pool** | Labeling source | All new annotations come from here. |\n",
    "\n",
    "**If you skip this:** Your metrics become meaningless. You'll overfit to your own selection strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify setup\n",
    "print(\"Saved views:\", dataset.list_saved_views())\n",
    "print(f\"\\nTest set: {len(dataset.load_saved_view('test_set'))} samples\")\n",
    "print(f\"Val set: {len(dataset.load_saved_view('val_set'))} samples\")\n",
    "print(f\"Golden QA: {len(dataset.load_saved_view('golden_qa'))} samples\")\n",
    "print(f\"Pool: {len(dataset.load_saved_view('pool'))} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You created four data splits with clear purposes:\n",
    "- Test (frozen), Val (iteration), Golden (QA), Pool (labeling source)\n",
    "\n",
    "**Next:** Step 3 - Smart Sample Selection"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

=== 03_SMART_SELECTION.IPYNB ===
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Smart Sample Selection\n",
    "\n",
    "Random sampling wastes labels on redundant near-duplicates. This step uses **diversity-based selection** to pick high-value samples that cover your data distribution efficiently.\n",
    "\n",
    "We'll use **ZCore (Zero-Shot Coreset Selection)** to score samples based on:\n",
    "- **Coverage**: How much of the embedding space does this sample represent?\n",
    "- **Redundancy**: How many near-duplicates exist?\n",
    "\n",
    "High ZCore score = valuable for labeling. Low score = redundant, skip it.\n",
    "\n",
    "> **Reference:** ZCore is from [Voxel51's research](https://github.com/voxel51/zcore). The implementation below is simplified for understanding. For production use with large datasets, see the official repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "import numpy as np\n",
    "\n",
    "dataset = fo.load_dataset(\"annotation_tutorial\")\n",
    "pool = dataset.load_saved_view(\"pool\")\n",
    "\n",
    "print(f\"Pool size: {len(pool)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Embeddings\n",
    "\n",
    "Diversity selection needs embeddings to understand dataset structure. We'll compute them using FiftyOne Brain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute embeddings (takes a few minutes)\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"embeddings\",\n",
    "    brain_key=\"img_viz\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Embeddings computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZCore: Zero-Shot Coreset Selection\n",
    "\n",
    "ZCore scores each sample by iteratively:\n",
    "1. Sampling random points in embedding space\n",
    "2. Finding the nearest data point (coverage bonus)\n",
    "3. Penalizing nearby neighbors (redundancy penalty)\n",
    "\n",
    "The result: samples covering unique regions score high; redundant samples score low.\n",
    "\n",
    "> **Note:** This is a simplified reference implementation. It works well for datasets up to a few thousand samples. For larger datasets, use the optimized version at [github.com/voxel51/zcore](https://github.com/voxel51/zcore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zcore_score(embeddings, n_sample=10000, sample_dim=2, redund_nn=100, redund_exp=4, seed=42):\n",
    "    \"\"\"\n",
    "    Compute ZCore scores for coverage-based sample selection.\n",
    "    \n",
    "    Reference implementation from https://github.com/voxel51/zcore\n",
    "    For production use with large datasets, use the official package.\n",
    "    \n",
    "    Args:\n",
    "        embeddings: np.array of shape (n_samples, embedding_dim)\n",
    "        n_sample: Number of random samples to draw\n",
    "        sample_dim: Number of dimensions to sample at a time\n",
    "        redund_nn: Number of nearest neighbors for redundancy penalty\n",
    "        redund_exp: Exponent for distance-based redundancy penalty\n",
    "        seed: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "        Normalized scores (0-1) where higher = more valuable for labeling\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    n = len(embeddings)\n",
    "    n_dim = embeddings.shape[1]\n",
    "    \n",
    "    # Embedding statistics\n",
    "    emb_min = np.min(embeddings, axis=0)\n",
    "    emb_max = np.max(embeddings, axis=0)\n",
    "    emb_med = np.median(embeddings, axis=0)\n",
    "    \n",
    "    # Initialize scores\n",
    "    scores = np.random.uniform(0, 1, n)\n",
    "    \n",
    "    for i in range(n_sample):\n",
    "        if i % 2000 == 0:\n",
    "            print(f\"  ZCore progress: {i}/{n_sample}\")\n",
    "        \n",
    "        # Random embedding dimensions\n",
    "        dim = np.random.choice(n_dim, min(sample_dim, n_dim), replace=False)\n",
    "        \n",
    "        # Sample point using triangular distribution (biased toward median)\n",
    "        sample = np.random.triangular(emb_min[dim], emb_med[dim], emb_max[dim])\n",
    "        \n",
    "        # Coverage: find nearest sample to random point\n",
    "        embed_dist = np.sum(np.abs(embeddings[:, dim] - sample), axis=1)\n",
    "        idx = np.argmin(embed_dist)\n",
    "        scores[idx] += 1  # Reward coverage\n",
    "        \n",
    "        # Redundancy: penalize nearby neighbors\n",
    "        cover_sample = embeddings[idx, dim]\n",
    "        nn_dist = np.sum(np.abs(embeddings[:, dim] - cover_sample), axis=1)\n",
    "        nn = np.argsort(nn_dist)[1:]  # Exclude self\n",
    "        \n",
    "        if nn_dist[nn[0]] == 0:\n",
    "            # Exact duplicate\n",
    "            scores[nn[0]] -= 1\n",
    "        else:\n",
    "            # Distance-weighted penalty for neighbors\n",
    "            nn = nn[:redund_nn]\n",
    "            dist_penalty = 1 / (nn_dist[nn] ** redund_exp + 1e-8)\n",
    "            dist_penalty /= np.sum(dist_penalty)\n",
    "            scores[nn] -= dist_penalty\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores) + 1e-8)\n",
    "    \n",
    "    return scores.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for pool samples\n",
    "pool_samples = list(pool)\n",
    "embeddings = np.array([s.embeddings for s in pool_samples if s.embeddings is not None])\n",
    "valid_ids = [s.id for s in pool_samples if s.embeddings is not None]\n",
    "\n",
    "print(f\"Computing ZCore for {len(embeddings)} samples...\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ZCore scores\n",
    "# n_sample=5000 is fast for tutorials; increase for larger datasets\n",
    "scores = zcore_score(\n",
    "    embeddings,\n",
    "    n_sample=5000,\n",
    "    sample_dim=2,\n",
    "    redund_nn=50,\n",
    "    redund_exp=4,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\nZCore scores computed!\")\n",
    "print(f\"Score range: {scores.min():.3f} - {scores.max():.3f}\")\n",
    "print(f\"Mean: {scores.mean():.3f}, Std: {scores.std():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add ZCore scores to samples\n",
    "for sample_id, score in zip(valid_ids, scores):\n",
    "    sample = dataset[sample_id]\n",
    "    sample[\"zcore\"] = float(score)\n",
    "    sample.save()\n",
    "\n",
    "print(f\"Added 'zcore' field to {len(valid_ids)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Your First Batch\n",
    "\n",
    "**How many samples to label?**\n",
    "\n",
    "A good starting point:\n",
    "- **50-200 samples** for initial batch (1-2 hours of labeling)\n",
    "- Size subsequent batches based on your labeling throughput\n",
    "- Typical iteration: half-day to one-day of labeling per batch\n",
    "\n",
    "For this tutorial with ~130 pool samples, we'll select about 30 (roughly 25%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload pool to see new zcore field\n",
    "pool = dataset.load_saved_view(\"pool\")\n",
    "\n",
    "# Select top samples by ZCore score\n",
    "# Adjust batch_size based on your labeling capacity\n",
    "batch_size = min(30, int(0.25 * len(pool)))  # ~30 samples or 25% of pool\n",
    "batch_v0 = pool.sort_by(\"zcore\", reverse=True).limit(batch_size)\n",
    "\n",
    "print(f\"Selected {len(batch_v0)} samples for Batch 0\")\n",
    "print(f\"ZCore range of selected: {min(s.zcore for s in batch_v0):.3f} - {max(s.zcore for s in batch_v0):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the selection\n",
    "batch_v0.tag_samples(\"batch:v0\")\n",
    "batch_v0.tag_samples(\"to_annotate\")\n",
    "batch_v0.set_values(\"annotation_status\", [\"selected\"] * len(batch_v0))\n",
    "\n",
    "# Save as view\n",
    "dataset.save_view(\"batch_v0\", dataset.match_tags(\"batch:v0\"))\n",
    "\n",
    "print(f\"Tagged and saved view: batch_v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize in the App\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the App:\n",
    "1. Open the **Embeddings** panel to see the 2D projection\n",
    "2. Color by the `zcore` field to see the score distribution\n",
    "3. Filter by `batch:v0` tag to see your selection\n",
    "4. Verify high-ZCore samples are spread across clusters (good coverage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Diversity Sampling Beats Random\n",
    "\n",
    "| Method | What it does | Result |\n",
    "|--------|-------------|--------|\n",
    "| **Random** | Picks samples uniformly | Over-samples dense regions, misses rare cases |\n",
    "| **ZCore** | Balances coverage vs redundancy | Maximizes diversity, fewer wasted labels |\n",
    "\n",
    "Research shows diversity-based selection can significantly reduce labeling requirements while maintaining model performance. See the [ZCore paper](https://arxiv.org/pdf/2411.15349) for detailed benchmarks on ImageNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You selected a diverse batch using ZCore:\n",
    "- Computed embeddings to map images to vector space\n",
    "- Ran ZCore algorithm to score coverage vs redundancy\n",
    "- Selected top samples by ZCore score\n",
    "- Tagged as `batch:v0` and `to_annotate`\n",
    "\n",
    "**Artifacts:**\n",
    "- `embeddings` field on all samples\n",
    "- `zcore` field with selection scores\n",
    "- `batch_v0` saved view\n",
    "\n",
    "**Next:** Step 4 - Annotation + QA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

=== 04_ANNOTATION_QA.IPYNB ===
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annotation + QA\n",
    "\n",
    "Now we annotate the selected batch. This step covers:\n",
    "1. Setting up a consistent annotation schema\n",
    "2. **Actually labeling in the App** (not simulated)\n",
    "3. QA checks before training\n",
    "\n",
    "> **Time commitment:** Plan 1-2 minutes per sample for careful annotation. Start with 10-20 samples to get the feel, then continue or use the fast-forward option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "dataset = fo.load_dataset(\"annotation_tutorial\")\n",
    "batch_v0 = dataset.load_saved_view(\"batch_v0\")\n",
    "\n",
    "print(f\"Batch v0: {len(batch_v0)} samples to annotate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Your Schema\n",
    "\n",
    "Before labeling, define the rules. This prevents class drift and maintains consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define annotation schema\n",
    "LABEL_FIELD = \"human_labels\"  # Use this exact name in the App\n",
    "\n",
    "SCHEMA = {\n",
    "    \"classes\": [\n",
    "        \"person\", \"car\", \"truck\", \"bus\", \"motorcycle\", \"bicycle\",\n",
    "        \"dog\", \"cat\", \"bird\", \"horse\",\n",
    "        \"chair\", \"couch\", \"dining table\", \"tv\",\n",
    "        \"bottle\", \"cup\", \"bowl\",\n",
    "        \"other\"  # catch-all for edge cases\n",
    "    ],\n",
    "    \"field_name\": LABEL_FIELD\n",
    "}\n",
    "\n",
    "SCHEMA_CLASSES = set(SCHEMA[\"classes\"])\n",
    "\n",
    "# Store in dataset for reference\n",
    "dataset.info[\"annotation_schema\"] = SCHEMA\n",
    "dataset.save()\n",
    "\n",
    "print(f\"Schema defined: {len(SCHEMA['classes'])} classes\")\n",
    "print(f\"Target field: {LABEL_FIELD}\")\n",
    "print(f\"\\nWhen you create a field in the App, name it exactly: {LABEL_FIELD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate in the App\n",
    "\n",
    "**This is the real labeling step.** Open the App and annotate your selected samples.\n",
    "\n",
    "### Setup (one time)\n",
    "1. Launch the App with your batch\n",
    "2. Click a sample to open the modal\n",
    "3. Click the **Annotate** tab (pencil icon)\n",
    "4. Click **Schema** -> **Add Field** -> name it exactly `human_labels`\n",
    "5. Set type to **Detections** and add your classes\n",
    "\n",
    "### For each sample\n",
    "1. Review the image\n",
    "2. Click **Detection** button (square icon)\n",
    "3. Draw boxes around all objects of interest\n",
    "4. Assign the correct class to each box\n",
    "5. Move to the next sample\n",
    "\n",
    "### Tips\n",
    "- **Be consistent:** Same object = same class, every time\n",
    "- **Tight boxes:** As close as possible without cutting off the object\n",
    "- **Don't skip:** If it's ambiguous, label it \"other\" rather than skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App with your batch\n",
    "session = fo.launch_app(batch_v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop here and annotate samples\n",
    "\n",
    "Take 15-30 minutes to actually label some samples. This is the core skill.\n",
    "\n",
    "When you're done (or want to fast-forward), continue below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fast-Forward Option\n",
    "\n",
    "If you want to proceed without labeling everything manually, set `FAST_FORWARD = True` below. This copies `ground_truth` labels to `human_labels` to simulate completed annotation.\n",
    "\n",
    "> **Note:** In real projects, there's no shortcut. Label quality determines model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True ONLY if you want to skip manual annotation\n",
    "# Default is False - you should label samples yourself\n",
    "FAST_FORWARD = False\n",
    "\n",
    "if FAST_FORWARD:\n",
    "    print(\"Fast-forwarding: copying ground_truth to human_labels...\")\n",
    "    print(f\"Filtering to schema classes: {len(SCHEMA_CLASSES)} classes\")\n",
    "    \n",
    "    copied = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for sample in batch_v0:\n",
    "        if sample.ground_truth:\n",
    "            # Only copy detections that match our schema\n",
    "            human_dets = []\n",
    "            for det in sample.ground_truth.detections:\n",
    "                if det.label in SCHEMA_CLASSES:\n",
    "                    human_dets.append(fo.Detection(\n",
    "                        label=det.label,\n",
    "                        bounding_box=det.bounding_box,\n",
    "                    ))\n",
    "                    copied += 1\n",
    "                else:\n",
    "                    skipped += 1\n",
    "            sample[LABEL_FIELD] = fo.Detections(detections=human_dets)\n",
    "        else:\n",
    "            sample[LABEL_FIELD] = fo.Detections(detections=[])\n",
    "        sample.save()\n",
    "    \n",
    "    print(f\"Copied {copied} detections, skipped {skipped} (not in schema)\")\n",
    "else:\n",
    "    print(\"Using your manual annotations.\")\n",
    "    print(f\"Make sure you created the '{LABEL_FIELD}' field and labeled samples in the App!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark Annotated Samples\n",
    "\n",
    "**Important:** We only mark samples as \"annotated\" if they actually have labels. This prevents training on unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload to see changes\n",
    "dataset.reload()\n",
    "\n",
    "# Find samples that actually have labels\n",
    "batch_samples = dataset.match_tags(\"batch:v0\")\n",
    "\n",
    "if LABEL_FIELD in dataset.get_field_schema():\n",
    "    has_labels = batch_samples.match(F(f\"{LABEL_FIELD}.detections\").length() > 0)\n",
    "    no_labels = batch_samples.match(\n",
    "        (F(LABEL_FIELD) == None) | (F(f\"{LABEL_FIELD}.detections\").length() == 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch v0 status:\")\n",
    "    print(f\"  With labels: {len(has_labels)}\")\n",
    "    print(f\"  Without labels: {len(no_labels)}\")\n",
    "    \n",
    "    if len(has_labels) == 0:\n",
    "        print(f\"\\n>>> No samples have labels in '{LABEL_FIELD}'.\")\n",
    "        print(\">>> Either label some samples in the App, or set FAST_FORWARD = True above.\")\n",
    "    else:\n",
    "        # Tag ONLY samples that have labels as annotated\n",
    "        has_labels.untag_samples(\"to_annotate\")\n",
    "        has_labels.tag_samples(\"annotated:v0\")\n",
    "        has_labels.set_values(\"annotation_status\", [\"annotated\"] * len(has_labels))\n",
    "        \n",
    "        # Mark unlabeled samples as still needing annotation\n",
    "        if len(no_labels) > 0:\n",
    "            no_labels.set_values(\"annotation_status\", [\"pending\"] * len(no_labels))\n",
    "        \n",
    "        print(f\"\\nTagged {len(has_labels)} samples as 'annotated:v0'\")\n",
    "        if len(no_labels) > 0:\n",
    "            print(f\"{len(no_labels)} samples still need annotation.\")\n",
    "else:\n",
    "    print(f\"Field '{LABEL_FIELD}' not found. Create it in the App first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Checks\n",
    "\n",
    "Before training, verify label quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get annotated samples\n",
    "annotated = dataset.match_tags(\"annotated:v0\")\n",
    "\n",
    "if len(annotated) == 0:\n",
    "    print(\"No annotated samples yet. Complete the annotation step above first.\")\n",
    "else:\n",
    "    print(f\"QA Check 1: Label coverage\")\n",
    "    print(f\"  Annotated samples: {len(annotated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2: Class distribution\n",
    "from collections import Counter\n",
    "\n",
    "if len(annotated) > 0:\n",
    "    all_labels = []\n",
    "    for sample in annotated:\n",
    "        if sample[LABEL_FIELD]:\n",
    "            all_labels.extend([d.label for d in sample[LABEL_FIELD].detections])\n",
    "\n",
    "    print(f\"QA Check 2: Class distribution ({len(all_labels)} total detections)\")\n",
    "    for label, count in Counter(all_labels).most_common(10):\n",
    "        print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: Unexpected classes\n",
    "if len(annotated) > 0 and len(all_labels) > 0:\n",
    "    actual = set(all_labels)\n",
    "    unexpected = actual - SCHEMA_CLASSES\n",
    "\n",
    "    if unexpected:\n",
    "        print(f\"QA Check 3: Unexpected classes found: {unexpected}\")\n",
    "        print(\"   These don't match your schema. Review before training.\")\n",
    "    else:\n",
    "        print(f\"QA Check 3: All classes match schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You annotated Batch v0:\n",
    "- Defined a schema for consistency\n",
    "- Labeled samples in the App (or fast-forwarded)\n",
    "- **Only samples with actual labels** were marked as annotated\n",
    "- Ran QA checks: coverage, class distribution, schema compliance\n",
    "\n",
    "**Artifacts:**\n",
    "- `human_labels` field with your annotations\n",
    "- `annotated:v0` tag on samples that have labels\n",
    "\n",
    "**Next:** Step 5 - Train + Evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

=== 05_TRAIN_EVALUATE.IPYNB ===
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Train + Evaluate\n",
    "\n",
    "Train a detector on your labeled data and evaluate it properly. Understanding **where** the model fails tells you what to label next.\n",
    "\n",
    "> **Note:** We train on `human_labels` and evaluate against `human_labels` on the val set. For this tutorial, we copy ground_truth to human_labels for val samples (filtered to schema classes) so we have labels to evaluate against. In production, you'd label the val set too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import os\n",
    "\n",
    "LABEL_FIELD = \"human_labels\"\n",
    "\n",
    "dataset = fo.load_dataset(\"annotation_tutorial\")\n",
    "\n",
    "# Get schema classes from dataset info\n",
    "if \"annotation_schema\" in dataset.info:\n",
    "    SCHEMA_CLASSES = set(dataset.info[\"annotation_schema\"][\"classes\"])\n",
    "else:\n",
    "    # Fallback\n",
    "    SCHEMA_CLASSES = {\n",
    "        \"person\", \"car\", \"truck\", \"bus\", \"motorcycle\", \"bicycle\",\n",
    "        \"dog\", \"cat\", \"bird\", \"horse\",\n",
    "        \"chair\", \"couch\", \"dining table\", \"tv\",\n",
    "        \"bottle\", \"cup\", \"bowl\", \"other\"\n",
    "    }\n",
    "\n",
    "print(f\"Schema classes: {len(SCHEMA_CLASSES)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Training Data\n",
    "\n",
    "**Important:** We only train on samples that actually have labels, not just samples tagged as \"annotated\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data: annotated samples WITH actual labels\n",
    "annotated = dataset.match_tags(\"annotated:v0\")\n",
    "\n",
    "# Filter to only samples with detections\n",
    "train_view = annotated.match(F(f\"{LABEL_FIELD}.detections\").length() > 0)\n",
    "\n",
    "# Validation data\n",
    "val_view = dataset.load_saved_view(\"val_set\")\n",
    "\n",
    "print(f\"Annotated samples: {len(annotated)}\")\n",
    "print(f\"Training samples (with labels): {len(train_view)}\")\n",
    "print(f\"Validation samples: {len(val_view)}\")\n",
    "\n",
    "if len(train_view) == 0:\n",
    "    print(\"\\n>>> No training samples with labels. Complete Step 4 first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation, we need human_labels on val set\n",
    "# In production, you'd label these. For tutorial, we copy ground_truth\n",
    "# FILTERED to schema classes for consistency\n",
    "\n",
    "copied_count = 0\n",
    "skipped_count = 0\n",
    "\n",
    "for sample in val_view:\n",
    "    if sample.ground_truth and not sample[LABEL_FIELD]:\n",
    "        filtered_dets = []\n",
    "        for d in sample.ground_truth.detections:\n",
    "            if d.label in SCHEMA_CLASSES:\n",
    "                filtered_dets.append(fo.Detection(\n",
    "                    label=d.label,\n",
    "                    bounding_box=d.bounding_box\n",
    "                ))\n",
    "                copied_count += 1\n",
    "            else:\n",
    "                skipped_count += 1\n",
    "        sample[LABEL_FIELD] = fo.Detections(detections=filtered_dets)\n",
    "        sample.save()\n",
    "\n",
    "print(f\"Val set prepared: {copied_count} detections copied, {skipped_count} skipped (not in schema)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(train_view) == 0:\n",
    "    raise ValueError(\"No training samples. Complete Step 4 first.\")\n",
    "\n",
    "# Get classes from training data\n",
    "classes = train_view.distinct(f\"{LABEL_FIELD}.detections.label\")\n",
    "print(f\"Classes in training data: {classes}\")\n",
    "\n",
    "export_dir = \"/tmp/annotation_tutorial_yolo\"\n",
    "os.makedirs(export_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training data\n",
    "train_view.export(\n",
    "    export_dir=os.path.join(export_dir, \"train\"),\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=LABEL_FIELD,\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "# Export validation data\n",
    "val_view.export(\n",
    "    export_dir=os.path.join(export_dir, \"val\"),\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=LABEL_FIELD,\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "print(f\"Exported {len(train_view)} train, {len(val_view)} val samples to {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create YAML config\n",
    "yaml_content = f\"\"\"path: {export_dir}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "names:\n",
    "\"\"\"\n",
    "for i, cls in enumerate(classes):\n",
    "    yaml_content += f\"  {i}: {cls}\\n\"\n",
    "\n",
    "yaml_path = os.path.join(export_dir, \"dataset.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"Created {yaml_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train YOLOv8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Train (small model, few epochs for demo)\n",
    "model = YOLO('yolov8n.pt')\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=10,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    name='tutorial_v0',\n",
    "    project='/tmp/yolo_tutorial'\n",
    ")\n",
    "\n",
    "model_path = '/tmp/yolo_tutorial/tutorial_v0/weights/best.pt'\n",
    "print(f\"Model saved: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = YOLO(model_path)\n",
    "\n",
    "# Run inference on val set\n",
    "for sample in val_view:\n",
    "    results = model(sample.filepath, verbose=False)[0]\n",
    "    \n",
    "    detections = []\n",
    "    if results.boxes is not None:\n",
    "        for box in results.boxes:\n",
    "            x1, y1, x2, y2 = box.xyxyn[0].tolist()\n",
    "            conf = box.conf[0].item()\n",
    "            cls_idx = int(box.cls[0].item())\n",
    "            label = classes[cls_idx] if cls_idx < len(classes) else f\"class_{cls_idx}\"\n",
    "            \n",
    "            detections.append(fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[x1, y1, x2-x1, y2-y1],\n",
    "                confidence=conf\n",
    "            ))\n",
    "    \n",
    "    sample[\"predictions\"] = fo.Detections(detections=detections)\n",
    "    sample.save()\n",
    "\n",
    "print(f\"Added predictions to {len(val_view)} val samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "results = val_view.evaluate_detections(\n",
    "    \"predictions\",\n",
    "    gt_field=LABEL_FIELD,\n",
    "    eval_key=\"eval_v0\",\n",
    "    compute_mAP=True\n",
    ")\n",
    "\n",
    "print(f\"mAP: {results.mAP():.3f}\")\n",
    "results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Failures\n",
    "\n",
    "Understanding failures is more important than the mAP number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-FN samples (model missed objects)\n",
    "high_fn = val_view.sort_by(\"eval_v0_fn\", reverse=True).limit(10)\n",
    "high_fn.tag_samples(\"failure:high_fn\")\n",
    "\n",
    "# Find high-FP samples (model hallucinated)\n",
    "high_fp = val_view.sort_by(\"eval_v0_fp\", reverse=True).limit(10)\n",
    "high_fp.tag_samples(\"failure:high_fp\")\n",
    "\n",
    "print(f\"Tagged {len(high_fn)} high-FN samples\")\n",
    "print(f\"Tagged {len(high_fp)} high-FP samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View failures in App\n",
    "session = fo.launch_app(val_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the App:\n",
    "1. Filter by `failure:high_fn` to see samples where model missed objects\n",
    "2. Filter by `failure:high_fp` to see samples where model hallucinated\n",
    "3. Look for patterns: specific classes? lighting conditions? object sizes?\n",
    "\n",
    "These patterns tell you what to label next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save evaluation info\n",
    "dataset.info[\"eval_v0\"] = {\n",
    "    \"mAP\": results.mAP(),\n",
    "    \"train_samples\": len(train_view),\n",
    "    \"val_samples\": len(val_view),\n",
    "    \"model_path\": model_path\n",
    "}\n",
    "dataset.save()\n",
    "\n",
    "# Save failure view\n",
    "failures = val_view.match_tags([\"failure:high_fn\", \"failure:high_fp\"])\n",
    "dataset.save_view(\"eval_v0_failures\", failures)\n",
    "\n",
    "print(f\"Saved {len(failures)} failure samples to view 'eval_v0_failures'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You trained and evaluated a model:\n",
    "- Filtered to samples with actual labels (not just tagged as annotated)\n",
    "- Exported in YOLO format\n",
    "- Trained YOLOv8n for 10 epochs\n",
    "- Evaluated with FiftyOne: mAP + per-sample FP/FN\n",
    "- Tagged failure cases for next iteration\n",
    "\n",
    "**Key insight:** The failure tags tell you what to label next.\n",
    "\n",
    "**Next:** Step 6 - Iteration Loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

=== 06_ITERATION.IPYNB ===
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Iteration Loop\n",
    "\n",
    "Now you have a trained model and know where it fails. This step shows how to:\n",
    "1. Run a **Golden QA check** to detect annotation drift\n",
    "2. Select the **next batch** using a hybrid strategy\n",
    "\n",
    "The hybrid strategy balances:\n",
    "- **30% Coverage** - Diversity sampling to avoid tunnel vision\n",
    "- **70% Targeted** - Samples similar to failures\n",
    "\n",
    "This balance is critical. Only chasing failures creates a model that's great at edge cases and terrible at normal cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "\n",
    "LABEL_FIELD = \"human_labels\"\n",
    "\n",
    "dataset = fo.load_dataset(\"annotation_tutorial\")\n",
    "\n",
    "# Get schema classes\n",
    "if \"annotation_schema\" in dataset.info:\n",
    "    SCHEMA_CLASSES = set(dataset.info[\"annotation_schema\"][\"classes\"])\n",
    "else:\n",
    "    SCHEMA_CLASSES = {\n",
    "        \"person\", \"car\", \"truck\", \"bus\", \"motorcycle\", \"bicycle\",\n",
    "        \"dog\", \"cat\", \"bird\", \"horse\",\n",
    "        \"chair\", \"couch\", \"dining table\", \"tv\",\n",
    "        \"bottle\", \"cup\", \"bowl\", \"other\"\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Golden QA Check\n",
    "\n",
    "Before selecting the next batch, verify annotation quality hasn't drifted. The golden set is a small, carefully reviewed sample we check each iteration.\n",
    "\n",
    "**What to look for:**\n",
    "- Label count distribution staying stable\n",
    "- No unexpected empty samples\n",
    "- Class distribution roughly matching earlier rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load golden QA set\n",
    "golden = dataset.load_saved_view(\"golden_qa\")\n",
    "\n",
    "# For tutorial, copy ground_truth to human_labels if not present\n",
    "# FILTERED to schema classes for consistency\n",
    "for sample in golden:\n",
    "    if sample.ground_truth and not sample[LABEL_FIELD]:\n",
    "        filtered_dets = [\n",
    "            fo.Detection(label=d.label, bounding_box=d.bounding_box)\n",
    "            for d in sample.ground_truth.detections\n",
    "            if d.label in SCHEMA_CLASSES\n",
    "        ]\n",
    "        sample[LABEL_FIELD] = fo.Detections(detections=filtered_dets)\n",
    "        sample.save()\n",
    "\n",
    "print(f\"Golden QA set: {len(golden)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Golden QA Check: Compute baseline stats\n",
    "golden_stats = {\n",
    "    \"total_samples\": len(golden),\n",
    "    \"samples_with_labels\": 0,\n",
    "    \"total_detections\": 0,\n",
    "    \"class_counts\": Counter()\n",
    "}\n",
    "\n",
    "for sample in golden:\n",
    "    if sample[LABEL_FIELD] and len(sample[LABEL_FIELD].detections) > 0:\n",
    "        golden_stats[\"samples_with_labels\"] += 1\n",
    "        golden_stats[\"total_detections\"] += len(sample[LABEL_FIELD].detections)\n",
    "        for det in sample[LABEL_FIELD].detections:\n",
    "            golden_stats[\"class_counts\"][det.label] += 1\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"GOLDEN QA CHECK\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Samples with labels: {golden_stats['samples_with_labels']}/{golden_stats['total_samples']}\")\n",
    "print(f\"Total detections: {golden_stats['total_detections']}\")\n",
    "print(f\"Avg detections/sample: {golden_stats['total_detections']/max(1,golden_stats['samples_with_labels']):.1f}\")\n",
    "print(f\"\\nTop classes:\")\n",
    "for cls, count in golden_stats[\"class_counts\"].most_common(5):\n",
    "    print(f\"  {cls}: {count}\")\n",
    "print(\"=\" * 40)\n",
    "print(\"\\nIf these numbers change unexpectedly between iterations,\")\n",
    "print(\"investigate annotation consistency before continuing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store golden stats for comparison in future iterations\n",
    "if \"golden_qa_history\" not in dataset.info:\n",
    "    dataset.info[\"golden_qa_history\"] = []\n",
    "\n",
    "dataset.info[\"golden_qa_history\"].append({\n",
    "    \"iteration\": len(dataset.info[\"golden_qa_history\"]),\n",
    "    \"samples_with_labels\": golden_stats[\"samples_with_labels\"],\n",
    "    \"total_detections\": golden_stats[\"total_detections\"],\n",
    "    \"top_classes\": dict(golden_stats[\"class_counts\"].most_common(5))\n",
    "})\n",
    "dataset.save()\n",
    "\n",
    "print(f\"Saved golden QA stats (iteration {len(dataset.info['golden_qa_history'])-1})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Next Batch Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get truly unlabeled samples from pool\n",
    "# This excludes: annotated, selected for batch, pending annotation\n",
    "pool = dataset.load_saved_view(\"pool\")\n",
    "\n",
    "# Use annotation_status to find truly unlabeled samples\n",
    "remaining = pool.match(F(\"annotation_status\") == \"unlabeled\")\n",
    "\n",
    "# Get failure samples from evaluation\n",
    "try:\n",
    "    failures = dataset.load_saved_view(\"eval_v0_failures\")\n",
    "    print(f\"Failure samples: {len(failures)}\")\n",
    "except:\n",
    "    failures = dataset.limit(0)  # Empty view\n",
    "    print(\"No failure view found. Run Step 5 first, or continue with coverage-only selection.\")\n",
    "\n",
    "print(f\"Remaining unlabeled pool: {len(remaining)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Acquisition Budget\n",
    "\n",
    "**Batch sizing guidance:**\n",
    "- Size batches to your labeling capacity (e.g., half-day to one-day of work)\n",
    "- For this tutorial, we'll select ~20% of remaining pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select batch size based on remaining pool\n",
    "# Minimum 10 samples, or 20% of remaining\n",
    "batch_size = max(10, int(0.20 * len(remaining)))\n",
    "\n",
    "# Split: 30% coverage (ZCore), 70% targeted\n",
    "coverage_budget = int(0.30 * batch_size)\n",
    "targeted_budget = batch_size - coverage_budget\n",
    "\n",
    "print(f\"Batch v1 budget: {batch_size} samples\")\n",
    "print(f\"  Coverage (diversity): {coverage_budget} (30%)\")\n",
    "print(f\"  Targeted (failures): {targeted_budget} (70%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Coverage Selection (30%)\n",
    "\n",
    "Use ZCore scores computed in Step 3 to select diverse samples from remaining pool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get samples with ZCore scores from remaining pool\n",
    "remaining_with_scores = remaining.match(F(\"zcore\") != None)\n",
    "\n",
    "if len(remaining_with_scores) == 0:\n",
    "    print(\"No ZCore scores found in remaining pool. Run Step 3 first.\")\n",
    "    coverage_ids = set()\n",
    "else:\n",
    "    # Select top by ZCore (already computed in Step 3)\n",
    "    coverage_samples = remaining_with_scores.sort_by(\"zcore\", reverse=True).limit(coverage_budget)\n",
    "    coverage_ids = set(s.id for s in coverage_samples)\n",
    "    print(f\"Coverage selection (ZCore): {len(coverage_ids)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Targeted Selection (70%)\n",
    "\n",
    "Find samples similar to failures using embedding-based neighbor search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_neighbors(query_embs, pool_embs, pool_ids, n_per_query=3):\n",
    "    \"\"\"Find nearest neighbors in embedding space.\"\"\"\n",
    "    if len(query_embs) == 0 or len(pool_embs) == 0:\n",
    "        return []\n",
    "    \n",
    "    sims = cosine_similarity(query_embs, pool_embs)\n",
    "    neighbor_ids = set()\n",
    "    \n",
    "    for sim_row in sims:\n",
    "        top_idx = np.argsort(sim_row)[-n_per_query:]\n",
    "        for idx in top_idx:\n",
    "            neighbor_ids.add(pool_ids[idx])\n",
    "    \n",
    "    return list(neighbor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for remaining samples\n",
    "remaining_samples = list(remaining)\n",
    "remaining_embs = np.array([s.embeddings for s in remaining_samples if s.embeddings is not None])\n",
    "remaining_ids = [s.id for s in remaining_samples if s.embeddings is not None]\n",
    "\n",
    "if len(failures) > 0 and len(remaining_embs) > 0:\n",
    "    failure_embs = np.array([s.embeddings for s in failures if s.embeddings is not None])\n",
    "    print(f\"Finding neighbors of {len(failure_embs)} failure samples...\")\n",
    "    \n",
    "    # Find neighbors (excluding already-selected coverage samples)\n",
    "    failure_neighbors = find_neighbors(failure_embs, remaining_embs, remaining_ids, n_per_query=5)\n",
    "    targeted_ids = [sid for sid in failure_neighbors if sid not in coverage_ids][:targeted_budget]\n",
    "    print(f\"Targeted selection: {len(targeted_ids)} samples\")\n",
    "else:\n",
    "    print(\"No failures to target or no embeddings. Using coverage-only selection.\")\n",
    "    # Fall back to more coverage samples\n",
    "    if len(remaining_with_scores) > coverage_budget:\n",
    "        extra_coverage = remaining_with_scores.sort_by(\"zcore\", reverse=True).skip(coverage_budget).limit(targeted_budget)\n",
    "        targeted_ids = [s.id for s in extra_coverage if s.id not in coverage_ids]\n",
    "    else:\n",
    "        targeted_ids = []\n",
    "    print(f\"Additional coverage selection: {len(targeted_ids)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Tag Batch v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine selections\n",
    "batch_v1_ids = list(coverage_ids) + targeted_ids\n",
    "\n",
    "if len(batch_v1_ids) == 0:\n",
    "    print(\"No samples selected. Check that Steps 3 and 5 completed successfully.\")\n",
    "else:\n",
    "    batch_v1 = dataset.select(batch_v1_ids)\n",
    "\n",
    "    # Tag\n",
    "    batch_v1.tag_samples(\"batch:v1\")\n",
    "    batch_v1.tag_samples(\"to_annotate\")\n",
    "    batch_v1.set_values(\"annotation_status\", [\"selected\"] * len(batch_v1))\n",
    "\n",
    "    # Track source for analysis\n",
    "    if len(coverage_ids) > 0:\n",
    "        dataset.select(list(coverage_ids)).tag_samples(\"source:coverage\")\n",
    "    if len(targeted_ids) > 0:\n",
    "        dataset.select(targeted_ids).tag_samples(\"source:targeted\")\n",
    "\n",
    "    # Save view\n",
    "    dataset.save_view(\"batch_v1\", dataset.match_tags(\"batch:v1\"))\n",
    "\n",
    "    print(f\"\\nBatch v1: {len(batch_v1)} samples\")\n",
    "    print(f\"  Coverage: {len(coverage_ids)}\")\n",
    "    print(f\"  Targeted: {len(targeted_ids)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Loop\n",
    "\n",
    "You now have the full iteration recipe:\n",
    "\n",
    "```\n",
    "1. Run Golden QA check (detect drift)\n",
    "2. Annotate the current batch (Step 4 workflow)\n",
    "3. Train on all annotated data (Step 5)\n",
    "4. Evaluate on val set, tag failures\n",
    "5. Select next batch: 30% coverage + 70% targeted\n",
    "6. Repeat until stopping criteria\n",
    "```\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "Stop when:\n",
    "- Gains per labeled sample flatten (diminishing returns)\n",
    "- Remaining failures are mostly label ambiguity\n",
    "- Val metrics hit your target threshold\n",
    "\n",
    "### The 30% Coverage Rule\n",
    "\n",
    "**Don't skip the coverage budget.** Only chasing failures leads to:\n",
    "- Overfitting to edge cases\n",
    "- Distorted class priors\n",
    "- Models that fail on \"normal\" inputs\n",
    "\n",
    "Coverage keeps you honest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Progress summary\n",
    "pool = dataset.load_saved_view(\"pool\")\n",
    "total_pool = len(pool)\n",
    "\n",
    "annotated = len(dataset.match_tags(\"annotated:v0\"))\n",
    "selected_v1 = len(dataset.match_tags(\"batch:v1\"))\n",
    "still_unlabeled = len(pool.match(F(\"annotation_status\") == \"unlabeled\"))\n",
    "\n",
    "print(\"=\" * 40)\n",
    "print(\"ANNOTATION PROGRESS\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"Pool total:      {total_pool}\")\n",
    "print(f\"Annotated (v0):  {annotated} ({100*annotated/total_pool:.0f}%)\")\n",
    "print(f\"Selected (v1):   {selected_v1} ({100*selected_v1/total_pool:.0f}%)\")\n",
    "print(f\"Still unlabeled: {still_unlabeled} ({100*still_unlabeled/total_pool:.0f}%)\")\n",
    "print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You implemented the iteration loop:\n",
    "- **Golden QA check** to detect annotation drift\n",
    "- **Hybrid acquisition**: 30% coverage + 70% targeted\n",
    "- Tagged `batch:v1` ready for annotation\n",
    "\n",
    "**Why this works:** \n",
    "- Coverage prevents overfitting to edge cases\n",
    "- Targeting fixes known failures\n",
    "- Golden QA catches annotation drift early\n",
    "- The combination improves faster than either strategy alone\n",
    "\n",
    "**Your turn:** Repeat Steps 4-6 with batch_v1, then batch_v2, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

=== SUMMARY.RST ===
Summary: What You've Learned
============================

.. default-role:: code

You've completed the Human Annotation Guide. Here's what you can now do.

Quickstart Track
----------------

You learned the basics of in-app annotation:

- Clone datasets to keep originals clean
- Enter Annotate mode (sample modal -> Annotate tab)
- Create label fields with enforced schemas
- Draw bounding boxes and assign classes
- Verify labels saved correctly
- Export labeled data for training

Full Loop Track
---------------

You built a complete data-centric annotation pipeline:

**Step 2: Setup Splits**
   Created test (frozen), val (iteration), golden (QA), and pool splits. These prevent evaluation contamination.

**Step 3: Smart Selection**
   Used ZCore diversity scoring to select high-coverage samples. Better than random.

**Step 4: Annotation + QA**
   Labeled samples with schema enforcement. Only samples with actual labels get marked as annotated.

**Step 5: Train + Evaluate**
   Trained YOLOv8, evaluated on val set, tagged FP/FN failures for targeting.

**Step 6: Iteration**
   Ran Golden QA check, then selected next batch using hybrid strategy: 30% coverage + 70% targeted.

Key Takeaways
-------------

1. **Splits are non-negotiable.** Without frozen test and golden QA, your metrics lie.

2. **Label smarter, not harder.** Diversity sampling + failure targeting beats random selection.

3. **30% coverage budget matters.** Only chasing failures creates a model that fails on normal cases.

4. **QA before training.** Golden QA checks catch annotation drift early.

5. **Understand your failures.** FP/FN analysis tells you what to label next.

When to Use What
----------------

**In-app annotation is good for:**

- Small to medium tasks (tens to hundreds of samples)
- Quick corrections and QA passes
- Prototyping label schemas
- Single annotator workflows
- Tight model-labeling feedback loops

**Use external tools (CVAT, Label Studio) when:**

- High-volume annotation with teams
- Role-based review workflows
- Audit trails and agreement metrics needed

FiftyOne integrates with external tools via the :ref:`annotation API <fiftyone-annotation>`.

What's Next
-----------

- **Apply to your data** - Use this workflow on your production datasets
- **Scale with teams** - The schema and QA workflow supports multiple annotators
- **Explore plugins** - Check `@voxel51/brain` for advanced selection operators

Resources
---------

* `FiftyOne Brain - Embeddings <../../user_guide/brain.html>`_
* `FiftyOne Evaluation API <../../user_guide/evaluation.html>`_
* `External Annotation Integration <../../user_guide/annotation.html>`_
* `ZCore Repository <https://github.com/voxel51/zcore>`_
* `YOLOv8 Documentation <https://docs.ultralytics.com/>`_

Feedback
--------

Questions or suggestions? Reach us at `support@voxel51.com` or join our `Discord <https://community.voxel51.com>`_.
