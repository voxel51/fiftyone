{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Train Baseline Detector + Evaluate\n",
    "\n",
    "Now we train a model on our human labels and evaluate it properly. This step teaches you to:\n",
    "1. Export data in YOLO format for training\n",
    "2. Train YOLOv8 on your annotated batch\n",
    "3. Run inference and evaluate with FiftyOne\n",
    "4. Analyze failure modes: FP, FN, class confusion, localization errors\n",
    "\n",
    "**Why This Matters**: Evaluation isn't just a number. Understanding *where* and *why* your model fails tells you what to label next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Load dataset\n",
    "dataset = fo.load_dataset(\"kitti_annotation_tutorial\")\n",
    "\n",
    "# Get annotated samples (our training data)\n",
    "train_view = dataset.match_tags(\"annotated:v0\")\n",
    "\n",
    "# Get validation data (from pool, not yet annotated)\n",
    "# We'll use a portion of remaining pool for validation\n",
    "pool_remaining = dataset.match_tags(\"split:pool\").match(F(\"annotation_status\") != \"annotated\")\n",
    "\n",
    "print(f\"Training samples (annotated): {len(train_view)}\")\n",
    "print(f\"Pool remaining: {len(pool_remaining)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation, we'll use ground_truth on a held-out portion\n",
    "# Note: In production, you'd have human labels on validation too\n",
    "# For this tutorial, we use a subset of remaining pool with ground_truth\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "val_ids = random.sample(list(pool_remaining.values(\"id\")), min(50, len(pool_remaining)))\n",
    "val_view = dataset.select(val_ids)\n",
    "val_view.tag_samples(\"split:val_v0\")\n",
    "\n",
    "print(f\"Validation samples: {len(val_view)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data for YOLOv8 Training\n",
    "\n",
    "YOLOv8 expects data in a specific format. FiftyOne makes export easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create export directory\n",
    "export_dir = \"/tmp/kitti_yolo_v0\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "# Get unique classes from human_labels\n",
    "classes = train_view.distinct(\"human_labels.detections.label\")\n",
    "print(f\"Classes for training: {classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export training data in YOLOv5 format (compatible with YOLOv8)\n",
    "train_view.export(\n",
    "    export_dir=os.path.join(export_dir, \"train\"),\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=\"human_labels\",\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "print(f\"Exported training data to {export_dir}/train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export validation data (using ground_truth for now)\n",
    "val_view.export(\n",
    "    export_dir=os.path.join(export_dir, \"val\"),\n",
    "    dataset_type=fo.types.YOLOv5Dataset,\n",
    "    label_field=\"ground_truth\",\n",
    "    classes=classes,\n",
    ")\n",
    "\n",
    "print(f\"Exported validation data to {export_dir}/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the YAML config file for YOLOv8\n",
    "yaml_content = f\"\"\"path: {export_dir}\n",
    "train: train/images\n",
    "val: val/images\n",
    "\n",
    "names:\n",
    "\"\"\"\n",
    "\n",
    "for i, cls in enumerate(classes):\n",
    "    yaml_content += f\"  {i}: {cls}\\n\"\n",
    "\n",
    "yaml_path = os.path.join(export_dir, \"dataset.yaml\")\n",
    "with open(yaml_path, \"w\") as f:\n",
    "    f.write(yaml_content)\n",
    "\n",
    "print(f\"Created {yaml_path}\")\n",
    "print(\"\\nYAML content:\")\n",
    "print(yaml_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train YOLOv8\n",
    "\n",
    "We'll train a small YOLOv8n model for speed. In production, use larger models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "# Train on our data\n",
    "# Note: epochs=10 is just for demo; use more epochs for real training\n",
    "results = model.train(\n",
    "    data=yaml_path,\n",
    "    epochs=10,\n",
    "    imgsz=640,\n",
    "    batch=8,\n",
    "    name='kitti_v0',\n",
    "    project='/tmp/yolo_runs'\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model path\n",
    "best_model_path = '/tmp/yolo_runs/kitti_v0/weights/best.pt'\n",
    "print(f\"Best model saved at: {best_model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Inference on Validation Set\n",
    "\n",
    "Now we add predictions to our FiftyOne dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model = YOLO(best_model_path)\n",
    "\n",
    "# Get filepaths for inference\n",
    "filepaths = val_view.values(\"filepath\")\n",
    "print(f\"Running inference on {len(filepaths)} validation images...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference and add predictions to FiftyOne\n",
    "for sample in val_view:\n",
    "    # Run inference\n",
    "    results = model(sample.filepath, verbose=False)[0]\n",
    "    \n",
    "    # Convert to FiftyOne detections\n",
    "    detections = []\n",
    "    if results.boxes is not None:\n",
    "        for box in results.boxes:\n",
    "            # Get normalized coordinates\n",
    "            x1, y1, x2, y2 = box.xyxyn[0].tolist()\n",
    "            conf = box.conf[0].item()\n",
    "            cls_idx = int(box.cls[0].item())\n",
    "            label = classes[cls_idx] if cls_idx < len(classes) else f\"class_{cls_idx}\"\n",
    "            \n",
    "            # Convert to FiftyOne format [x, y, w, h]\n",
    "            det = fo.Detection(\n",
    "                label=label,\n",
    "                bounding_box=[x1, y1, x2-x1, y2-y1],\n",
    "                confidence=conf\n",
    "            )\n",
    "            detections.append(det)\n",
    "    \n",
    "    sample[\"predictions_v0\"] = fo.Detections(detections=detections)\n",
    "    sample.save()\n",
    "\n",
    "print(f\"Added predictions_v0 to {len(val_view)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with FiftyOne\n",
    "\n",
    "FiftyOne's evaluation computes mAP and provides per-sample TP/FP/FN counts for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run evaluation\n",
    "eval_results = val_view.evaluate_detections(\n",
    "    \"predictions_v0\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"eval_v0\",\n",
    "    compute_mAP=True\n",
    ")\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(f\"  mAP: {eval_results.mAP():.3f}\")\n",
    "print(f\"  mAP@50: {eval_results.mAP(iou=0.5):.3f}\" if hasattr(eval_results, 'mAP') else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print per-class metrics\n",
    "eval_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Failure Modes\n",
    "\n",
    "Now the important part: understanding *where* the model fails. We'll analyze:\n",
    "1. **False Negatives (FN)**: Objects the model missed\n",
    "2. **False Positives (FP)**: Detections that don't match ground truth\n",
    "3. **Class Confusion**: Correct localization but wrong class\n",
    "4. **Localization Errors**: Right class but poor IoU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the App with evaluation results\n",
    "session = fo.launch_app(val_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-FN samples (model missed many objects)\n",
    "high_fn_view = val_view.sort_by(\"eval_v0_fn\", reverse=True).limit(10)\n",
    "\n",
    "print(\"Top 10 samples by False Negatives:\")\n",
    "for sample in high_fn_view:\n",
    "    fn_count = sample.eval_v0_fn if hasattr(sample, 'eval_v0_fn') else 0\n",
    "    print(f\"  {sample.filepath.split('/')[-1]}: {fn_count} FN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find high-FP samples (model hallucinated detections)\n",
    "high_fp_view = val_view.sort_by(\"eval_v0_fp\", reverse=True).limit(10)\n",
    "\n",
    "print(\"\\nTop 10 samples by False Positives:\")\n",
    "for sample in high_fp_view:\n",
    "    fp_count = sample.eval_v0_fp if hasattr(sample, 'eval_v0_fp') else 0\n",
    "    print(f\"  {sample.filepath.split('/')[-1]}: {fp_count} FP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix analysis\n",
    "confusion = eval_results.confusion_matrix()\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "confusion.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "confusion.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tag Failure Cases for Next Iteration\n",
    "\n",
    "We'll tag samples that represent different failure modes. These will guide our next batch selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag samples with high FN (recall issues)\n",
    "fn_threshold = 3  # More than 3 missed objects\n",
    "high_fn_samples = val_view.match(F(\"eval_v0_fn\") > fn_threshold)\n",
    "high_fn_samples.tag_samples(\"failure:high_fn\")\n",
    "print(f\"Tagged {len(high_fn_samples)} samples with 'failure:high_fn'\")\n",
    "\n",
    "# Tag samples with high FP (precision issues)\n",
    "fp_threshold = 3\n",
    "high_fp_samples = val_view.match(F(\"eval_v0_fp\") > fp_threshold)\n",
    "high_fp_samples.tag_samples(\"failure:high_fp\")\n",
    "print(f\"Tagged {len(high_fp_samples)} samples with 'failure:high_fp'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze failures by class\n",
    "print(\"\\nPer-class failure analysis:\")\n",
    "for cls in classes:\n",
    "    cls_gt = val_view.filter_labels(\"ground_truth\", F(\"label\") == cls)\n",
    "    total_gt = sum(len(s.ground_truth.detections) for s in cls_gt if s.ground_truth)\n",
    "    \n",
    "    cls_pred = val_view.filter_labels(\"predictions_v0\", F(\"label\") == cls)\n",
    "    total_pred = sum(len(s.predictions_v0.detections) for s in cls_pred if s.predictions_v0)\n",
    "    \n",
    "    print(f\"  {cls}: GT={total_gt}, Pred={total_pred}, Diff={total_pred - total_gt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Evaluation Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a view of failure cases\n",
    "failure_view = val_view.match_tags([\"failure:high_fn\", \"failure:high_fp\"])\n",
    "dataset.save_view(\"eval_v0_failures\", failure_view)\n",
    "\n",
    "print(f\"Saved view 'eval_v0_failures' with {len(failure_view)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store evaluation metrics in dataset info\n",
    "dataset.info[\"eval_v0\"] = {\n",
    "    \"mAP\": eval_results.mAP(),\n",
    "    \"train_samples\": len(train_view),\n",
    "    \"val_samples\": len(val_view),\n",
    "    \"model_path\": best_model_path\n",
    "}\n",
    "dataset.save()\n",
    "\n",
    "print(\"Evaluation metrics saved to dataset.info['eval_v0']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this step, you:\n",
    "\n",
    "1. **Exported data for YOLOv8** - Converted FiftyOne dataset to YOLO format\n",
    "2. **Trained a baseline model** - YOLOv8n on your annotated batch\n",
    "3. **Ran inference** - Added `predictions_v0` to validation samples\n",
    "4. **Evaluated thoroughly**:\n",
    "   - mAP and per-class metrics\n",
    "   - Confusion matrix analysis\n",
    "   - FP/FN breakdown per sample\n",
    "5. **Tagged failure cases** - `failure:high_fn`, `failure:high_fp` for next iteration\n",
    "\n",
    "**Key Insight**: Don't just look at mAP. The confusion matrix and per-sample failures tell you *what to label next*.\n",
    "\n",
    "**Artifacts Created**:\n",
    "- `predictions_v0` field on validation samples\n",
    "- `eval_v0` evaluation key with metrics\n",
    "- Failure tags for targeted selection\n",
    "- Model checkpoint at `/tmp/yolo_runs/kitti_v0/weights/best.pt`\n",
    "\n",
    "**Next up**: Step 5 - Iteration: Hybrid Acquisition Loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
