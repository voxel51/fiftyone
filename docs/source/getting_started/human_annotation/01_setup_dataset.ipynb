{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Setup - Flatten Dataset and Create Splits\n",
    "\n",
    "In this step, we'll load the KITTI-style `quickstart-groups` dataset, flatten it to a single camera view for annotation, and establish the three critical data splits that make iterative annotation actually work.\n",
    "\n",
    "**Why This Matters**: Without proper splits, you'll contaminate your evaluation and think you're improving when you're not. This step is non-negotiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Grouped Dataset\n",
    "\n",
    "The `quickstart-groups` dataset contains 200 KITTI scenes with multiple sensor modalities:\n",
    "- `left`: Left camera images with 2D detections\n",
    "- `right`: Right camera images\n",
    "- `pcd`: Point cloud data with 3D annotations\n",
    "\n",
    "Let's load it and explore the structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Load the grouped dataset\n",
    "dataset = foz.load_zoo_dataset(\"quickstart-groups\")\n",
    "\n",
    "print(f\"Dataset: {dataset.name}\")\n",
    "print(f\"Media type: {dataset.media_type}\")\n",
    "print(f\"Num groups: {len(dataset)}\")\n",
    "print(f\"Group slices: {dataset.group_slices}\")\n",
    "print(f\"\\nFields:\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the App to explore the grouped data\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten to Left Camera Slice\n",
    "\n",
    "Human annotation in FiftyOne works best with standard image datasets. We'll flatten the grouped dataset to work with just the left camera images, which have 2D bounding box annotations.\n",
    "\n",
    "This approach:\n",
    "1. Preserves the 2D detection workflow (our focus for this guide)\n",
    "2. Avoids UI complexity with grouped data\n",
    "3. Gives us a clean image dataset for YOLOv8 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only the left camera slice and flatten to a standard image dataset\n",
    "left_view = dataset.select_group_slices([\"left\"], flat=True)\n",
    "\n",
    "print(f\"Flattened view: {len(left_view)} samples\")\n",
    "print(f\"Media type: {left_view.media_type}\")\n",
    "print(f\"\\nSample fields:\")\n",
    "for field_name, field in left_view.get_field_schema().items():\n",
    "    print(f\"  {field_name}: {field}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone to a new dataset for our annotation workflow\n",
    "# This ensures we don't modify the original zoo dataset\n",
    "annotation_dataset = left_view.clone(\"kitti_annotation_tutorial\")\n",
    "annotation_dataset.persistent = True\n",
    "\n",
    "print(f\"Created: {annotation_dataset.name}\")\n",
    "print(f\"Num samples: {len(annotation_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Three Critical Splits\n",
    "\n",
    "Now we establish the splits that make this workflow actually work:\n",
    "\n",
    "| Split | Size | Purpose |\n",
    "|-------|------|----------|\n",
    "| **test** | 15% | Frozen. Never used for selection. Final evaluation only. |\n",
    "| **golden** | 5% | Heavily reviewed QA set. Detects label drift. |\n",
    "| **pool** | 80% | Active learning pool. All new labels come from here. |\n",
    "\n",
    "We'll use FiftyOne tags to mark these splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Set seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Get all sample IDs and shuffle\n",
    "sample_ids = list(annotation_dataset.values(\"id\"))\n",
    "random.shuffle(sample_ids)\n",
    "\n",
    "# Calculate split sizes\n",
    "n_total = len(sample_ids)\n",
    "n_test = int(0.15 * n_total)      # 15% for frozen test\n",
    "n_golden = int(0.05 * n_total)    # 5% for golden QA\n",
    "n_pool = n_total - n_test - n_golden  # Remainder for active pool\n",
    "\n",
    "print(f\"Total samples: {n_total}\")\n",
    "print(f\"Test split: {n_test} samples (15%)\")\n",
    "print(f\"Golden split: {n_golden} samples (5%)\")\n",
    "print(f\"Pool split: {n_pool} samples (80%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign splits using tags\n",
    "test_ids = sample_ids[:n_test]\n",
    "golden_ids = sample_ids[n_test:n_test + n_golden]\n",
    "pool_ids = sample_ids[n_test + n_golden:]\n",
    "\n",
    "# Tag the samples\n",
    "annotation_dataset.select(test_ids).tag_samples(\"split:test\")\n",
    "annotation_dataset.select(golden_ids).tag_samples(\"split:golden\")\n",
    "annotation_dataset.select(pool_ids).tag_samples(\"split:pool\")\n",
    "\n",
    "print(\"Splits assigned!\")\n",
    "print(f\"  split:test - {len(annotation_dataset.match_tags('split:test'))} samples\")\n",
    "print(f\"  split:golden - {len(annotation_dataset.match_tags('split:golden'))} samples\")\n",
    "print(f\"  split:pool - {len(annotation_dataset.match_tags('split:pool'))} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Label Fields for Human Annotations\n",
    "\n",
    "We'll keep the original `ground_truth` field intact and create a new field for human annotations. This maintains provenance and lets us compare original vs. human-corrected labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original annotations are in 'ground_truth'\n",
    "# We'll create 'human_labels' for our annotation work\n",
    "\n",
    "# First, let's see what classes exist in the original annotations\n",
    "classes = annotation_dataset.distinct(\"ground_truth.detections.label\")\n",
    "print(f\"Original classes: {classes}\")\n",
    "\n",
    "# Count detections per class\n",
    "from collections import Counter\n",
    "\n",
    "all_labels = []\n",
    "for sample in annotation_dataset:\n",
    "    if sample.ground_truth:\n",
    "        all_labels.extend([det.label for det in sample.ground_truth.detections])\n",
    "\n",
    "label_counts = Counter(all_labels)\n",
    "print(f\"\\nDetection counts by class:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a field to track annotation status\n",
    "annotation_dataset.add_sample_field(\"annotation_status\", fo.StringField)\n",
    "\n",
    "# Initialize all samples as 'unlabeled'\n",
    "annotation_dataset.set_values(\"annotation_status\", [\"unlabeled\"] * len(annotation_dataset))\n",
    "\n",
    "print(\"Added annotation_status field (all samples start as 'unlabeled')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Helper Views\n",
    "\n",
    "We'll save views for easy access to each split during the annotation workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save views for each split\n",
    "test_view = annotation_dataset.match_tags(\"split:test\")\n",
    "golden_view = annotation_dataset.match_tags(\"split:golden\")\n",
    "pool_view = annotation_dataset.match_tags(\"split:pool\")\n",
    "\n",
    "annotation_dataset.save_view(\"test_set\", test_view)\n",
    "annotation_dataset.save_view(\"golden_qa\", golden_view)\n",
    "annotation_dataset.save_view(\"active_pool\", pool_view)\n",
    "\n",
    "print(\"Saved views:\")\n",
    "for name in annotation_dataset.list_saved_views():\n",
    "    print(f\"  - {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the App with the pool view (where we'll select samples to annotate)\n",
    "session.dataset = annotation_dataset\n",
    "session.view = pool_view\n",
    "session.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify the Setup\n",
    "\n",
    "Let's confirm everything is correctly configured before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"SETUP VERIFICATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nDataset: {annotation_dataset.name}\")\n",
    "print(f\"Total samples: {len(annotation_dataset)}\")\n",
    "print(f\"\\nSplits:\")\n",
    "print(f\"  Test (frozen):  {len(test_view)} samples\")\n",
    "print(f\"  Golden (QA):    {len(golden_view)} samples\")\n",
    "print(f\"  Pool (active):  {len(pool_view)} samples\")\n",
    "print(f\"\\nLabel fields:\")\n",
    "print(f\"  ground_truth: Original KITTI annotations\")\n",
    "print(f\"  human_labels: (Will be created during annotation)\")\n",
    "print(f\"\\nSaved views: {annotation_dataset.list_saved_views()}\")\n",
    "print(f\"\\nClasses: {classes}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Ready for Step 2: Bootstrap Selection!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this step, you:\n",
    "\n",
    "1. **Loaded the quickstart-groups dataset** - Multi-modal KITTI data with camera + LiDAR\n",
    "2. **Flattened to left camera images** - Created a clean image dataset for annotation\n",
    "3. **Created three critical splits**:\n",
    "   - **Test (15%)**: Frozen, never touched during active learning\n",
    "   - **Golden (5%)**: Small QA set to detect label drift\n",
    "   - **Pool (80%)**: Where all new labels will come from\n",
    "4. **Set up tracking fields** - For annotation status and provenance\n",
    "\n",
    "**Key Insight**: These splits aren't optional. Without them, you'll contaminate your evaluation and build a model that only looks good on paper.\n",
    "\n",
    "**Next up**: Step 2 - Bootstrap Selection with Embeddings + ZCore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
