{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Annotation + QA\n",
    "\n",
    "Now we annotate the selected batch. This step covers:\n",
    "1. Setting up a consistent annotation schema\n",
    "2. **Actually labeling in the App** (not simulated)\n",
    "3. QA checks before training\n",
    "\n",
    "> **Time commitment:** Plan 1-2 minutes per sample for careful annotation. Start with 10-20 samples to get the feel, then continue or use the fast-forward option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "dataset = fo.load_dataset(\"annotation_tutorial\")\n",
    "batch_v0 = dataset.load_saved_view(\"batch_v0\")\n",
    "\n",
    "print(f\"Batch v0: {len(batch_v0)} samples to annotate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Your Schema\n",
    "\n",
    "Before labeling, define the rules. This prevents class drift and maintains consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define annotation schema\n",
    "LABEL_FIELD = \"human_labels\"  # Use this exact name in the App\n",
    "\n",
    "SCHEMA = {\n",
    "    \"classes\": [\n",
    "        \"person\", \"car\", \"truck\", \"bus\", \"motorcycle\", \"bicycle\",\n",
    "        \"dog\", \"cat\", \"bird\", \"horse\",\n",
    "        \"chair\", \"couch\", \"dining table\", \"tv\",\n",
    "        \"bottle\", \"cup\", \"bowl\",\n",
    "        \"other\"  # catch-all for edge cases\n",
    "    ],\n",
    "    \"field_name\": LABEL_FIELD\n",
    "}\n",
    "\n",
    "SCHEMA_CLASSES = set(SCHEMA[\"classes\"])\n",
    "\n",
    "# Store in dataset for reference\n",
    "dataset.info[\"annotation_schema\"] = SCHEMA\n",
    "dataset.save()\n",
    "\n",
    "print(f\"Schema defined: {len(SCHEMA['classes'])} classes\")\n",
    "print(f\"Target field: {LABEL_FIELD}\")\n",
    "print(f\"\\nWhen you create a field in the App, name it exactly: {LABEL_FIELD}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotate in the App\n",
    "\n",
    "**This is the real labeling step.** Open the App and annotate your selected samples.\n",
    "\n",
    "### Setup (one time)\n",
    "1. Launch the App with your batch\n",
    "2. Click a sample to open the modal\n",
    "3. Click the **Annotate** tab (pencil icon)\n",
    "4. Click **Schema** -> **Add Field** -> name it exactly `human_labels`\n",
    "5. Set type to **Detections** and add your classes\n",
    "\n",
    "### For each sample\n",
    "1. Review the image\n",
    "2. Click **Detection** button (square icon)\n",
    "3. Draw boxes around all objects of interest\n",
    "4. Assign the correct class to each box\n",
    "5. Move to the next sample\n",
    "\n",
    "### Tips\n",
    "- **Be consistent:** Same object = same class, every time\n",
    "- **Tight boxes:** As close as possible without cutting off the object\n",
    "- **Don't skip:** If it's ambiguous, label it \"other\" rather than skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App with your batch\n",
    "session = fo.launch_app(batch_v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop here and annotate samples\n",
    "\n",
    "Take 15-30 minutes to actually label some samples. This is the core skill.\n",
    "\n",
    "When you're done (or want to fast-forward), continue below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fast-Forward Option\n",
    "\n",
    "If you want to proceed without labeling everything manually, set `FAST_FORWARD = True` below. This copies `ground_truth` labels to `human_labels` to simulate completed annotation.\n",
    "\n",
    "> **Note:** In real projects, there's no shortcut. Label quality determines model quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True ONLY if you want to skip manual annotation\n",
    "# Default is False - you should label samples yourself\n",
    "FAST_FORWARD = False\n",
    "\n",
    "if FAST_FORWARD:\n",
    "    print(\"Fast-forwarding: copying ground_truth to human_labels...\")\n",
    "    print(f\"Filtering to schema classes: {len(SCHEMA_CLASSES)} classes\")\n",
    "    \n",
    "    copied = 0\n",
    "    skipped = 0\n",
    "    \n",
    "    for sample in batch_v0:\n",
    "        if sample.ground_truth:\n",
    "            # Only copy detections that match our schema\n",
    "            human_dets = []\n",
    "            for det in sample.ground_truth.detections:\n",
    "                if det.label in SCHEMA_CLASSES:\n",
    "                    human_dets.append(fo.Detection(\n",
    "                        label=det.label,\n",
    "                        bounding_box=det.bounding_box,\n",
    "                    ))\n",
    "                    copied += 1\n",
    "                else:\n",
    "                    skipped += 1\n",
    "            sample[LABEL_FIELD] = fo.Detections(detections=human_dets)\n",
    "        else:\n",
    "            sample[LABEL_FIELD] = fo.Detections(detections=[])\n",
    "        sample.save()\n",
    "    \n",
    "    print(f\"Copied {copied} detections, skipped {skipped} (not in schema)\")\n",
    "else:\n",
    "    print(\"Using your manual annotations.\")\n",
    "    print(f\"Make sure you created the '{LABEL_FIELD}' field and labeled samples in the App!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mark Annotated Samples\n",
    "\n",
    "**Important:** We only mark samples as \"annotated\" if they actually have labels. This prevents training on unlabeled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload to see changes\n",
    "dataset.reload()\n",
    "\n",
    "# Find samples that actually have labels\n",
    "batch_samples = dataset.match_tags(\"batch:v0\")\n",
    "\n",
    "if LABEL_FIELD in dataset.get_field_schema():\n",
    "    has_labels = batch_samples.match(F(f\"{LABEL_FIELD}.detections\").length() > 0)\n",
    "    no_labels = batch_samples.match(\n",
    "        (F(LABEL_FIELD) == None) | (F(f\"{LABEL_FIELD}.detections\").length() == 0)\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch v0 status:\")\n",
    "    print(f\"  With labels: {len(has_labels)}\")\n",
    "    print(f\"  Without labels: {len(no_labels)}\")\n",
    "    \n",
    "    if len(has_labels) == 0:\n",
    "        print(f\"\\n>>> No samples have labels in '{LABEL_FIELD}'.\")\n",
    "        print(\">>> Either label some samples in the App, or set FAST_FORWARD = True above.\")\n",
    "    else:\n",
    "        # Tag ONLY samples that have labels as annotated\n",
    "        has_labels.untag_samples(\"to_annotate\")\n",
    "        has_labels.tag_samples(\"annotated:v0\")\n",
    "        has_labels.set_values(\"annotation_status\", [\"annotated\"] * len(has_labels))\n",
    "        \n",
    "        # Mark unlabeled samples as still needing annotation\n",
    "        if len(no_labels) > 0:\n",
    "            no_labels.set_values(\"annotation_status\", [\"pending\"] * len(no_labels))\n",
    "        \n",
    "        print(f\"\\nTagged {len(has_labels)} samples as 'annotated:v0'\")\n",
    "        if len(no_labels) > 0:\n",
    "            print(f\"{len(no_labels)} samples still need annotation.\")\n",
    "else:\n",
    "    print(f\"Field '{LABEL_FIELD}' not found. Create it in the App first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Checks\n",
    "\n",
    "Before training, verify label quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get annotated samples\n",
    "annotated = dataset.match_tags(\"annotated:v0\")\n",
    "\n",
    "if len(annotated) == 0:\n",
    "    print(\"No annotated samples yet. Complete the annotation step above first.\")\n",
    "else:\n",
    "    print(f\"QA Check 1: Label coverage\")\n",
    "    print(f\"  Annotated samples: {len(annotated)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 2: Class distribution\n",
    "from collections import Counter\n",
    "\n",
    "if len(annotated) > 0:\n",
    "    all_labels = []\n",
    "    for sample in annotated:\n",
    "        if sample[LABEL_FIELD]:\n",
    "            all_labels.extend([d.label for d in sample[LABEL_FIELD].detections])\n",
    "\n",
    "    print(f\"QA Check 2: Class distribution ({len(all_labels)} total detections)\")\n",
    "    for label, count in Counter(all_labels).most_common(10):\n",
    "        print(f\"  {label}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check 3: Unexpected classes\n",
    "if len(annotated) > 0 and len(all_labels) > 0:\n",
    "    actual = set(all_labels)\n",
    "    unexpected = actual - SCHEMA_CLASSES\n",
    "\n",
    "    if unexpected:\n",
    "        print(f\"QA Check 3: Unexpected classes found: {unexpected}\")\n",
    "        print(\"   These don't match your schema. Review before training.\")\n",
    "    else:\n",
    "        print(f\"QA Check 3: All classes match schema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You annotated Batch v0:\n",
    "- Defined a schema for consistency\n",
    "- Labeled samples in the App (or fast-forwarded)\n",
    "- **Only samples with actual labels** were marked as annotated\n",
    "- Ran QA checks: coverage, class distribution, schema compliance\n",
    "\n",
    "**Artifacts:**\n",
    "- `human_labels` field with your annotations\n",
    "- `annotated:v0` tag on samples that have labels\n",
    "\n",
    "**Next:** Step 5 - Train + Evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
