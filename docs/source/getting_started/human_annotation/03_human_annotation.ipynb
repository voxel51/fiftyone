{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Human Annotation Pass + QA\n",
    "\n",
    "Now we annotate. This step covers:\n",
    "1. Setting up an annotation schema for consistency\n",
    "2. Using patch views to focus annotation effort\n",
    "3. Creating and editing labels in the FiftyOne App\n",
    "4. Running a QA pass to catch errors before training\n",
    "\n",
    "**Why This Matters**: Bad labels create bad models. A disciplined annotation + QA workflow is the difference between a model that works and one that fails mysteriously in production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset and Selected Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "# Load dataset\n",
    "dataset = fo.load_dataset(\"kitti_annotation_tutorial\")\n",
    "\n",
    "# Load the batch we selected for annotation\n",
    "batch_v0 = dataset.load_saved_view(\"batch_v0_to_annotate\")\n",
    "\n",
    "print(f\"Dataset: {dataset.name}\")\n",
    "print(f\"Batch v0: {len(batch_v0)} samples to annotate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Annotation Schema\n",
    "\n",
    "Before anyone touches a label, define the rules. A schema prevents:\n",
    "- Class drift (annotator A uses \"car\", annotator B uses \"Car\")\n",
    "- Attribute inconsistency (some add \"occluded\", some don't)\n",
    "- Taxonomy bloat (adding classes mid-project)\n",
    "\n",
    "For KITTI, we'll use the standard classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our annotation schema\n",
    "ANNOTATION_SCHEMA = {\n",
    "    \"classes\": [\n",
    "        \"Car\",\n",
    "        \"Pedestrian\", \n",
    "        \"Cyclist\",\n",
    "        \"Truck\",\n",
    "        \"Van\",\n",
    "        \"Tram\",\n",
    "        \"Person_sitting\",\n",
    "        \"Misc\",\n",
    "        \"DontCare\"  # For ambiguous regions\n",
    "    ],\n",
    "    \"attributes\": {\n",
    "        \"occluded\": {\n",
    "            \"type\": \"categorical\",\n",
    "            \"values\": [\"none\", \"partial\", \"heavy\"]\n",
    "        },\n",
    "        \"truncated\": {\n",
    "            \"type\": \"boolean\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Annotation Schema:\")\n",
    "print(f\"  Classes: {ANNOTATION_SCHEMA['classes']}\")\n",
    "print(f\"  Attributes: {list(ANNOTATION_SCHEMA['attributes'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store schema as dataset info for reference\n",
    "dataset.info[\"annotation_schema\"] = ANNOTATION_SCHEMA\n",
    "dataset.save()\n",
    "\n",
    "print(\"Schema saved to dataset.info['annotation_schema']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Patch Views for Efficient Annotation\n",
    "\n",
    "Patch views let annotators focus on individual objects instead of scrolling through full images. This is especially useful for:\n",
    "- **Reviewing existing detections** - One patch per detection for quick verification\n",
    "- **Fixing localization** - Easier to see box boundaries on cropped patches\n",
    "- **Class verification** - Quick yes/no decisions per object\n",
    "\n",
    "Let's create a patch view of our selected batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a patches view for the ground_truth detections in our batch\n",
    "patches_view = batch_v0.to_patches(\"ground_truth\")\n",
    "\n",
    "print(f\"Patches view: {len(patches_view)} object patches\")\n",
    "print(f\"From {len(batch_v0)} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the App with patches view\n",
    "session = fo.launch_app(patches_view)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Annotation Workflow in the App\n",
    "\n",
    "### Creating New Labels\n",
    "\n",
    "To add labels in the FiftyOne App:\n",
    "\n",
    "1. **Enter Annotate Mode**: Click the pencil icon in the sample modal\n",
    "2. **Select a label field**: Choose `human_labels` (or create it if it doesn't exist)\n",
    "3. **Draw bounding boxes**: Click and drag to create detection boxes\n",
    "4. **Assign class**: Select from the schema-defined classes\n",
    "5. **Add attributes**: Set occlusion, truncation as needed\n",
    "6. **Save**: Changes persist automatically\n",
    "\n",
    "### Editing Existing Labels\n",
    "\n",
    "To fix existing `ground_truth` labels:\n",
    "1. Click on a detection to select it\n",
    "2. Drag corners to adjust the bounding box\n",
    "3. Click the class label to change it\n",
    "4. Use the delete key to remove incorrect detections\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Be consistent**: Follow the schema strictly\n",
    "- **When in doubt, DontCare**: Ambiguous objects get the DontCare class\n",
    "- **Tight boxes**: Boxes should be as tight as possible without cutting off the object\n",
    "- **Occluded objects**: Still annotate them, but mark the occlusion attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate Annotation (For Tutorial Purposes)\n",
    "\n",
    "In a real workflow, you'd spend time in the App annotating. For this tutorial, we'll simulate the annotation process by copying ground_truth to human_labels with some modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For tutorial: Copy ground_truth to human_labels for the selected batch\n",
    "# In real usage, this field would be populated by human annotation in the App\n",
    "\n",
    "for sample in batch_v0:\n",
    "    if sample.ground_truth:\n",
    "        # Clone the detections\n",
    "        human_dets = []\n",
    "        for det in sample.ground_truth.detections:\n",
    "            human_det = fo.Detection(\n",
    "                label=det.label,\n",
    "                bounding_box=det.bounding_box,\n",
    "                confidence=1.0  # Human labels have confidence 1.0\n",
    "            )\n",
    "            human_dets.append(human_det)\n",
    "        \n",
    "        sample[\"human_labels\"] = fo.Detections(detections=human_dets)\n",
    "    else:\n",
    "        sample[\"human_labels\"] = fo.Detections(detections=[])\n",
    "    \n",
    "    # Update annotation status\n",
    "    sample[\"annotation_status\"] = \"annotated\"\n",
    "    sample.save()\n",
    "\n",
    "print(f\"Created human_labels for {len(batch_v0)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag samples as annotated for Batch v0\n",
    "batch_v0.tag_samples(\"annotated:v0\")\n",
    "\n",
    "# Remove the 'to_annotate' tag since we're done\n",
    "batch_v0.untag_samples(\"to_annotate\")\n",
    "\n",
    "print(\"Updated tags: removed 'to_annotate', added 'annotated:v0'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QA Pass: Verify Label Quality\n",
    "\n",
    "Before training, run a QA pass to catch systematic errors. We'll check for:\n",
    "\n",
    "1. **Missing labels**: Samples that should have detections but don't\n",
    "2. **Class distribution anomalies**: Unexpected class frequencies\n",
    "3. **Bounding box issues**: Boxes that are too small, too large, or malformed\n",
    "4. **Golden set verification**: Extra scrutiny on the QA samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Check 1: Samples with no detections\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "no_labels = batch_v0.match(F(\"human_labels.detections\").length() == 0)\n",
    "print(f\"QA Check 1 - Samples with no human_labels: {len(no_labels)}\")\n",
    "\n",
    "if len(no_labels) > 0:\n",
    "    print(\"  Review these samples - they may need annotation or are legitimately empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Check 2: Class distribution\n",
    "from collections import Counter\n",
    "\n",
    "human_labels_list = []\n",
    "for sample in batch_v0:\n",
    "    if sample.human_labels:\n",
    "        human_labels_list.extend([det.label for det in sample.human_labels.detections])\n",
    "\n",
    "label_counts = Counter(human_labels_list)\n",
    "print(f\"\\nQA Check 2 - Class distribution in human_labels:\")\n",
    "for label, count in sorted(label_counts.items(), key=lambda x: -x[1]):\n",
    "    print(f\"  {label}: {count}\")\n",
    "\n",
    "# Flag if any unexpected classes appear\n",
    "expected_classes = set(ANNOTATION_SCHEMA['classes'])\n",
    "actual_classes = set(label_counts.keys())\n",
    "unexpected = actual_classes - expected_classes\n",
    "if unexpected:\n",
    "    print(f\"\\n  WARNING: Unexpected classes found: {unexpected}\")\n",
    "else:\n",
    "    print(\"\\n  All classes match schema.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Check 3: Bounding box sanity checks\n",
    "import numpy as np\n",
    "\n",
    "box_areas = []\n",
    "box_issues = []\n",
    "\n",
    "for sample in batch_v0:\n",
    "    if sample.human_labels:\n",
    "        for det in sample.human_labels.detections:\n",
    "            x, y, w, h = det.bounding_box\n",
    "            area = w * h\n",
    "            box_areas.append(area)\n",
    "            \n",
    "            # Flag issues\n",
    "            if area < 0.001:  # Very small box\n",
    "                box_issues.append((sample.id, det.id, \"tiny_box\", area))\n",
    "            if area > 0.5:  # Very large box (> 50% of image)\n",
    "                box_issues.append((sample.id, det.id, \"huge_box\", area))\n",
    "            if w <= 0 or h <= 0:\n",
    "                box_issues.append((sample.id, det.id, \"invalid_dimensions\", (w, h)))\n",
    "\n",
    "print(f\"\\nQA Check 3 - Bounding box statistics:\")\n",
    "print(f\"  Total boxes: {len(box_areas)}\")\n",
    "print(f\"  Mean area: {np.mean(box_areas):.4f}\")\n",
    "print(f\"  Min area: {np.min(box_areas):.4f}\")\n",
    "print(f\"  Max area: {np.max(box_areas):.4f}\")\n",
    "print(f\"  Issues found: {len(box_issues)}\")\n",
    "\n",
    "if box_issues:\n",
    "    print(\"\\n  Flagged boxes:\")\n",
    "    for sample_id, det_id, issue, value in box_issues[:5]:\n",
    "        print(f\"    Sample {sample_id[:8]}..., Detection {det_id[:8]}...: {issue} ({value})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Check 4: Verify golden set samples were annotated\n",
    "golden_view = dataset.load_saved_view(\"golden_qa\")\n",
    "golden_in_batch = golden_view.match_tags(\"batch:v0\")\n",
    "\n",
    "print(f\"\\nQA Check 4 - Golden set in Batch v0:\")\n",
    "print(f\"  Golden samples total: {len(golden_view)}\")\n",
    "print(f\"  Golden samples in this batch: {len(golden_in_batch)}\")\n",
    "\n",
    "if len(golden_in_batch) > 0:\n",
    "    print(\"  These samples need extra review (they're in the golden QA set)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track Annotation Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of annotation progress\n",
    "pool_view = dataset.load_saved_view(\"active_pool\")\n",
    "annotated = dataset.match(F(\"annotation_status\") == \"annotated\")\n",
    "remaining = pool_view.match(F(\"annotation_status\") != \"annotated\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"ANNOTATION PROGRESS\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Pool total:     {len(pool_view)}\")\n",
    "print(f\"Annotated:      {len(annotated)} ({100*len(annotated)/len(pool_view):.1f}%)\")\n",
    "print(f\"Remaining:      {len(remaining)} ({100*len(remaining)/len(pool_view):.1f}%)\")\n",
    "print(f\"\\nBatch v0:       {len(batch_v0)} samples\")\n",
    "print(f\"Total labels:   {sum(len(s.human_labels.detections) for s in batch_v0 if s.human_labels)}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this step, you:\n",
    "\n",
    "1. **Defined an annotation schema** - Consistent classes and attributes\n",
    "2. **Created patch views** - Efficient per-object annotation\n",
    "3. **Annotated Batch v0** - Created `human_labels` field with detections\n",
    "4. **Ran QA checks**:\n",
    "   - Missing labels\n",
    "   - Class distribution\n",
    "   - Bounding box sanity\n",
    "   - Golden set verification\n",
    "\n",
    "**Key Insight**: QA isn't optional. A 5-minute check now saves hours of debugging mysterious model failures later.\n",
    "\n",
    "**Artifacts Created**:\n",
    "- `human_labels` field on batch_v0 samples\n",
    "- `annotated:v0` tag on completed samples\n",
    "- Schema stored in `dataset.info`\n",
    "\n",
    "**Next up**: Step 4 - Train Baseline + Evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
