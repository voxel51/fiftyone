{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Iteration - Hybrid Acquisition Loop\n",
    "\n",
    "This is where the magic happens. Instead of randomly labeling more data, we use a **hybrid acquisition strategy**:\n",
    "\n",
    "- **30% Coverage Refresh** (ZCore): Ensures we don't tunnel-vision on failures\n",
    "- **70% Targeted Failure Mining**: Focus on where the model struggles\n",
    "\n",
    "This step teaches you to:\n",
    "1. Implement the hybrid acquisition recipe\n",
    "2. Use embedding-based neighbor expansion\n",
    "3. Balance FN vs FP vs class confusion in your selection\n",
    "4. Iterate without contaminating your test set\n",
    "\n",
    "**Why This Matters**: Only chasing failures creates a model that's great at edge cases and terrible at normal cases. The coverage budget keeps you honest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import fiftyone.brain as fob\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "dataset = fo.load_dataset(\"kitti_annotation_tutorial\")\n",
    "\n",
    "# Get the remaining unlabeled pool\n",
    "pool_remaining = dataset.match_tags(\"split:pool\").match(F(\"annotation_status\") != \"annotated\")\n",
    "\n",
    "# Get validation failures from Step 4\n",
    "failures_view = dataset.load_saved_view(\"eval_v0_failures\")\n",
    "\n",
    "print(f\"Remaining pool: {len(pool_remaining)} samples\")\n",
    "print(f\"Failure samples from eval: {len(failures_view)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Acquisition Budget\n",
    "\n",
    "For iteration 1, we'll select another batch. The split:\n",
    "- 30% from ZCore (coverage)\n",
    "- 70% from failure mining (targeted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size for iteration 1\n",
    "batch_size = int(0.15 * len(pool_remaining))  # 15% of remaining pool\n",
    "\n",
    "# Split budget\n",
    "coverage_budget = int(0.30 * batch_size)  # 30% for coverage\n",
    "targeted_budget = batch_size - coverage_budget  # 70% for targeted\n",
    "\n",
    "# Further split targeted budget\n",
    "fn_budget = int(0.50 * targeted_budget)  # 50% of targeted for FN\n",
    "fp_budget = int(0.30 * targeted_budget)  # 30% of targeted for FP\n",
    "confusion_budget = targeted_budget - fn_budget - fp_budget  # 20% for confusion\n",
    "\n",
    "print(\"Acquisition Budget for Iteration 1:\")\n",
    "print(f\"  Total batch: {batch_size} samples\")\n",
    "print(f\"  ├── Coverage (ZCore): {coverage_budget} (30%)\")\n",
    "print(f\"  └── Targeted: {targeted_budget} (70%)\")\n",
    "print(f\"      ├── FN mining: {fn_budget} (35% of total)\")\n",
    "print(f\"      ├── FP mining: {fp_budget} (21% of total)\")\n",
    "print(f\"      └── Confusion: {confusion_budget} (14% of total)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Coverage Refresh (ZCore)\n",
    "\n",
    "First, we select coverage-optimized samples from the remaining pool using the same uniqueness-based approach from Step 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure embeddings exist on remaining pool\n",
    "# (Should already exist from Step 2)\n",
    "has_embeddings = pool_remaining.exists(\"embeddings\")\n",
    "print(f\"Samples with embeddings: {len(has_embeddings)}/{len(pool_remaining)}\")\n",
    "\n",
    "# If missing, compute them\n",
    "if len(has_embeddings) < len(pool_remaining):\n",
    "    print(\"Computing missing embeddings...\")\n",
    "    missing = pool_remaining.match(~F(\"embeddings\").exists())\n",
    "    fob.compute_visualization(\n",
    "        missing,\n",
    "        embeddings=\"embeddings\",\n",
    "        brain_key=\"img_viz_iter1\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute uniqueness on remaining pool\n",
    "fob.compute_uniqueness(\n",
    "    pool_remaining,\n",
    "    uniqueness_field=\"uniqueness_v1\",\n",
    "    embeddings=\"embeddings\"\n",
    ")\n",
    "\n",
    "print(\"Uniqueness scores computed for remaining pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select coverage samples (highest uniqueness)\n",
    "coverage_samples = pool_remaining.sort_by(\"uniqueness_v1\", reverse=True).limit(coverage_budget)\n",
    "coverage_ids = list(coverage_samples.values(\"id\"))\n",
    "\n",
    "print(f\"Coverage selection: {len(coverage_ids)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Targeted Failure Mining\n",
    "\n",
    "Now we mine for samples similar to our failure cases. The strategy:\n",
    "1. Take the failure samples from evaluation\n",
    "2. Find their nearest neighbors in embedding space\n",
    "3. Select neighbors that are still unlabeled\n",
    "\n",
    "This expands around failure cases without just picking the exact failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get embeddings for remaining pool\n",
    "pool_embeddings = np.array([s.embeddings for s in pool_remaining if s.embeddings is not None])\n",
    "pool_ids_with_emb = [s.id for s in pool_remaining if s.embeddings is not None]\n",
    "\n",
    "print(f\"Pool samples with embeddings: {len(pool_ids_with_emb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find nearest neighbors\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def find_neighbors(query_embeddings, pool_embeddings, pool_ids, n_neighbors=5):\n",
    "    \"\"\"Find nearest neighbors in embedding space.\"\"\"\n",
    "    if len(query_embeddings) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Compute similarities\n",
    "    similarities = cosine_similarity(query_embeddings, pool_embeddings)\n",
    "    \n",
    "    # For each query, get top n_neighbors\n",
    "    neighbor_ids = set()\n",
    "    for sim_row in similarities:\n",
    "        top_indices = np.argsort(sim_row)[-n_neighbors:]\n",
    "        for idx in top_indices:\n",
    "            neighbor_ids.add(pool_ids[idx])\n",
    "    \n",
    "    return list(neighbor_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine around high-FN samples\n",
    "high_fn_samples = failures_view.match_tags(\"failure:high_fn\")\n",
    "fn_embeddings = np.array([s.embeddings for s in high_fn_samples if s.embeddings is not None])\n",
    "\n",
    "fn_neighbors = find_neighbors(\n",
    "    fn_embeddings, \n",
    "    pool_embeddings, \n",
    "    pool_ids_with_emb,\n",
    "    n_neighbors=max(1, fn_budget // max(1, len(fn_embeddings)))\n",
    ")\n",
    "\n",
    "# Filter to only include samples not already selected for coverage\n",
    "fn_selection = [sid for sid in fn_neighbors if sid not in coverage_ids][:fn_budget]\n",
    "print(f\"FN-targeted selection: {len(fn_selection)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mine around high-FP samples\n",
    "high_fp_samples = failures_view.match_tags(\"failure:high_fp\")\n",
    "fp_embeddings = np.array([s.embeddings for s in high_fp_samples if s.embeddings is not None])\n",
    "\n",
    "fp_neighbors = find_neighbors(\n",
    "    fp_embeddings,\n",
    "    pool_embeddings,\n",
    "    pool_ids_with_emb,\n",
    "    n_neighbors=max(1, fp_budget // max(1, len(fp_embeddings)))\n",
    ")\n",
    "\n",
    "# Filter out already selected\n",
    "already_selected = set(coverage_ids + fn_selection)\n",
    "fp_selection = [sid for sid in fp_neighbors if sid not in already_selected][:fp_budget]\n",
    "print(f\"FP-targeted selection: {len(fp_selection)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For confusion cases, we'd ideally look at class-confused samples\n",
    "# For simplicity, we'll select random samples from remaining pool\n",
    "import random\n",
    "\n",
    "already_selected = set(coverage_ids + fn_selection + fp_selection)\n",
    "remaining_ids = [sid for sid in pool_ids_with_emb if sid not in already_selected]\n",
    "\n",
    "random.seed(42)\n",
    "confusion_selection = random.sample(remaining_ids, min(confusion_budget, len(remaining_ids)))\n",
    "print(f\"Confusion/diversity selection: {len(confusion_selection)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine and Tag Batch v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all selections\n",
    "batch_v1_ids = coverage_ids + fn_selection + fp_selection + confusion_selection\n",
    "\n",
    "# Remove duplicates (shouldn't be any, but just in case)\n",
    "batch_v1_ids = list(set(batch_v1_ids))\n",
    "\n",
    "print(f\"\\nBatch v1 Total: {len(batch_v1_ids)} samples\")\n",
    "print(f\"  Coverage: {len(coverage_ids)}\")\n",
    "print(f\"  FN-targeted: {len(fn_selection)}\")\n",
    "print(f\"  FP-targeted: {len(fp_selection)}\")\n",
    "print(f\"  Confusion/diversity: {len(confusion_selection)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tag the batch\n",
    "batch_v1 = dataset.select(batch_v1_ids)\n",
    "\n",
    "# Add tags\n",
    "batch_v1.tag_samples(\"batch:v1\")\n",
    "batch_v1.tag_samples(\"to_annotate\")\n",
    "\n",
    "# Update annotation status\n",
    "batch_v1.set_values(\"annotation_status\", [\"selected\"] * len(batch_v1))\n",
    "\n",
    "# Tag by source for tracking\n",
    "dataset.select(coverage_ids).tag_samples(\"source:coverage_v1\")\n",
    "dataset.select(fn_selection).tag_samples(\"source:fn_mining_v1\")\n",
    "dataset.select(fp_selection).tag_samples(\"source:fp_mining_v1\")\n",
    "dataset.select(confusion_selection).tag_samples(\"source:diversity_v1\")\n",
    "\n",
    "print(\"\\nBatch v1 tagged and ready for annotation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as a view\n",
    "dataset.save_view(\"batch_v1_to_annotate\", batch_v1)\n",
    "print(f\"Saved view: batch_v1_to_annotate ({len(batch_v1)} samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the Selection\n",
    "\n",
    "Let's see how our hybrid selection looks in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the App\n",
    "session = fo.launch_app(batch_v1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of selection sources\n",
    "print(\"\\nBatch v1 Selection Sources:\")\n",
    "for source_tag in [\"source:coverage_v1\", \"source:fn_mining_v1\", \"source:fp_mining_v1\", \"source:diversity_v1\"]:\n",
    "    count = len(batch_v1.match_tags(source_tag))\n",
    "    print(f\"  {source_tag}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Iteration Recipe\n",
    "\n",
    "Here's the full loop you'll repeat:\n",
    "\n",
    "```\n",
    "1. Annotate batch_v1 (Step 3 workflow)\n",
    "2. Retrain YOLOv8 on all annotated data (Step 4 workflow)\n",
    "3. Evaluate on validation (compare to v0)\n",
    "4. Check frozen test set (occasional reality check)\n",
    "5. Check golden set (detect label drift)\n",
    "6. Select batch_v2 using this hybrid recipe\n",
    "7. Repeat until stopping criteria\n",
    "```\n",
    "\n",
    "### Stopping Criteria\n",
    "\n",
    "Stop iterating when:\n",
    "- Gains per labeled sample flatten (diminishing returns)\n",
    "- Remaining failures are mostly label ambiguity (irreducible)\n",
    "- Production-like metrics hit acceptable thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track progress across iterations\n",
    "print(\"=\"*50)\n",
    "print(\"ANNOTATION LOOP PROGRESS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "total_pool = len(dataset.match_tags(\"split:pool\"))\n",
    "annotated_v0 = len(dataset.match_tags(\"annotated:v0\"))\n",
    "selected_v1 = len(dataset.match_tags(\"batch:v1\"))\n",
    "remaining = total_pool - annotated_v0 - selected_v1\n",
    "\n",
    "print(f\"\\nPool total:        {total_pool}\")\n",
    "print(f\"├── Annotated v0:  {annotated_v0} ({100*annotated_v0/total_pool:.1f}%)\")\n",
    "print(f\"├── Selected v1:   {selected_v1} ({100*selected_v1/total_pool:.1f}%)\")\n",
    "print(f\"└── Remaining:     {remaining} ({100*remaining/total_pool:.1f}%)\")\n",
    "\n",
    "print(f\"\\nAfter v1 annotation: {100*(annotated_v0+selected_v1)/total_pool:.1f}% labeled\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Warnings\n",
    "\n",
    "### Don't Contaminate Your Test Set\n",
    "\n",
    "The frozen test set (`split:test`) should NEVER be used for:\n",
    "- Selection decisions\n",
    "- Training data\n",
    "- Frequent evaluation\n",
    "\n",
    "Only check test set occasionally (e.g., every 3-4 iterations) as a reality check.\n",
    "\n",
    "### Don't Only Chase Failures\n",
    "\n",
    "The 30% coverage budget isn't optional. Without it, you'll:\n",
    "- Overweight rare edge cases\n",
    "- Distort class priors\n",
    "- Create a model that's worse on normal cases\n",
    "\n",
    "### Track Label Quality, Not Just Quantity\n",
    "\n",
    "Every iteration, spot-check:\n",
    "- Are FPs actually model errors, or label errors?\n",
    "- Are FNs missed because they're hard, or because the label is wrong?\n",
    "- Is the golden set still accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this step, you:\n",
    "\n",
    "1. **Defined a hybrid acquisition budget**:\n",
    "   - 30% coverage (ZCore)\n",
    "   - 35% FN mining\n",
    "   - 21% FP mining\n",
    "   - 14% diversity/confusion\n",
    "\n",
    "2. **Implemented embedding-based neighbor expansion** - Don't just pick failures; expand around them\n",
    "\n",
    "3. **Selected Batch v1** - Ready for annotation\n",
    "\n",
    "4. **Learned the complete iteration loop** - Annotate → Train → Evaluate → Select → Repeat\n",
    "\n",
    "**Key Insight**: Smart iteration beats random labeling. The hybrid strategy balances exploration (coverage) and exploitation (failure mining).\n",
    "\n",
    "**Artifacts Created**:\n",
    "- `batch:v1` tag on selected samples\n",
    "- Source tags for tracking (`source:coverage_v1`, etc.)\n",
    "- `batch_v1_to_annotate` saved view\n",
    "\n",
    "**Your Turn**: Repeat Steps 3-5 with batch_v1, then batch_v2, etc. until your model meets your quality bar."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
