{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monocular Depth Estimation with FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough, you'll learn how to run monocular depth estimation models on your data using FiftyOne, Replicate, and Hugging Face libraries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It covers the following:\n",
    "\n",
    "- What is monocular depth estimation?\n",
    "- Downloading the SUN RGB-D dataset from source and loading it into FiftyOne\n",
    "- Running monocular depth estimation models on your data\n",
    "- Evaluating prediction performance\n",
    "- Visualizing the results in FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Monocular Depth Estimation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Monocular depth estimation](https://paperswithcode.com/task/monocular-depth-estimation) is the task of predicting the depth of a scene from *a single image*. Often, depth information is necessary for downstream tasks, such as 3D reconstruction or scene understanding. However, depth sensors are expensive and not always available.\n",
    "\n",
    "This is a challenging task because depth is inherently ambiguous from a single image. The same scene can be projected onto the image plane in many different ways, and it is impossible to know which one is correct without additional information.\n",
    "\n",
    "If you have multiple cameras, you can use [stereo depth estimation](https://paperswithcode.com/task/stereo-depth-estimation) techniques. But in some real world scenarios, you may be constrained to a single camera. When this is the case, you must rely on other cues, such as object size, occlusion, and perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "Monocular depth estimation has many applications in computer vision. For example, it can be used for:\n",
    "\n",
    "- **3D reconstruction**: Given a single image, estimate the depth of the scene and reconstruct the 3D geometry of the scene.\n",
    "- **Scene understanding**: Given a single image, estimate the depth of the scene and use it to understand the scene better.\n",
    "- **Autonomous driving**: Given a single image, estimate the depth of the scene and use it to navigate the vehicle.\n",
    "- **Augmented reality**: Given a single image, estimate the depth of the scene and use it to place virtual objects in the scene.\n",
    "\n",
    "Beyond these industry applications, the ability to extract high-quality depth information from a single image has found fascinating use cases in content creation and editing, for instance:\n",
    "\n",
    "- **Image editing**: Given a single image, estimate the depth of the scene and use it to apply depth-aware effects to the image.\n",
    "- **Image generation**: Given a single image, estimate the depth of the scene and use it to generate a 3D model of the scene.\n",
    "- **Depth-map guided text-to-image generation**: Given a single image, estimate the depth of the scene and use it to generate a new image that both adheres to your input text prompt and has the same depth map. (See [ControlNet](https://huggingface.co/lllyasviel/sd-controlnet-depth)!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import all the necessary libraries, installing `fiftyone` if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the SUN RGB-D dataset from [here](https://rgbd.cs.princeton.edu/) and unzip it, or use the following command to download it directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -o sunrgbd.zip https://rgbd.cs.princeton.edu/data/SUNRGBD.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and then unzip it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip sunrgbd.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [SUN RGB-D dataset](https://rgbd.cs.princeton.edu/) contains 10,335 RGB-D images, each of which has a corresponding RGB image, depth image, and camera intrinsics. It contains images from the [NYU depth v2](https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html), Berkeley [B3DO](http://kinectdata.com/), and [SUN3D](https://sun3d.cs.princeton.edu/) datasets. SUN RGB-D is [one of the most popular](https://paperswithcode.com/dataset/sun-rgb-d) datasets for monocular depth estimation and semantic segmentation tasks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use the dataset for other tasks, you can fully convert the annotations and load them into your `fiftyone.Dataset`. However, for this tutorial, we will only be using the depth images, so we will only use the RGB images and the depth images (stored in the `depth_bfx` sub-directories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we are just interested in getting the point across, we'll restrict ourselves to the first 20 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fo.Dataset(name=\"SUNRGBD-20\", persistent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in images and ground truth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## restrict to 20 scenes\n",
    "scene_dirs = glob(\"SUNRGBD/k*/*/*\")[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be representing depth maps with FiftyOne's [Heatmap](https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps) labels. For a thorough guide to working with heatmaps in FiftyOne, check out these [FiftyOne Heatmaps Tips and Tricks](https://voxel51.com/blog/heatmaps-fiftyone-tips-and-tricks-october-6th-2023/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to store everything in terms of normalized, *relative* distances, where 255 represents the maximum distance in the scene and 0 represents the minimum distance in the scene. This is a common way to represent depth maps, although it is far from the only way to do so. If we were interested in *absolute* distances, we could store sample-wise parameters for the minimum and maximum distances in the scene, and use these to reconstruct the absolute distances from the relative distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 20/20 [192.2ms elapsed, 0s remaining, 104.1 samples/s]     \n"
     ]
    }
   ],
   "source": [
    "samples = []\n",
    "for scene_dir in scene_dirs:\n",
    "    ## Get image file path from scene directory\n",
    "    image_path = glob(f\"{scene_dir}/image/*\")[0]\n",
    "\n",
    "    ## Get depth map file path from scene directory\n",
    "    depth_path = glob(f\"{scene_dir}/depth_bfx/*\")[0]\n",
    "\n",
    "    depth_map = np.array(Image.open(depth_path))\n",
    "    depth_map = (depth_map * 255 / np.max(depth_map)).astype(\"uint8\")\n",
    "    sample = fo.Sample(\n",
    "        filepath=image_path,\n",
    "        gt_depth=fo.Heatmap(map=depth_map),\n",
    "    )\n",
    "    \n",
    "    samples.append(sample)\n",
    "\n",
    "dataset.add_samples(samples);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize our images and depth maps in the [FiftyOne App](https://docs.voxel51.com/user_guide/app.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "## then open tab to localhost:5151 in browser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sun-rgbd-dataset](images/mde_gt_heatmaps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with depth maps, the color scheme and opacity of the heatmap are important. We can customize these as illustrated [here](https://docs.voxel51.com/user_guide/app.html#color-schemes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![color-customization](images/mde_color_customization.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting these RGB images and depth maps, we can see that there are some inaccuracies in the ground truth depth maps. For example, in this image, the dark rift through the center of the image is actually the *farthest* part of the scene, but the ground truth depth map shows it as the *closest* part of the scene:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gt-issue](images/mde_gt_issue.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Monocular Depth Estimation Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataset loaded in, we can run monocular depth estimation models on it! For a long time, the state-of-the-art models for monocular depth estimation such as [DORN](https://github.com/hufu6371/DORN) and [DenseDepth](https://github.com/ialhashim/DenseDepth) were built with convolutional neural networks. Recently, however, both transformer-based models ([DPT](https://huggingface.co/docs/transformers/model_doc/dpt), [GLPN](https://huggingface.co/docs/transformers/model_doc/glpn)) and diffusion-based models ([Marigold](https://huggingface.co/Bingxin/Marigold)) have achieved remarkable results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DPT (Transformer Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model we'll run is a Transformer-based model called [DPT](https://huggingface.co/docs/transformers/model_doc/dpt). The checkpoint below uses [MiDaS](https://github.com/isl-org/MiDaS/tree/master), which returns the [inverse depth map](https://pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/), so we have to invert it back to get a comparable depth map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Run locally with Hugging Face [Transformers](https://huggingface.co/docs/transformers/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If necessary, install `transformers`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 20/20 [15.1s elapsed, 0s remaining, 1.5 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "dpt_model = AutoModelForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\")\n",
    "\n",
    "## you can also us a different model:\n",
    "# image_processor = AutoImageProcessor.from_pretrained(\"Intel/dpt-large\")\n",
    "# dpt_model = AutoModelForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "def apply_dpt_model(sample, model, label_field):\n",
    "    image = Image.open(sample.filepath)\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predicted_depth = outputs.predicted_depth\n",
    "\n",
    "    prediction = torch.nn.functional.interpolate(\n",
    "        predicted_depth.unsqueeze(1),\n",
    "        size=image.size[::-1],\n",
    "        mode=\"bicubic\",\n",
    "        align_corners=False,\n",
    "    )\n",
    "\n",
    "    output = prediction.squeeze().cpu().numpy()\n",
    "    ## flip b/c MiDaS returns inverse depth\n",
    "    formatted = (255 - output * 255 / np.max(output)).astype(\"uint8\")\n",
    "\n",
    "    sample[label_field] = fo.Heatmap(map=formatted)\n",
    "    sample.save()\n",
    "\n",
    "for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "    apply_dpt_model(sample, dpt_model, \"dpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![dpt-heatmaps](images/mde_dpt_heatmaps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpolating Depth Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our `apply_dpt_model()` function, between the model's forward pass and the heatmap generation, notice that we make a call to `torch.nn.functional.interpolate()`. This is because the model's forward pass is run on a downsampled version of the image, and we want to return a heatmap that is the same size as the original image.\n",
    "\n",
    "Why do we need to do this? If we just want to *look* at the heatmaps, this would not matter. But if we want to compare the ground truth depth maps to the model's predictions on a per-pixel basis, we need to make sure that they are the same size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hugging Face Transformers Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we manually applied the `transformers` model to our data to generate heatmaps. In practice, we have made it even easier to apply transformer-based models (for monocular depth estimation as well as other tasks) to your data via FiftyOne's [Hugging Face Transformers Integration](https://docs.voxel51.com/integrations/huggingface.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the transformer models via Hugging Face's `transformers` library, and then just apply them to FiftyOne datasets via the `apply_model()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPT\n",
    "from transformers import DPTForDepthEstimation\n",
    "model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n",
    "\n",
    "# GLPN\n",
    "from transformers import GLPNForDepthEstimation\n",
    "model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")\n",
    "\n",
    "# Depth Anything\n",
    "from transformers import AutoModelForDepthEstimation\n",
    "model = AutoModelForDepthEstimation.from_pretrained(\"LiheYoung/depth-anything-small-hf\")\n",
    "\n",
    "dataset.apply_model(model, label_field=\"depth_predictions\")\n",
    "\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can load any Hugging Face Transformers model directly from the [FiftyOne Model Zoo](https://docs.voxel51.com/user_guide/model_zoo/index.html) via the name `depth-estimation-transformer-torch`, and specifying the model's location on the Hugging Face Hub (`repo_id`) via the `name_or_path` parameter. To load and apply [this DPT MiDaS hybrid model](https://huggingface.co/Intel/dpt-hybrid-midas), for instance, you would use the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "model = foz.load_zoo_model(\n",
    "    \"depth-estimation-transformer-torch\",\n",
    "    name_or_path=\"Intel/dpt-hybrid-midas\",\n",
    ")\n",
    "\n",
    "dataset.apply_model(model, label_field=\"dpt_hybrid_midas\")\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Run with [Replicate](https://replicate.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the `replicate` Python client if necessary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install replicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And set your API Token:\n",
    "\n",
    "Then run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export REPLICATE_API_TOKEN=r8_<your_token_here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 It might take a minute for the model to load into memory on the server (cold-start problem), but once it does the prediction should only take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import replicate\n",
    "import requests\n",
    "\n",
    "rgb_fp = dataset.first().filepath\n",
    "\n",
    "output = replicate.run(\n",
    "    \"cjwbw/midas:a6ba5798f04f80d3b314de0f0a62277f21ab3503c60c84d4817de83c5edfdae0\",\n",
    "    input={\n",
    "        \"model_type\": \"dpt_beit_large_512\",\n",
    "        \"image\":open(rgb_fp, \"rb\")\n",
    "    }\n",
    ")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Marigold (Diffusion Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While diffusion is a very powerful approach to monocular depth estimation, it is also very computationally expensive and can take a while. I personally recommend going for option 2, where predictions with Replicate take about 15 seconds per image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Download and run locally with Hugging Face [Diffusers](https://huggingface.co/docs/diffusers/index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone the Marigold GH repo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/prs-eth/Marigold.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Marigold.marigold import MarigoldPipeline\n",
    "pipe = MarigoldPipeline.from_pretrained(\"Bingxin/Marigold\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then prediction looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_image = Image.open(dataset.first().filepath)\n",
    "output = pipe(rgb_image)\n",
    "depth_image = output['depth_colored']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Run via [Replicate](https://replicate.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 It might take a minute for the model to load into memory on the server (cold-start problem), but once it does the prediction should only take a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |███████████████████| 20/20 [5.3m elapsed, 0s remaining, 0.1 samples/s]    \n"
     ]
    }
   ],
   "source": [
    "import replicate\n",
    "import requests\n",
    "import io\n",
    "\n",
    "def marigold_model(rgb_image):\n",
    "    output = replicate.run(\n",
    "        \"adirik/marigold:1a363593bc4882684fc58042d19db5e13a810e44e02f8d4c32afd1eb30464818\",\n",
    "        input={\n",
    "            \"image\":rgb_image\n",
    "        }\n",
    "    )\n",
    "    ## get the black and white depth map\n",
    "    response = requests.get(output[1]).content\n",
    "    return response\n",
    "\n",
    "def apply_marigold_model(sample, model, label_field):\n",
    "    rgb_image = open(sample.filepath, \"rb\")\n",
    "    response = model(rgb_image)\n",
    "    depth_image = np.array(Image.open(io.BytesIO(response)))[:, :, 0] ## all channels are the same\n",
    "    formatted = (255 - depth_image).astype(\"uint8\")\n",
    "    sample[label_field] = fo.Heatmap(map=formatted)\n",
    "    sample.save()\n",
    "\n",
    "for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "    apply_marigold_model(sample, marigold_model, \"marigold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![marigold-heatmaps](images/mde_marigold_heatmaps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have predictions from multiple models, let's evaluate them! We will leverage sklearn to apply three simple metrics commonly used for monocular depth estimation: [root mean squared error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation), [peak signal to noise ratio (PSNR)](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio), and [structural similarity index (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💡 Higher PSNR and SSIM scores indicate better predictions, while lower RMSE scores indicate better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio, mean_squared_error, structural_similarity\n",
    "\n",
    "def rmse(gt, pred):\n",
    "    \"\"\"Compute root mean squared error between ground truth and prediction\"\"\"\n",
    "    return np.sqrt(mean_squared_error(gt, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_depth(dataset, prediction_field, gt_field):\n",
    "    for sample in dataset.iter_samples(autosave=True, progress=True):\n",
    "        gt_map = sample[gt_field].map\n",
    "        pred = sample[prediction_field]\n",
    "        pred_map = pred.map\n",
    "        pred[\"rmse\"] = rmse(gt_map, pred_map)\n",
    "        pred[\"psnr\"] = peak_signal_noise_ratio(gt_map, pred_map)\n",
    "        pred[\"ssim\"] = structural_similarity(gt_map, pred_map)\n",
    "        sample[prediction_field] = pred\n",
    "    \n",
    "    ## add dynamic fields to dataset so we can view them in the App\n",
    "    dataset.add_dynamic_sample_fields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_depth(dataset, \"dpt\", \"gt_depth\")\n",
    "evaluate_depth(dataset, \"marigold\", \"gt_depth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute average metrics across the entire dataset very easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Error Metrics\n",
      "--------------------------------------------------\n",
      "Mean rmse for dpt: 49.8915828817003\n",
      "Mean psnr for dpt: 14.805904629602551\n",
      "Mean ssim for dpt: 0.8398022368184576\n",
      "--------------------------------------------------\n",
      "Mean rmse for marigold: 104.0061165272178\n",
      "Mean psnr for marigold: 7.93015537185192\n",
      "Mean ssim for marigold: 0.42766803372861134\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean Error Metrics\")\n",
    "for model in [\"dpt\", \"marigold\"]:\n",
    "    print(\"-\"*50)\n",
    "    for metric in [\"rmse\", \"psnr\", \"ssim\"]:\n",
    "        mean_metric_value = dataset.mean(f\"{model}.{metric}\")\n",
    "        print(f\"Mean {metric} for {model}: {mean_metric_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the metrics seem to agree that DPT outperforms Marigold. However, it is important to note that these metrics are not perfect. For example, RMSE is very sensitive to outliers, and SSIM is not very sensitive to small errors. For a more thorough evaluation, we can filter by these metrics in the app in order to visualize what the model is doing well and what it is doing poorly — or where the metrics are failing to capture the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toggling masks on and off is a great way to visualize the differences between the ground truth and the model's predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![compare-heatmaps](images/mde_compare_heatmaps.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Challenges with Monocular Depth Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've explored some model predictions, let's quickly recap some of the key challenges with monocular depth estimation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data quality and quantity\n",
    "\n",
    "Ground truth data is hard to come by, and is often noisy. For example, the SUN RGB-D dataset contains 10,335 RGB-D images, which is a lot, but it is still a relatively small dataset compared to other datasets such as ImageNet, which contains 1.2 million images. And in many cases, the ground truth data is noisy. For example, the ground truth depth maps in the SUN RGB-D dataset are generated by projecting the 3D point clouds onto the 2D image plane, and then computing the Euclidean distance between the projected points and the camera. This process is inherently noisy, and the resulting depth maps are often noisy as well.\n",
    "\n",
    "### Poor generalization\n",
    "\n",
    "Models often struggle to generalize to new environments. Outdoors, for example, is a very different environment than indoors, and models trained on indoor data often fail to generalize to outdoor data.\n",
    "\n",
    "### Precarious metrics\n",
    "\n",
    "Metrics are not always a good indicator of model performance. For example, a model might have a low RMSE, but still produce very noisy depth maps. This is why it is important to look at the depth maps themselves, and not just the metrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough, we learned how to run monocular depth estimation models on your data using FiftyOne, Replicate, and Hugging Face libraries! We also learned how to evaluate the predictions using common metrics, and how to visualize the results in FiftyOne. In real-world applications, it is important to look at the depth maps themselves, and not just the metrics! It is also important to understand that model performance is limited by the quality, quantity, and diversity of data they are trained on."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
