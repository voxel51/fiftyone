{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO75fXaFtdvE"
      },
      "source": [
        "# Specialist vs Foundation Models: Zero shot Classification with CLIP vs Supervised Fine-tuning with LeNet-5 on an Iterative Image Dataset Curation Flow\n",
        "\n",
        " We explore the zero-shot capabilities of the CLIP foundation model vs a traditional supervised fine-tuning with LeNet-5 highlighting the strengths of each approach on a workflow of image dataset curation.\n",
        "\n",
        "## What You Will Learn\n",
        "\n",
        "**Dataset Management & Exploration**\n",
        "- Load and explore MNIST with FiftyOne's dataset tools\n",
        "- Generate high-dimensional embeddings using CLIP\n",
        "- Visualize embeddings through PCA and UMAP dimensionality reduction\n",
        "- Calculate sample uniqueness and representativeness metrics\n",
        "\n",
        "**Model Development & Training**\n",
        "- Build and train LeNet-5 from scratch in PyTorch\n",
        "- Implement custom PyTorch datasets and data loaders\n",
        "- Perform zero-shot classification with CLIP (no task-specific training required)\n",
        "- Extract intermediate layer representations for analysis\n",
        "\n",
        "**Model Evaluation & Improvement**\n",
        "- Evaluate performance with confusion matrices and per-class metrics\n",
        "- Calculate prediction hardness and mistakenness scores\n",
        "- Identify model weaknesses through visual debugging\n",
        "- Apply targeted data curation and strategic augmentation to fix failures\n",
        "\n",
        "**Foundation Concepts**\n",
        "- Embedding spaces as semantic image representations\n",
        "- Zero-shot vs supervised learning tradeoffs\n",
        "- Data quality impact on model performance\n",
        "- Fine-tuning strategies for model improvement\n",
        "\n",
        "\n",
        "### So, what is the takeaway?\n",
        "\n",
        "This tutorial demonstrates that while modern foundation models like CLIP are powerful, they can have limitations on specialized domain datasets.\n",
        "\n",
        "Supervised models like LeNet-5 enable domain-specific optimization through targeted fine-tuning. You will learn that embeddings serve as universal representations for semantic similarity analysis, that visual debugging tools reveal model behavior patterns, and that systematic dataset curation produces larger performance gains than architecture changes alone.\n",
        "\n",
        "## Setup\n",
        "FiftyOne, PyTorch, and several other packages are required. You can install them with:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "W3OK5Jc4b0R4"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!uv pip install fiftyone==1.7.0 torch==2.6.0 torchvision==0.21 numpy==2.0.2 open-clip-torch==3.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RCDaiM_Ebcuw"
      },
      "outputs": [],
      "source": [
        "# Check installed versions\n",
        "import fiftyone\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy\n",
        "import open_clip\n",
        "(fiftyone.__version__, torch.__version__,\n",
        " torchvision.__version__, numpy.__version__,\n",
        " open_clip.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISDyu809KcxZ"
      },
      "source": [
        "### **Content Overview**\n",
        "\n",
        "Here is a breakdown of what we will cover in this tutorial, split into two main parts. First, we will explore the capabilities of a large foundation model, CLIP, for zero-shot classification on MNIST. Second, we will build, train, and curate a dataset for a specialized supervised model, LeNet-5, and compare its performance.\n",
        "\n",
        "### **Part 1: Zero-Shot Classification and Dataset Analysis with CLIP**\n",
        "\n",
        "This section focuses on using a pre-trained foundation model to understand and classify the MNIST dataset without any specific training.\n",
        "\n",
        "**1. MNIST Dataset Exploration with FiftyOne**\n",
        "We will begin by understanding the structure of the MNIST dataset. You will load the test split into FiftyOne, compute metadata, and visualize data distributions using aggregations and the FiftyOne App.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   Loading datasets from the FiftyOne Dataset Zoo\n",
        "*   Computing image metadata\n",
        "*   Using FiftyOne aggregations for data statistics\n",
        "*   Visualizing dataset distributions\n",
        "\n",
        "**2. Image Embeddings with CLIP**\n",
        "Next, you will generate image embeddings for the test dataset using a pre-trained CLIP model. We will visualize these high-dimensional vectors in 2D using PCA and UMAP to get a sense of image similarity.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   Loading pre-trained models from the FiftyOne Model Zoo\n",
        "*   Computing image embeddings with CLIP\n",
        "*   Assigning embeddings to dataset samples\n",
        "*   Dimensionality reduction with PCA and UMAP\n",
        "*   Visualizing embedding plots in FiftyOne\n",
        "\n",
        "**3. Dataset Analysis using CLIP Embeddings**\n",
        "We will continue the analysis of the CLIP embeddings on the test dataset. This involves exploring dataset clustering, and computing sample uniqueness and representativeness based on the embeddings.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   Introduction to clustering with embeddings\n",
        "*   Creating a similarity index\n",
        "*   Identifying outliers and representative samples\n",
        "\n",
        "**4. Zero-Shot Classification with CLIP**\n",
        "Here, we will perform image classification on the test dataset using CLIP without any task-specific training. You will then evaluate CLIP's performance using FiftyOne's evaluation tools.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   Zero-shot classification principles\n",
        "*   Using text prompts for classification\n",
        "*   Applying a model to a FiftyOne dataset\n",
        "*   Evaluating classification results, including accuracy and a confusion matrix\n",
        "\n",
        "---\n",
        "\n",
        "### **Part 2: Supervised Training and Dataset Curation with LeNet-5**\n",
        "\n",
        "This section details the process of building a traditional convolutional neural network and using an iterative workflow to improve its performance through data curation.\n",
        "\n",
        "**5. Supervised Classification: LeNet-5 with PyTorch**\n",
        "You will build and train a LeNet-5 convolutional neural network from scratch using PyTorch. This includes preparing the MNIST training data and implementing the training and validation loops.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   The LeNet-5 architecture\n",
        "*   Defining a subclass of PyTorch's `nn.Module`\n",
        "*   Splitting FiftyOne data for training and validation\n",
        "*   Creating custom PyTorch Datasets from FiftyOne views\n",
        "*   Data normalization through mean and standard deviation computation\n",
        "*   PyTorch DataLoaders\n",
        "*   Defining loss functions and optimizers\n",
        "*   Training loops and model checkpointing\n",
        "\n",
        "**6. LeNet-5 Model Evaluation on Test Data**\n",
        "We will apply the trained LeNet-5 model to the MNIST test set, store the predictions in FiftyOne, and evaluate its performance. We will also analyze prediction characteristics like hardness and mistakenness.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   Applying a PyTorch model to a FiftyOne dataset\n",
        "*   Storing predictions, confidence, and logits\n",
        "*   Evaluating classification performance\n",
        "*   Analyzing prediction confidence distributions\n",
        "*   Computing sample hardness and mistakenness\n",
        "\n",
        "**7. Analysis of LeNet-5 Learned Features using Training Data**\n",
        "This part involves extracting embeddings from the trained LeNet-5 model using the training data. You will compute and visualize these embeddings to analyze uniqueness and representativeness based on LeNet's learned features and identify misclassifications within the training set.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   Extracting embeddings from intermediate PyTorch model layers\n",
        "*   Storing custom model embeddings in FiftyOne\n",
        "*   Visualizing custom embeddings with PCA and UMAP\n",
        "*   Analyzing uniqueness and representativeness of training samples\n",
        "*   Evaluating model performance on training data\n",
        "*   Identifying false positives and false negatives in training data\n",
        "\n",
        "**8. Data Augmentation Concepts for MNIST**\n",
        "Finally, we will have a conceptual discussion on effective data augmentation strategies for the MNIST dataset, covering geometric transformations, elastic deformations, and augmentations to avoid.\n",
        "\n",
        "*Key concepts covered:*\n",
        "*   Rationale for data augmentation\n",
        "*   Geometric and elastic transformations suitable for MNIST\n",
        "*   Best practices and pitfalls in augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG3_ZBDEjNwq"
      },
      "source": [
        "### FiftyOne Plug-ins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRlSg9rPlT2H"
      },
      "source": [
        "We'll also install FiftyOne plugins for model evaluation and data augmentation:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzHKYfmnjRYa"
      },
      "outputs": [],
      "source": [
        "# Plug-in to evaluate the performance of our classification models\n",
        "!fiftyone plugins download \\\n",
        "    https://github.com/voxel51/fiftyone-plugins \\\n",
        "    --plugin-names @voxel51/evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_r0OvvAflvmV"
      },
      "outputs": [],
      "source": [
        "# Plug-in for image augmentations\n",
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCOAC2gFi4ND"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rj1VqYYi6J1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables for reproducibility BEFORE importing torch\n",
        "os.environ['PYTHONHASHSEED'] = '51'\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fun\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.brain as fob\n",
        "from torch.utils.data import Dataset, ConcatDataset\n",
        "from fiftyone import ViewField as F\n",
        "import fiftyone.utils.random as four\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import random\n",
        "from typing import Optional, Dict, Tuple, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18b508e6"
      },
      "source": [
        "# Part 1: Exploring the Capabilities and Limits of CLIP with MNIST\n",
        "\n",
        "In this first part, we will dive into the world of zero-shot image classification using the powerful CLIP model. We will explore how CLIP, a model trained on a massive dataset of image-text pairs, can classify images it has never explicitly seen during training.\n",
        "\n",
        "We will use the classic MNIST dataset of handwritten digits as a case study. While MNIST is a seemingly simple dataset for traditional supervised models, it provides an excellent playground to understand the strengths and limitations of a general-purpose foundation model like CLIP in a specialized domain.\n",
        "\n",
        "Through hands-on examples using FiftyOne, you will learn how to:\n",
        "\n",
        "- Load and explore the MNIST dataset.\n",
        "- Generate and visualize image embeddings with CLIP.\n",
        "- Perform zero-shot classification using text prompts.\n",
        "- Analyze CLIP's performance on MNIST and identify where it struggles compared to specialized models.\n",
        "\n",
        "This section will lay the groundwork for understanding the difference between general-purpose foundation models and task-specific supervised models, which we will explore in the second part of this tutorial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKx_LKY_W87E"
      },
      "source": [
        "## The MNIST dataset\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/mnist_clean.png?raw=true)\n",
        "\n",
        "The Modified National Institute of Standards and Technology (MNIST) dataset stands as one of the most influential benchmarks in computer vision and machine learning history. Created by Yann LeCun and colleagues in 1998, MNIST transformed a collection of handwritten digits from American Census Bureau employees and high school students into a standardized machine learning challenge that has shaped decades of research.\n",
        "\n",
        "\n",
        "\n",
        "**Dataset Structure and Characteristics**\n",
        "\n",
        "The MNIST dataset contains 60,000 training images and 10,000 testing images of handwritten digits (0-9). These are grayscale images of size 28x28 pixels, with each pixel value ranging from 0 (black) to 255 (white). The images have been size-normalized and centered, making them ideal for learning fundamental computer vision concepts without the complexity of dealing with varying scales, rotations, or backgrounds found in natural images. You can inspect the samples on the test portion of the dataset through [try.fiftyone.ai](https://try.fiftyone.ai/datasets/mnist/samples).\n",
        "\n",
        "**Historical Significance and Impact**\n",
        "\n",
        "MNIST earned its status as the \"Hello World\" of computer vision for several reasons. First, it provided the research community with a common benchmark that was computationally tractable, even early personal computers could train models on MNIST in reasonable time. Second, its simplicity allowed researchers to focus on algorithmic innovations rather than data preprocessing challenges. Landmark achievements in deep learning, from early multilayer perceptrons to convolutional architectures like LeNet-5, were first demonstrated and validated on MNIST.\n",
        "\n",
        "The dataset served as a proving ground for fundamental concepts of modern computer vision: convolutional neural networks, regularization techniques, and optimization algorithms were all tested on these handwritten digits. Many techniques that seem obvious today, like data augmentation, dropout, and batch normalization, were first explored and validated using MNIST as a testbed.\n",
        "\n",
        "**Why MNIST Remains Relevant**\n",
        "\n",
        "While critics sometimes dismiss MNIST as \"too easy\" for modern standards, it continues to serve crucial educational and research purposes. For newcomers to computer vision, MNIST provides an ideal environment to understand core concepts without overwhelming complexity. The dataset is small enough to experiment with quickly, yet rich enough to demonstrate important phenomena like overfitting, the importance of data augmentation, and the impact of architectural choices.\n",
        "\n",
        "Moreover, MNIST's apparent simplicity can be deceptive. Achieving state-of-the-art performance >99.7% accuracy requires sophisticated techniques and careful attention to detail, making it an interesting benchmark for testing new methodologies.\n",
        "\n",
        "See the [discussion on Kaggle here](https://www.kaggle.com/competitions/digit-recognizer/discussion/61480) to get inspiration of how to best tackle this challenge.\n",
        "\n",
        "\n",
        "## MNIST in the Modern Era\n",
        "\n",
        "Today, MNIST serves as an excellent starting point for understanding how modern techniques like embeddings, zero-shot classification, and transfer learning work. While a model trained specifically on MNIST might achieve 99%+ accuracy, applying a general-purpose vision model like CLIP without any MNIST-specific training provides insights into how well these models generalize and what they've learned about visual patterns from their massive training datasets.\n",
        "\n",
        "This makes MNIST perfect for comparing traditional supervised learning approaches with modern pre-trained models, helping us understand the trade-offs between task-specific optimization and general-purpose visual understanding.\n",
        "\n",
        "\n",
        "## CLIP\n",
        "\n",
        "**CLIP (Contrastive Language-Image Pre-training)** is a vision-language model developed by OpenAI that learns to understand the relationship between images and text descriptions. Traditional computer vision models are trained on fixed sets of image categories, but CLIP was trained on 400 million image-text pairs from the internet, learning to match images with their captions. This training enables CLIP to perform \"zero-shot\" classification: the ability to classify images into categories it has not seen during training by comparing the image representation with text descriptions of potential classes. The model works by encoding both images and text into the same high-dimensional embedding space, where similar concepts cluster together, allowing it to determine which text description best matches a given image through similarity comparison.\n",
        "\n",
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/clip%20contrastive%20pre-training.png?raw=true)\n",
        "\n",
        "### CLIP vs MNIST\n",
        "\n",
        "In [OpenAI's 2021 CLIP paper](https://arxiv.org/abs/2103.00020), the comparison with MNIST revealed fascinating insights about zero-shot learning capabilities. While supervised models trained specifically on MNIST achieve near-perfect accuracy (>99.7%), OpenAI's best performing variant of CLIP (trained on about 400 million image-text pairs) achieved only 88% accuracy on these handwritten digits. This is interesting considering the model was not trained explicitly trained on any variant of this dataset. This comparison highlighted a shift in modern computer vision from specialized models that excel at narrow tasks to general models that perform reasonably well across diverse domains. The 11+ percentage point gap between supervised and zero-shot approaches on MNIST demonstrates both the power and limitations of general-purpose vision-language models, making MNIST an excellent case study for understanding the trade-offs between specialized optimization and general-purpose learning, particularly valuable for exploring modern approaches like few-shot learning, prompt engineering, and transfer learning strategies.\n",
        "\n",
        "Note that the variant of CLIP that we use in our experiments of this notebook, `\"clip-vit-base32-torch\"` (a Vit-B/32 model) is *not* the top performing variant of CLIP that is showcased on the original OpenAI paper. That would be \"ViT-L/14@336px\", a bigger vision transformer model. The Vit-B/32 base model remains interesting and widely popular due to its lower number of parameters.\n",
        "\n",
        "\n",
        "**Known Issues and Research Opportunities**\n",
        "\n",
        "The original dataset contains several images where the ground truth labeling is ambiguous or questionable. The academic community has identified [several of these ambiguous samples](https://arxiv.org/abs/1912.05283). These labeling inconsistencies, while representing less than 0.1% of the dataset, provide opportunities to explore data quality assessment techniques.\n",
        "\n",
        "Finding and analyzing these edge cases teaches valuable lessons about real-world data challenges. In production systems, you'll inevitably encounter ambiguous samples, annotation errors, and edge cases. MNIST's imperfections make it an excellent sandbox for developing robust approaches to handle these issues. Let's see if we can find them using FiftyOne's powerful analysis capabilities!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws9PjUuSZhH1"
      },
      "source": [
        "## Zero-shot Classification vs \"Traditional\" Supervised Training\n",
        "\n",
        "**Zero-shot classification** leverages pre-trained models like CLIP that have learned rich visual representations from massive datasets, allowing them to classify images into \"interpolated\" categories they've not explicitly seen during training. These models understand the semantic relationship between images and text descriptions, enabling classification through natural language prompts like \"a photo of the digit 3\" without requiring any task-specific training data. In contrast, **traditional supervised training** requires labeled examples for each class you want to predict. You must provide the model with many of images of each digit along with their correct labels, then train the network to learn the mapping from pixel patterns to class labels through backpropagation. While supervised training often achieves higher accuracy on specific datasets and allows for domain-specific optimization, zero-shot approaches offer remarkable flexibility and can instantly work on new classification tasks without additional training time or computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bo7U6NjqTSu"
      },
      "source": [
        "## Where CLIP Fails: Specialized Domains\n",
        "\n",
        "**Understanding the Limitations of General-Purpose Vision Models**\n",
        "\n",
        "While CLIP excels at understanding natural images and common objects, it struggles with specialized domains that differ from its internet-scale training data. MNIST handwritten digits represent one such domain where CLIP's performance drops compared to task-specific models.\n",
        "\n",
        "**Domain-Specific Challenges:**\n",
        "\n",
        "**Limited Training Exposure**: CLIP trained on web-scraped image-text pairs, which contain fewer examples of handwritten digits compared to photographs of everyday objects. The model lacks deep exposure to the subtle variations in handwriting styles.\n",
        "\n",
        "**Scale and Context Mismatch**: CLIP expects high-resolution, colorful images with rich contextual information. MNIST's 28x28 grayscale digits provide minimal visual context that CLIP relies on for classification decisions.\n",
        "\n",
        "**Specialized Visual Features**: Handwritten digits require recognition of fine-grained stroke patterns, curves, and connections that differ from the broader visual concepts CLIP learned from natural images. A \"6\" versus \"9\" distinction depends on subtle orientation cues.\n",
        "\n",
        "**Text-Image Alignment**: CLIP's strength lies in matching images with descriptive captions. Simple digit classification lacks the rich semantic relationships between visual and textual information that CLIP exploits in other domains.\n",
        "\n",
        "**Performance Gap**: This explains why CLIP achieves only ~88% accuracy on MNIST while specialized CNNs reach >99% (on OpenAI's reported experiments using the `ViT-L-14` base for CLIP). The 11+ percentage point gap highlights the trade-off between general-purpose capabilities and domain-specific optimization.\n",
        "\n",
        "Other specialized domains where CLIP struggles include medical imaging, satellite imagery, microscopy, and industrial inspection: areas where domain expertise and task-specific training data prove more valuable than general visual understanding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3C20CqVqgRN"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/clip_limitations.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3J8_p_7ZsvW"
      },
      "source": [
        "### Loading the MNIST Dataset from FiftyOne's Dataset Zoo\n",
        "\n",
        "A FiftyOne dataset wraps together the annotations and image data into a unified, queryable structure that makes computer vision workflows seamless. Unlike traditional approaches where you might manage separate files for images and labels, FiftyOne treats each sample as a rich object containing the image itself, ground truth labels, metadata, and any predictions or embeddings you add later. This design enables powerful operations like filtering by class imbalance, visualizing prediction confidence, or finding samples with specific characteristics, all through a consistent API.\n",
        "\n",
        "Loading MNIST from [FiftyOne's Dataset Zoo](https://docs.voxel51.com/dataset_zoo/index.html) is straightforward:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzg-FFDoTVWr"
      },
      "outputs": [],
      "source": [
        "# We will load the test split from the dataset first\n",
        "test_dataset = foz.load_zoo_dataset(\"mnist\", split='test')\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UYLNkgyKNj"
      },
      "source": [
        "We launch the FiftyOne app to visualize the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xD7xgYznyGZR"
      },
      "outputs": [],
      "source": [
        "session = fo.launch_app(test_dataset, auto=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym7vp063WUm-"
      },
      "source": [
        "With `compute_metadata()` we add the size in bytes, the image file type, the width and height of the image, and the number of channels to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKBLQnntVila"
      },
      "outputs": [],
      "source": [
        "test_dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTH-cijUZfAm"
      },
      "source": [
        "We can do [aggregations](https://docs.voxel51.com/user_guide/using_aggregations.html) on the dataset to explore the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr_v-8_yZ041"
      },
      "source": [
        "We can use the [bounds](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.bounds) aggregation to compute the [min, max] range of a numeric field of a dataset. And [mean()](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.bounds) and [std()](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.std) to compute the mean and standard deviation of it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yphCNxUxY-BW"
      },
      "outputs": [],
      "source": [
        "test_dataset.bounds(\"metadata.size_bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXWLJSuYZV0-"
      },
      "outputs": [],
      "source": [
        "test_dataset.mean(\"metadata.size_bytes\"), test_dataset.std(\"metadata.size_bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOmjhEc6X4Am"
      },
      "source": [
        "Try filtering by label and visualizing the metadata of the MNIST images through the FiftyOne app.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/filtering_by_label_mnist_w_metadata.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm8fbYruVVZI"
      },
      "outputs": [],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-aubVNe3JX"
      },
      "source": [
        "We can also visualize the distributions of `metadata.size_bytes` and `ground_truth.label`. Both of these are relevant when we decide the batch size on which to run our models and to evaluate the balance of classes in our dataset.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/distribtution_size_bytes.png?raw=true)\n",
        "\n",
        "For this try clicking on the `+` symbol next to samples, select `Histograms` and then `metadata.size_bytes` and `ground_truth.label` from the dropdown menu. The `Split horizontally button` will allow you to see the panel alongside the image data.\n",
        "\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/ground_truh_distribution_mnist.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQQPA9HTXVsC"
      },
      "source": [
        "## Creating Image Embeddings with CLIP\n",
        "\n",
        "**Image embeddings** are high-dimensional vector representations that capture the semantic and aesthetic content of images in a vector format that machine learning models can understand and compare.\n",
        "\n",
        "Think of embeddings as a way to translate visual concepts into vectors where similar images will correspond to similar embedding vectors, while visually or semantically different images will have more distant vectors in a high-dimensional space.\n",
        "\n",
        "OpenAI's CLIP model creates particularly powerful embeddings because it was trained to understand the relationship between images and their matching text descriptions, enabling it to capture rich semantic meaning. In FiftyOne, creating embeddings with CLIP is straightforward:\n",
        "\n",
        "1. We obtain the model through [`foz.load_zoo_model(\"\"clip-vit-base32-torch\"\")`](https://docs.voxel51.com/model_zoo/models.html#clip-vit-base32-torch) and pass it to the GPU (if we have it available).\n",
        "2. We use the [`compute_embeddings()`](https://docs.voxel51.com/api/fiftyone.brain.internal.core.elasticsearch.html#fiftyone.brain.internal.core.elasticsearch.ElasticsearchSimilarityIndex.compute_embeddings) method:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ST1OahV5kvgi"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\",\n",
        "                                device=device)\n",
        "print(f\"The model is loaded on {clip_model._device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhHT2scrakDC"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of parameters in the model\n",
        "total_params = sum(p.numel() for p in clip_model._model.parameters())\n",
        "\n",
        "print(f\"The CLIP model has {total_params:,} parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Z9t8CBoXKz"
      },
      "source": [
        "This will take about 3 min on a Google Colab instance with GPU enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-va5BZtpFw6"
      },
      "outputs": [],
      "source": [
        "clip_embeddings = test_dataset.compute_embeddings(model=clip_model,\n",
        "                                        batch_size=512,\n",
        "                                        num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV6umlNpEmGe"
      },
      "source": [
        "The collection of embeddings is a NumPy array. Each embedding is a 512-dimensional vector. We can think about each embedding as a distinct point in a high-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSyH8a7M0IrT"
      },
      "outputs": [],
      "source": [
        "# Check the format and shape of our embeddings vector\n",
        "type(clip_embeddings), clip_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiyhmJRrqSNY"
      },
      "outputs": [],
      "source": [
        "test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQQtu1jkE0I8"
      },
      "source": [
        "Here we attach each embedding to its corresponding image sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKDE9MR80UJ7"
      },
      "outputs": [],
      "source": [
        "for index, sample in enumerate(test_dataset):\n",
        "    sample[\"clip_embeddings\"] = clip_embeddings[index]\n",
        "    sample.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZVmfIOFHtK"
      },
      "source": [
        "We see that the first sample now has the field `embeddings` attached to it, and we can again query its shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDSCqkEss_K4"
      },
      "outputs": [],
      "source": [
        "test_dataset.first().clip_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7i_BkCiFYXs"
      },
      "source": [
        "## Creating a Similarity Index\n",
        "\n",
        "A **similarity index** allows for efficient searching of similar samples within a dataset based on their embeddings. Instead of comparing a query sample's embedding to every other embedding in the dataset (which can be slow for large datasets), the index organizes the embeddings in a way that allows for fast retrieval of the most similar samples. This is particularly useful for tasks like:\n",
        "\n",
        "- Finding visually similar images.\n",
        "- Identifying near-duplicate samples.\n",
        "- Exploring clusters of similar data points.\n",
        "\n",
        "In this case, we're creating a similarity index based on the CLIP embeddings, which capture the semantic content of the images. This index will enable us to quickly find images that are semantically similar to a given query image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCcxt-GyB-ZP"
      },
      "outputs": [],
      "source": [
        "# Compute similarity index based on CLIP embeddings\n",
        "print(\"Computing similarity index based on CLIP embeddings...\")\n",
        "\n",
        "similarity_index = fob.compute_similarity(\n",
        "    test_dataset,\n",
        "    model=\"clip-vit-base32-torch\",\n",
        "    embeddings=\"clip_embeddings\",  # Field containing the CLIP embeddings\n",
        "    brain_key=\"clip_cosine_similarity_index\",  # Unique identifier for this similarity index\n",
        "    backend=\"sklearn\",  # Can also use \"pinecone\" for large datasets\n",
        "    metric=\"cosine\"  # Similarity metric (cosine is good for embeddings)\n",
        ")\n",
        "\n",
        "print(f\"Similarity index computed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kXiNdZaewRJ"
      },
      "outputs": [],
      "source": [
        "# Display similarity index information\n",
        "print(f\"Similarity Index Details:\")\n",
        "print(f\"- Total samples indexed: {len(test_dataset)}\")\n",
        "print(f\"- Embeddings field: {similarity_index.config.embeddings_field}\")\n",
        "print(f\"- Metric: {similarity_index.config.metric}\")\n",
        "\n",
        "# Example 1: Find similar images to a specific digit\n",
        "query_sample = test_dataset.first()\n",
        "print(f\"\"\"\\nQuerying for images similar to sample: {query_sample.id}\n",
        "            with label {query_sample.ground_truth.label}\"\"\")\n",
        "\n",
        "# Use the sample ID (string) instead of the sample object\n",
        "similar_view = test_dataset.sort_by_similarity(\n",
        "    query_sample.id,  # Pass the sample ID, not the sample object\n",
        "    brain_key=\"clip_cosine_similarity_index\",\n",
        "    k=10,  # Return top 10 most similar\n",
        "    reverse=False  # Most similar first\n",
        ")\n",
        "\n",
        "print(f\"Found {len(similar_view)} most similar samples\")\n",
        "session.view = similar_view\n",
        "session.refresh()\n",
        "print(session.url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "949a9a69"
      },
      "source": [
        "## Querying by a Text Prompt\n",
        "\n",
        "One of the key features of vision-language models like CLIP is the ability to search and filter images based on text descriptions. This \"zero-shot\" capability allows you to find relevant images without needing to manually annotate them with labels. By embedding both images and text into a common space, CLIP can compare the similarity between an image's visual features and the semantic meaning of a text query. FiftyOne leverages this by allowing you to use text prompts to query your dataset based on CLIP embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S616_X4BtnQw"
      },
      "outputs": [],
      "source": [
        "query = \"a handwritten number nine\"\n",
        "nines_view = test_dataset.sort_by_similarity(query, k=50,\n",
        "                                             brain_key=\"clip_cosine_similarity_index\")\n",
        "session.view = nines_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjHRCURddjR8"
      },
      "source": [
        "### Creating a 2D Projection of the Embeddings for Visualization\n",
        "\n",
        "**Compressing High-Dimensional Representations for Visual Analysis**\n",
        "\n",
        "Projecting high-dimensional embeddings to 2D enables visual exploration of data structure and relationships. This process represents lossy compression. The 2D visualization cannot capture all information present in the original embedding space. The projection serves as an approximation for understanding data patterns, not for precise analysis.\n",
        "\n",
        "**Dimensionality Reduction Methods**: PCA and UMAP compress embeddings through different approaches. PCA finds linear combinations that preserve maximum variance, making it faster and deterministic, the same data produces identical results. UMAP uses sophisticated manifold learning to preserve local neighborhoods and reveal cluster structure, but operates through stochastic processes that can produce different results across runs.\n",
        "\n",
        "**Visualization Purpose**: These projections help identify clusters, outliers, and data distribution patterns. Points close together in the visualization often share similar characteristics, while distant points represent different concepts. The projection quality depends on how well the chosen method preserves the relationships present in the original high-dimensional space.\n",
        "\n",
        "**Interpretation Limits**: Remember that 2D projections cannot represent all relationships from the original embeddings. Some distances may appear closer or farther than they actually are in the full dimensional space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XacGyC6OrtdR"
      },
      "outputs": [],
      "source": [
        "pca_visualization = fob.compute_visualization(test_dataset,\n",
        "                                              method=\"pca\",\n",
        "                                              embeddings=\"clip_embeddings\",\n",
        "                                              num_dims=2,\n",
        "                                              brain_key=\"pca_visualization_clip_embeds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4mq4hg9_ovbh"
      },
      "outputs": [],
      "source": [
        "umap_visualization = fob.compute_visualization(test_dataset,\n",
        "                                              method=\"umap\",\n",
        "                                              embeddings=\"clip_embeddings\",\n",
        "                                              num_dims=2,\n",
        "                                              brain_key=\"umap_visualization_clip_embeds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtBJDv7_JwMY"
      },
      "source": [
        "## Visualizing Embeddings in the FiftyOne App\n",
        "\n",
        "To view the embedding visualizations (PCA and UMAP) in the FiftyOne App:\n",
        "\n",
        "1. **Open the Panel**: Click on the \"**+**\" icon next to \"Samples\" in the FiftyOne App's header.\n",
        "2. **Select Visualizations**: From the dropdown menu, select \"Embeddings\" under the \"Visualizations\" section.\n",
        "3. **Choose Embeddings**: In the Embeddings panel that appears, select the embedding field you want to visualize from the dropdown (e.g., `pca_visualization_clip_embeds` or `umap_visualization_clip_embeds`).\n",
        "\n",
        "You can then interact with the 2D plot, hovering over points to see the corresponding images and selecting regions to filter the samples in the grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htl4-xTnt5xY"
      },
      "outputs": [],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJTHNOjydh7d"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/image_embeddings_zero_cluster.gif?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzdQd-g3A248"
      },
      "source": [
        "Let's use the FiftyOne app to explore the  samples in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDJ5HWQzAnEZ"
      },
      "outputs": [],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwoQi7T52naS"
      },
      "source": [
        "## Clustering through Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJeaax5-8Rww"
      },
      "source": [
        "\n",
        "**Clustering** transforms the high-dimensional embedding space into groups of similar images, helping us discover hidden patterns and structure in our dataset. Rather than manually browsing through thousands of images, clustering algorithms like K-means, HDBSCAN, or Gaussian Mixture Models automatically identify samples that share visual or semantic characteristics. This is particularly powerful when combined with CLIP embeddings, as the clusters often correspond to semantic concepts like \"outdoor scenes,\" \"close-up portraits,\" or \"nighttime images.\" FiftyOne provides a dedicated clustering plugin that makes this analysis seamless. Install it with:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNWq6eg43Fn_"
      },
      "outputs": [],
      "source": [
        "!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXlzLzWZ860S"
      },
      "source": [
        "Once installed, you can access clustering functionality directly from the FiftyOne App by pressing the backtick key (`) and typing `compute_clusters`. The plugin offers multiple clustering algorithms and customizable parameters. This workflow is valuable for understanding the groupings within your data produced by the embedding model. Learn more about clustering workflows and advanced techniques in the [FiftyOne clustering tutorial](https://docs.voxel51.com/tutorials/clustering.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WNnlzTX93OnS"
      },
      "outputs": [],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JloCmwLUXZ78"
      },
      "source": [
        "## Zero-shot Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87uZ17UaeBcw"
      },
      "source": [
        "Now that we understand how CLIP creates meaningful embeddings, we can leverage these representations for **zero-shot classification** - the ability to classify images into categories without any task-specific training. CLIP's vision-language training enables it to understand the semantic relationship between images and text descriptions, allowing us to classify MNIST digits simply by providing natural language descriptions of each class.\n",
        "\n",
        "The key insight is that CLIP learns to map both images and text into the same embedding space, where semantically similar concepts cluster together. By computing the similarity between an image's embedding and text embeddings for each class description, we can determine which category best matches the input image. This approach is remarkably flexible, we can change our classification categories simply by modifying the text prompts, without retraining the models.\n",
        "\n",
        "For MNIST digit classification, we'll use descriptive text prompts combined with the written names of each digit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8Jk1qpak2Q9"
      },
      "outputs": [],
      "source": [
        "# We obtain the distinct labels of the dataset\n",
        "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
        "dataset_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLYZ287EUJQh"
      },
      "source": [
        "## The Effect of the Text Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4oAWHCawr7b"
      },
      "source": [
        "With CLIP, the `text_prompt` parameter can have a significant effect on the accuracy of the model, as it affects the text embedding for the label.\n",
        "\n",
        "### Experiment with the Prompt\n",
        "\n",
        "Try modifying the `text_prompt` field with:\n",
        "\n",
        "* \"The handwritten digit \"\n",
        "* \"A grayscale image of the number \"\n",
        "* \"A pixel illustration of the digit \"\n",
        "* \"A low resolution image of the number\"\n",
        "* \"An MNIST digit \"\n",
        "\n",
        "And observe the effect on the accuracy, precision and recall on our classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6Q7NqbJdYCL"
      },
      "outputs": [],
      "source": [
        "clip_model = foz.load_zoo_model(\n",
        "    \"clip-vit-base32-torch\",\n",
        "    text_prompt=\"A photo of \",\n",
        "    classes=dataset_classes,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBgS_90dWgbf"
      },
      "outputs": [],
      "source": [
        "# The CLIP model preprocesses the input data\n",
        "clip_model.preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJvdRwml6Cff"
      },
      "source": [
        "Notice that the CLIP model was originally trained on much larger images and that\n",
        "these where RGB. In order to make the MNIST grayscale image data work with CLIP, we transform it scale it using the mean and standard deviations of the original training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rMuKMCQWP9u"
      },
      "outputs": [],
      "source": [
        "clip_model._transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mG-ECXjiQ7p"
      },
      "source": [
        "The `store_logits=True` parameter is important as it preserves the model's confidence scores for each prediction, enabling us to analyze prediction uncertainty and identify samples where the model was unsure. After applying the model, we can compute comprehensive evaluation metrics and visualize the results using FiftyOne's evaluation framework:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9F4bKQheb_-"
      },
      "outputs": [],
      "source": [
        "test_dataset.apply_model(\n",
        "    model=clip_model,\n",
        "    label_field=\"clip_zero_shot_classification\",\n",
        "    # We need to store the logits to compute the \"mistakenness\" value\n",
        "    store_logits=True,\n",
        "    # This is how many samples we will show to the model at once\n",
        "    batch_size=64,\n",
        "     # Use more running threads if you have multiple cpus\n",
        "    num_workers=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kACq6EVGf5RR"
      },
      "outputs": [],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plmWGrpTXfSQ"
      },
      "source": [
        "## Evaluating CLIP's Classification Performance\n",
        "\n",
        "With our zero-shot CLIP predictions generated, we need to evaluate how well the model performs compared to the ground truth labels. FiftyOne provides powerful evaluation capabilities that go beyond simple accuracy metrics, allowing us to understand where and why our model succeeds or fails.\n",
        "\n",
        "[FiftyOne's Model Evaluation Panel](https://docs.voxel51.com/user_guide/app.html#model-evaluation-panel-sub-new ) provides interactive confusion matrices, per-class metrics, and the ability to drill down into specific failure modes. This visual analysis helps identify systematic errors, class imbalances, and edge cases that pure numerical metrics might miss, enabling targeted improvements to your classification pipeline.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y04n9H8WhAUf"
      },
      "outputs": [],
      "source": [
        "clip_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"clip_zero_shot_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"clip_zero_shot_eval\")\n",
        "\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZaXb016KpLcd"
      },
      "outputs": [],
      "source": [
        "clip_evaluation_results.print_report(classes=dataset_classes, digits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TioSsNHjS_sw"
      },
      "outputs": [],
      "source": [
        "# The confusion matrix has a lot activity outside of the main diagonal\n",
        "clip_evaluation_results.plot_confusion_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Exkmk50l_Kr"
      },
      "source": [
        "## Inspect the Images where the Model is Performing Worst\n",
        "\n",
        "We can create a view of the images labeled as nines on the dataset, to get a closer look at what CLIP is misclassifying.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0WTxH3smMkE"
      },
      "outputs": [],
      "source": [
        "nines_view = test_dataset.filter_labels(\"ground_truth\",\n",
        "                                        F(\"$ground_truth.label\") == \"9 - nine\"\n",
        "                                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IOzwOIZGzeS"
      },
      "source": [
        "#### The Purpose of `session.refresh()` in FiftyOne\n",
        "\n",
        "The `session.refresh()` command in FiftyOne syncs the state of the FiftyOne App interface with the state of our FiftyOne dataset object in the Python kernel.\n",
        "\n",
        "When we modify a dataset or view in your Python code by adding new fields, computing embeddings, applying models, filtering samples, or adding tags, these changes are made to the dataset object in your Python session. However, the FiftyOne App, which runs in a separate process, doesn't automatically know about these updates.\n",
        "\n",
        "Calling `session.refresh()` explicitly tells the FiftyOne App to reload the current view of the dataset from the backend. This ensures that the App's visualization and sample grid reflect the latest changes you've made in your notebook, allowing you to see and interact with the results of your code in the App's UI. Without `session.refresh()`, the App might show an outdated version of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oFZbuxx-wv5L"
      },
      "outputs": [],
      "source": [
        "session.view = nines_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o96XPi0xXtYz"
      },
      "source": [
        "## Follow-Up: What if We Try the Bigger CLIP Model?\n",
        "\n",
        "`ViT-L-14` is the larger version of the original OpenAI CLIP model. It has 2.8x more parameters than CLIP with `ViT-B-32` and provides better performance. Most of the results on OpenAI's original CLIP paper come from `ViT-L-14`.\n",
        "\n",
        "I did some experiments with `ViT-L-14` and was surprised to find that the performance is still far from optimal in the MNIST dataset. OpenAI correctly reported than even a simple multilayer perceptron would outperform this version of CLIP on the MNIST dataset and you can verify this easily on the second part of this notebook. It was also interesting for me to see that the prompts that worked best for `ViT-B-32` were not the same as for `ViT-L-14`. You can try this! Be aware that this version of the model is heavier and that inference will be slower.\n",
        "\n",
        "```python\n",
        "\n",
        "# Load the OpenAI ViT-L/14 CLIP model via OpenCLIP\n",
        "model = foz.load_zoo_model(\n",
        "    \"open-clip-torch\",\n",
        "    clip_model=\"ViT-L-14\",\n",
        "    pretrained=\"openai\",\n",
        "    # Specify classes for zero-shot classification\n",
        "    # classes=[str(i) for i in range(10)],\n",
        "    # text_prompt=\"A photo of a handwritten digit\",\n",
        ")\n",
        "\n",
        "# Apply the model to the dataset\n",
        "test_dataset.apply_model(model, label_field=\"clip_vitl14_openai_predictions\")\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDh1Hpt2YFzG"
      },
      "source": [
        "## Follow-up: What if We Try another CLIP variant?\n",
        "\n",
        "FiftyOne [integrates with the OpenCLIP library](https://docs.voxel51.com/integrations/openclip.html), an open source implementation of OpenAIs CLIP (Contrastive Language-Image Pre-training) model that you can use to run inference on the MNIST dataset with a few lines of code.\n",
        "\n",
        "The previous snippet showed you how to use FiftyOne's OpenCLIP integration to get the `ViT-L-14` version of the OpenAI model. We can also use it to get predictions from more modern variants of the CLIP model.\n",
        "\n",
        "```python\n",
        "\n",
        "\n",
        "meta_clip = foz.load_zoo_model(\n",
        "    \"open-clip-torch\",\n",
        "    clip_model=\"ViT-B-32-quickgelu\",\n",
        "    pretrained=\"metaclip_400m\",\n",
        ")\n",
        "\n",
        "eva_clip = foz.load_zoo_model(\n",
        "    \"open-clip-torch\",\n",
        "    clip_model=\"EVA02-B-16\",\n",
        "    pretrained=\"merged2b_s8b_b131k\",\n",
        ")\n",
        "\n",
        "clipa = foz.load_zoo_model(\n",
        "    \"open-clip-torch\",\n",
        "    clip_model=\"hf-hub:UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B\",\n",
        "    pretrained=\"\",\n",
        ")\n",
        "\n",
        "siglip = foz.load_zoo_model(\n",
        "    \"open-clip-torch\",\n",
        "    clip_model=\"hf-hub:timm/ViT-B-16-SigLIP\",\n",
        "    pretrained=\"\",\n",
        ")\n",
        "```\n",
        "\n",
        "Try these and compare the results obtained with those from the original CLIP model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e74j5GZ8HAI5"
      },
      "source": [
        "## Clearing the CLIP Model from GPU memory\n",
        "\n",
        "After using the CLIP model, it's good practice to free up the GPU memory it occupied. This is done by:\n",
        "\n",
        "1. Deleting the model variable (`del clip_model`).\n",
        "2. Running Python's garbage collector (`gc.collect()`) to clean up references.\n",
        "3. Emptying the CUDA cache (`torch.cuda.empty_cache()`) to release cached memory on the GPU.\n",
        "\n",
        "This ensures that the GPU memory (aka VRAM) is available for the next batches of images and models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t0uw_4muPB-8"
      },
      "outputs": [],
      "source": [
        "# Delete the model variable\n",
        "del clip_model\n",
        "\n",
        "# Run Python's garbage collector\n",
        "gc.collect()\n",
        "\n",
        "# Empty the CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"CUDA device memory from clip_model should be cleared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "253f8ed4"
      },
      "source": [
        "# Part 2: Supervised Fine-Tuning with LeNet-5 and Dataset Curation with FiftyOne\n",
        "\n",
        "In the first part of this notebook, we explored zero-shot classification with the CLIP model and analyzed the MNIST dataset using its embeddings. We saw how a powerful pre-trained foundation model can perform classification without task-specific training, but also observed its limitations on a specialized domain like handwritten digits.\n",
        "\n",
        "Now, we shift our focus to a more traditional approach: **supervised fine-tuning with a convolutional neural network (CNN)**. We will build and train a LeNet-5 model from scratch on the MNIST training data. This will allow us to compare the performance of a task-specific supervised model against the zero-shot capabilities of a foundation model. Think of it as evaluating the knowledge of a generalist vs a super-specialized expert on a narrow subject matter.\n",
        "\n",
        "Furthermore, we will leverage the insights gained from our initial data exploration and model evaluation to demonstrate an **iterative dataset curation workflow**. Using FiftyOne's capabilities, we will identify misclassified samples, analyze model uncertainty and mistakenness, and strategically apply data augmentation to improve our model's performance.\n",
        "\n",
        "This second part will cover:\n",
        "\n",
        "- Building and training a LeNet-5 model in PyTorch.\n",
        "- Evaluating the LeNet-5 model's performance on the test set.\n",
        "- Analyzing model predictions to identify problematic samples.\n",
        "- Using FiftyOne to find mislabeled and hard samples in the training data.\n",
        "- Applying data augmentation to improve the model's robustness.\n",
        "- Retraining the model with the augmented dataset.\n",
        "- Comparing the performance of the original and fine-tuned models.\n",
        "\n",
        "By the end of this section, you will have a deeper understanding of supervised learning workflows, the power of data curation for model improvement, and how FiftyOne facilitates these processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUd2c3J0q79_"
      },
      "source": [
        "## Reproducibility for Training Experiments\n",
        "\n",
        "To ensure full reproducibility of your training experiments, you need to set random seeds for all libraries and operations that involve randomness. This ensures that initial model weights, data shuffling, and any other random operations are the same across different runs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-VJ7ajbq16D"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=51):\n",
        "    \"\"\"\n",
        "    Set seeds for complete reproducibility across all libraries and operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Random seed value\n",
        "    \"\"\"\n",
        "    # Set environment variables before other imports\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "    # Python random module\n",
        "    random.seed(seed)\n",
        "\n",
        "    # NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch CPU\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # PyTorch GPU (all devices)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
        "\n",
        "        # CUDA deterministic operations\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # OpenCV\n",
        "    cv2.setRNGSeed(seed)\n",
        "\n",
        "    # Albumentations (for data augmentation)\n",
        "    try:\n",
        "        A.seed_everything(seed)\n",
        "    except AttributeError:\n",
        "        # Older versions of albumentations\n",
        "        pass\n",
        "\n",
        "    # PyTorch deterministic algorithms (may impact performance)\n",
        "    try:\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "    except RuntimeError:\n",
        "        # Some operations don't have deterministic implementations\n",
        "        print(\"Warning: Some operations may not be deterministic\")\n",
        "\n",
        "    print(f\"All random seeds set to {seed} for reproducibility\")\n",
        "\n",
        "\n",
        "\n",
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(51)\n",
        "\n",
        "# Additional reproducibility considerations:\n",
        "\n",
        "def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a DataLoader with deterministic behavior.\n",
        "\n",
        "    Args:\n",
        "        dataset: PyTorch Dataset instance\n",
        "        batch_size: Batch size\n",
        "        shuffle: Whether to shuffle data\n",
        "        **kwargs: Additional DataLoader arguments\n",
        "\n",
        "    Returns:\n",
        "        Training DataLoader with reproducible behavior\n",
        "    \"\"\"\n",
        "    # Use a generator with fixed seed for reproducible shuffling\n",
        "    generator = torch.Generator()\n",
        "    generator.manual_seed(51)\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        generator=generator if shuffle else None,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "# Example usage:\n",
        "# train_loader = create_deterministic_dataloader(\n",
        "#     torch_train_set,\n",
        "#     batch_size=64,\n",
        "#     shuffle=True,\n",
        "#     num_workers=4,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKrksi3nXk2k"
      },
      "source": [
        "## Creating a Custom Convolutional Neural Networks in PyTorch (Two Versions of LeNet-5)\n",
        "\n",
        "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/lenet5-architecture.png)\n",
        "\n",
        "While zero-shot classification with CLIP demonstrates the power of modern pre-trained models, understanding how to build and train convolutional neural networks from scratch remains fundamental to computer vision. **LeNet-5**, proposed by Yann LeCun in 1998, represents one of the earliest and most influential CNN architectures. Despite its age, LeNet-5 perfectly illustrates core CNN concepts including convolutional layers, pooling layers, and the transition from feature extraction to classification.\n",
        "\n",
        "LeNet-5's architecture is elegantly simple yet effective: it uses alternating convolutional and pooling layers to extract hierarchical features, followed by fully connected layers for classification. The network learns low-level features like edges and curves in early layers, then combines these into higher-level digit patterns in deeper layers. This hierarchical feature learning principle underlies virtually all modern CNN architectures.\n",
        "\n",
        "For MNIST digit classification, LeNet-5 provides an excellent baseline to compare against CLIP's zero-shot performance. While CLIP leverages massive-scale pre-training and vision-language understanding, LeNet-5 demonstrates what's possible with task-specific supervised learning on a much smaller scale. Building this model from scratch in PyTorch teaches essential concepts about gradient-based optimization, backpropagation, and the relationship between network architecture and learning capacity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr3oiQDzibmw"
      },
      "source": [
        "Here is [a great animation](https://youtu.be/UxIS_PoVoz8?si=3ibZms7Hk1oSj55k) showcasing the architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9bB2RaOyc3G"
      },
      "outputs": [],
      "source": [
        "class ClassicLeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    LeNet-5 CNN architecture for MNIST digit classification.\n",
        "\n",
        "    Original paper: \"Gradient-Based Learning Applied to Document Recognition\"\n",
        "    by LeCun et al. (1998)\n",
        "\n",
        "    Architecture (maintains original design with padding):\n",
        "    Input (28x28) -> Pad to (32x32) -> C1 (6@28x28) -> S2 (6@14x14) ->\n",
        "    C3 (16@10x10) -> S4 (16@5x5) -> C5 (120@1x1) -> F6 (84) -> Output (10)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ClassicLeNet5, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        # C1: Convolutional layer - 6 feature maps, 5x5 kernels\n",
        "        # Add padding=2 to convert 28x28 input to 32x32, maintaining original design\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        # S2: Subsampling layer (average pooling) - 2x2 with stride 2\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # C3: Convolutional layer - 16 feature maps, 5x5 kernels\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "\n",
        "        # S4: Subsampling layer (average pooling) - 2x2 with stride 2\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # C5: Convolutional layer - 120 feature maps, 5x5 kernels (original design)\n",
        "        # This reduces the 5x5 feature maps to 1x1\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
        "\n",
        "        # Classification layers\n",
        "        # F6: Fully connected layer with 84 units\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "\n",
        "        # Output layer: 10 classes for digits 0-9\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
        "\n",
        "        Returns:\n",
        "            Output logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # C1: Convolution + activation (padding converts 28x28 to 32x32, then conv to 28x28)\n",
        "        # Input: (batch, 1, 28, 28) -> Pad to (32, 32) -> Conv to (batch, 6, 28, 28)\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "\n",
        "        # S2: Average pooling\n",
        "        # Input: (batch, 6, 28, 28) -> Output: (batch, 6, 14, 14)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # C3: Convolution + activation\n",
        "        # Input: (batch, 6, 14, 14) -> Output: (batch, 16, 10, 10)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "\n",
        "        # S4: Average pooling\n",
        "        # Input: (batch, 16, 10, 10) -> Output: (batch, 16, 5, 5)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # C5: Convolution + activation (original 5x5 kernel design)\n",
        "        # Input: (batch, 16, 5, 5) -> Output: (batch, 120, 1, 1)\n",
        "        x = torch.tanh(self.conv3(x))\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        # Input: (batch, 120, 1, 1) -> Output: (batch, 120)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # F6: Fully connected + activation\n",
        "        # Input: (batch, 120) -> Output: (batch, 84)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "\n",
        "        # Output layer (no activation - raw logits)\n",
        "        # Input: (batch, 84) -> Output: (batch, 10)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u31fRXOc1XA6"
      },
      "source": [
        "Below is an alternative and more modern implementation. Here the activation functions have been switched from tanh to ReLU and Max Pooling is used instead of Average Pooling. Feel free to choose either!\n",
        "\n",
        "Unlike the CLIP model, that has been pretrained, these networks are trained from scratch. We will use train portion of the MNIST dataset to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OXvBhUE1Nqz"
      },
      "outputs": [],
      "source": [
        "# Alternative modern version with ReLU and MaxPooling\n",
        "class ModernLeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    Modernized version of LeNet-5 with ReLU activations and max pooling.\n",
        "    Often performs better on MNIST than the original version.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModernLeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(Fun.relu(self.conv1(x)))\n",
        "        x = self.pool(Fun.relu(self.conv2(x)))\n",
        "        x = Fun.relu(self.conv3(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = Fun.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Add dropout for regularization\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ga26CjkcMAH"
      },
      "source": [
        "## Obtain the Training Dataset from FiftyOne's Zoo\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8860bb12"
      },
      "source": [
        "To train our supervised model, we need the training split of the MNIST dataset. FiftyOne's Dataset Zoo provides a convenient way to load this data. We will load the training split and make it persistent so that any changes we make to the dataset (like adding tags or embeddings) are saved across sessions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NezpK2qr18h3"
      },
      "outputs": [],
      "source": [
        "# We use the training split to train our LeNet model\n",
        "# We make this dataset persistent as we want to save our changes for multiple sessions\n",
        "train_val_dataset = foz.load_zoo_dataset(\"mnist\",\n",
        "                                         split='train',\n",
        "                                         dataset_name=\"mnist-train-val\",\n",
        "                                         persistent=True)\n",
        "\n",
        "train_val_dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328v5V3E7nmk"
      },
      "source": [
        "## Splitting into Train and Validation\n",
        "\n",
        "The **validation set** serves as an unbiased evaluation mechanism during model development, acting as a proxy for real-world performance before touching the final test set. While the training set teaches the model to recognize patterns in handwritten digits, the validation set reveals whether the model has truly learned generalizable features or simply memorized the training data, a phenomenon known as overfitting.\n",
        "\n",
        "During training, we monitor both training and validation **loss** simultaneously. **Loss** is a numerical measure of how far off the model's predictions are from the correct answers - lower loss means better performance. We use categorical cross-entropy loss, which penalizes confident wrong predictions more heavily than uncertain ones. **Training loss** typically decreases steadily as the model learns, but **validation loss** tells the real story. If validation loss plateaus or begins increasing while training loss continues decreasing, the model is overfitting and memorizing training-specific details rather than learning robust digit recognition patterns. This signals when to stop training, adjust hyperparameters, or modify the architecture.\n",
        "\n",
        "The validation set also enables **hyperparameter tuning** without contaminating our final evaluation. We can experiment with different learning rates, batch sizes, regularization techniques, or architectural modifications, using validation loss to guide these decisions. Each configuration gets evaluated on the same held-out validation data, ensuring fair comparisons.\n",
        "\n",
        "**Important: the test set remains completely untouched** throughout the development process. Only after we've selected our final model configuration based on validation performance do we evaluate on the test set once, giving us an honest estimate of how the model will perform on truly unseen data. This three-way split (train/validation/test) is fundamental to responsible machine learning development and prevents the subtle \"data leakage\" that can make models appear better than they actually are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M9_3Gri26Wo"
      },
      "outputs": [],
      "source": [
        "# The images come with the 'train' tag and this must be deleted\n",
        "# at the sample level.\n",
        "train_val_dataset.untag_samples([\"train\", \"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YfPp1Huh2258"
      },
      "outputs": [],
      "source": [
        "set_seeds(51)\n",
        "# Create random 85%/15% split using tags\n",
        "four.random_split(train_val_dataset,\n",
        "                  {\"train\": 0.85, \"validation\": 0.15},\n",
        "                  # The seed makes the split reproducible\n",
        "                  seed=51)\n",
        "\n",
        "# Verify the split by counting tags\n",
        "tag_counts = train_val_dataset.count_sample_tags()\n",
        "print(\"Tag counts after split:\")\n",
        "print(tag_counts)\n",
        "\n",
        "# Separate validation and train FO datasets\n",
        "train_dataset = train_val_dataset.match_tags(\"train\").clone()\n",
        "val_dataset = train_val_dataset.match_tags(\"validation\").clone()\n",
        "\n",
        "# Set names for FO datasets using the 'name' property\n",
        "train_dataset.name = \"mnist-training-set\"\n",
        "val_dataset.name = \"mnist-validation-set\"\n",
        "\n",
        "# Define persistency\n",
        "train_dataset.persistent = True\n",
        "val_dataset.persistent = True\n",
        "\n",
        "# Verify no overlap between train and validation\n",
        "train_ids = set(train_dataset.values(\"id\"))\n",
        "val_ids = set(val_dataset.values(\"id\"))\n",
        "overlap = train_ids.intersection(val_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCWt129XuCb"
      },
      "source": [
        "## Moving the FiftyOne Data Splits to torch Datasets\n",
        "\n",
        "To train our PyTorch model, we need to convert our FiftyOne dataset views into PyTorch `Dataset` objects that can load and preprocess images during training. This bridge between FiftyOne's dataset management and PyTorch's training pipeline is important for maintaining both the metadata and annotations while enabling batch processing. PyTorch's `Dataset` class provides a standardized interface for data loading, handling tasks like image loading, preprocessing transforms, and label conversion. By creating a custom dataset class that works with FiftyOne's file paths and labels, we can leverage PyTorch's `DataLoader` for batching, shuffling, and parallel data loading while preserving all the dataset analysis capabilities that FiftyOne provides. This approach allows us to move between FiftyOne's visual analysis and PyTorch's training workflows without duplicating data or losing the rich metadata we've computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZu4eGYSNq-0"
      },
      "outputs": [],
      "source": [
        "# Custom PyTorch Dataset class for MNIST training data\n",
        "class CustomTorchImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fiftyone_dataset,\n",
        "                 image_transforms=None,\n",
        "                 label_map=None,\n",
        "                 gt_field=\"ground_truth\"):\n",
        "        self.fiftyone_dataset = fiftyone_dataset\n",
        "        self.image_paths = self.fiftyone_dataset.values(\"filepath\")\n",
        "        self.str_labels = self.fiftyone_dataset.values(f\"{gt_field}.label\")\n",
        "        self.image_transforms = image_transforms\n",
        "\n",
        "        if label_map is None:\n",
        "            self.label_map = {str(i): i for i in range(10)}  # \"0\"->0, \"1\"->1, etc.\n",
        "        else:\n",
        "            self.label_map = label_map\n",
        "\n",
        "        print(f\"CustomTorchImageDataset initialized with {len(self.image_paths)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('L')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return torch.randn(1, 28, 28), torch.tensor(-1, dtype=torch.long)\n",
        "\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "\n",
        "        label_str = self.str_labels[idx]\n",
        "        label_idx = self.label_map.get(label_str, -1)\n",
        "        if label_idx == -1:\n",
        "            print(f\"Warning: Label '{label_str}' not in label_map for image {image_path}\")\n",
        "\n",
        "        return image, torch.tensor(label_idx, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_F8qU_vFuS-"
      },
      "source": [
        "## Computing the Mean and Standard Deviation\n",
        "\n",
        "Before training neural networks, we compute the **mean and standard deviation** of our input data to apply **standard scaling** (also called normalization or standardization).\n",
        "\n",
        "\n",
        "**Standard Scaling Formula:**\n",
        "\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "- $z$ = standardized value\n",
        "- $x$ = original pixel value\n",
        "- $\\mu$ = mean of all pixel values in the dataset\n",
        "- $\\sigma$ = standard deviation of all pixel values in the dataset\n",
        "\n",
        "In PyTorch, this is implemented with torch.`transforms.Normalize((mean_intensity), (std_intensity))`\n",
        "\n",
        "This preprocessing step transforms our pixel values to have zero mean and unit variance, which provides several critical benefits:\n",
        "\n",
        "**Why Standard Scaling Matters:**\n",
        "\n",
        "**Gradient Optimization**: Neural networks learn through gradient descent, which works best when input features are on similar scales. Without scaling, features with larger magnitudes (like raw pixel values 0-255) can dominate the gradient updates, leading to slower convergence and unstable training.\n",
        "\n",
        "**Weight Initialization Compatibility**: Modern weight initialization schemes (Xavier, He initialization) assume inputs are roughly centered around zero with unit variance. Standard scaling ensures our data matches these assumptions, preventing vanishing or exploding gradients during early training.\n",
        "\n",
        "**Activation Function Efficiency**: Many activation functions (tanh, sigmoid) work optimally when inputs are centered around zero. Scaled inputs help neurons operate in the most responsive regions of these functions rather than saturating in flat regions.\n",
        "\n",
        "**Learning Rate Stability**: With standardized inputs, we can use higher learning rates without instability, as the optimization landscape becomes more uniform across different dimensions.\n",
        "\n",
        "For MNIST images, we transform raw pixel values from the range [0, 255] to approximately [-1, 1] with mean  0, creating a more favorable training environment that typically results in faster convergence and better final performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwV9Hgh-E-PX"
      },
      "outputs": [],
      "source": [
        "def compute_stats_fiftyone(fiftyone_view):\n",
        "    \"\"\"\n",
        "    Compute stats directly from FiftyOne using aggregations.\n",
        "    Requires images to be loaded as arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Computing image intensity statistics from FiftyOne view...\")\n",
        "\n",
        "    # Get all image filepaths\n",
        "    filepaths = fiftyone_view.values(\"filepath\")\n",
        "\n",
        "    # Load all pixel values\n",
        "    all_pixels = []\n",
        "\n",
        "    for filepath in tqdm(filepaths):\n",
        "\n",
        "        try:\n",
        "            # Load image as grayscale array\n",
        "            image = Image.open(filepath).convert('L')\n",
        "            # Scale values to the range [0, 1]\n",
        "            pixels = np.array(image, dtype=np.float32) / 255.0\n",
        "            all_pixels.append(pixels.flatten())\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Concatenate all pixel values\n",
        "    all_pixels = np.concatenate(all_pixels)\n",
        "\n",
        "    # Compute statistics\n",
        "    mean = np.mean(all_pixels)\n",
        "    std = np.std(all_pixels)\n",
        "\n",
        "    print(f\"Computed from {len(filepaths)} images\")\n",
        "    print(f\"Total pixels: {len(all_pixels):,}\")\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "mean_intensity, std_intensity = compute_stats_fiftyone(train_dataset)\n",
        "f\"Mean: {mean_intensity:.4f}, Std: {std_intensity:.4f}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eO92PNlTB_Gi"
      },
      "outputs": [],
      "source": [
        "# Map the string labels to numerical values (we need this for the PyTorch dataset)\n",
        "label_map = {string_label: index for index, string_label in enumerate(dataset_classes)}\n",
        "label_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u_VZ-eoIpa6"
      },
      "source": [
        "Transform the PIL images into PyTorch tensors with scaling based on stats from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xr_lCpPDvBV"
      },
      "outputs": [],
      "source": [
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZpMBS8MBbdN"
      },
      "outputs": [],
      "source": [
        "torch_train_set = CustomTorchImageDataset(train_dataset,\n",
        "                                          label_map=label_map,\n",
        "                                          image_transforms=image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98QofkjSBqiE"
      },
      "outputs": [],
      "source": [
        "torch_val_set = CustomTorchImageDataset(val_dataset,\n",
        "                                     label_map=label_map,\n",
        "                                     image_transforms=image_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4OX2J221s6u"
      },
      "source": [
        "### Create PyTorch DataLoaders\n",
        "\n",
        "DataLoaders wrap our custom datasets and handle the mechanics of training: batching samples together, shuffling data between epochs, and loading images in parallel using multiple CPU cores. The batch size determines how many images the model processes at once, affecting both memory usage and training dynamics. We shuffle the training data to prevent the model from learning spurious patterns based on sample order, but keep validation data unshuffled since evaluation order doesn't matter. Parallel loading with multiple workers speeds up training by preparing the next batch while the GPU processes the current one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb36104f"
      },
      "outputs": [],
      "source": [
        "# Define batch size (you can adjust this based on your GPU memory)\n",
        "batch_size = 64\n",
        "num_workers = os.cpu_count()  # Number of CPU cores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uiiduZxDxIZ"
      },
      "outputs": [],
      "source": [
        "train_loader = create_deterministic_training_dataloader(\n",
        "    torch_train_set,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wD9r7mMUDt7B"
      },
      "outputs": [],
      "source": [
        "val_loader = torch.utils.data.DataLoader(\n",
        "    torch_val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # No need to shuffle validation data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Train and validation DataLoaders created successfully.\")\n",
        "print(f\"Train DataLoader has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkUDiGQT-U4m"
      },
      "source": [
        "## Instantiate the Loss Function (Categorical Cross Entropy)\n",
        "\n",
        "**Categorical Cross Entropy** is the standard loss function for multi-class classification problems like MNIST digit recognition. It measures how far our model's predicted probability distribution is from the true distribution (one-hot encoded labels).\n",
        "\n",
        "**Mathematical Formula:**\n",
        "$$\\text{CCE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
        "\n",
        "Where:\n",
        "- $C$ = number of classes (10 for MNIST digits 0-9)\n",
        "- $y_i$ = true label (1 for correct class, 0 for others)\n",
        "- $\\hat{y}_i$ = predicted probability for class $i$\n",
        "\n",
        "**Intuitions about Cross Entropy Loss**: The loss encourages the model to output high confidence (probability close to 1.0) for the correct class and low confidence for incorrect classes.\n",
        "\n",
        "For a perfectly correct prediction (probability = 1.0 for true class), the loss approaches 0 and gradients are small (meaning that there is little change on weights). For very wrong predictions (probability = 0.001 for the ground truth class), the loss approaches infinity, strongly penalizing confident mistakes and forcing the neural network to update its weights.\n",
        "\n",
        "PyTorch implements a numerically stable variant of CCE based on the LogSoftMax function. You can read more about it [here](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00k7b9P--XdQ"
      },
      "outputs": [],
      "source": [
        "ce_loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK27g21hC68B"
      },
      "source": [
        "## Prepare Training and Validation for our Custom Model\n",
        "\n",
        "We define two functions to handle the training and validation phases of each epoch. The `train_epoch()` function puts the model in training mode, processes batches through forward passes, computes loss, and updates weights via backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvXszqWQ4dAn"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader):\n",
        "  batch_losses = []\n",
        "  model.train()\n",
        "  for images, labels in tqdm(train_loader, desc=\"Training: \"):\n",
        "      #import pdb; pdb.set_trace()\n",
        "\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      logits = model(images)\n",
        "      loss_value = ce_loss(logits, labels)\n",
        "      # Clear gradients from previous iteration (PyTorch accumulates by default)\n",
        "      optimizer.zero_grad()\n",
        "      # Computes the gradients with backpropagation\n",
        "      loss_value.backward()\n",
        "      # Updates the weights\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_losses.append(loss_value.item())\n",
        "\n",
        "  train_loss = np.mean(batch_losses)\n",
        "  return train_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFGZQrERUTEA"
      },
      "source": [
        "The `val_epoch()` function switches the model to evaluation mode and computes validation loss without updating weights, giving us an unbiased measure of performance on held-out data. These functions return the average loss across all batches, which we'll track to monitor training progress and detect overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAf9bjcX51pf"
      },
      "outputs": [],
      "source": [
        "def val_epoch(model, val_loader):\n",
        "  batch_losses = []\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Validation: \"):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      logits = model(images)\n",
        "      loss_value = ce_loss(logits, labels)\n",
        "      batch_losses.append(loss_value.item())\n",
        "  val_loss = np.mean(batch_losses)\n",
        "  return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK52LQDkDRfW"
      },
      "source": [
        "## Defining the Optimizer\n",
        "\n",
        "**Configuring the Learning Algorithm**\n",
        "\n",
        "The optimizer determines how the neural network updates its weights based on computed gradients during training. This choice affects training speed, stability, and final model performance.\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)** is a variant of gradient descent that maintains running averages of both gradients and their squared values, allowing us to adapt the learning rate based on the historical behavior of each weight.\n",
        "\n",
        "```python\n",
        "optimizer = Adam(model.parameters(),\n",
        "                 lr=0.003, betas=(0.9, 0.999),\n",
        "                 eps=1e-08, weight_decay=0)\n",
        "```\n",
        "\n",
        "The learning rate (lr=0.003) controls the step size for weight updates, while the beta settings control how much history to consider when computing the adaptive rates. This configuration provides stable training for most computer vision tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-SPncALBtCi"
      },
      "outputs": [],
      "source": [
        "model = ModernLeNet5().to(device)\n",
        "\n",
        "# Define the optimizer (variant of stochastic gradient descent)\n",
        "optimizer = Adam(model.parameters(),\n",
        "                 lr=0.003, betas=(0.9, 0.999),\n",
        "                 eps=1e-08, weight_decay=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqo9S2i7RjDl"
      },
      "source": [
        "## Training and Checkpointing the Model\n",
        "\n",
        "**Iterative Learning with Performance Monitoring**\n",
        "\n",
        "Training a neural network involves showing the model batches of data, computing prediction errors, and updating weights to minimize those errors. This process continues for multiple epochs, where each epoch represents one complete pass through the entire training dataset.\n",
        "\n",
        "**The Role of Validation-Based Checkpointing**\n",
        "\n",
        "During training, we monitor performance on both training and validation sets. Training loss decreases as the model learns, but validation loss reveals the true generalization capability. The validation set acts as a proxy for real-world performance since the model never sees these samples during weight updates.\n",
        "\n",
        "We save model checkpoints based on validation performance rather than training performance to prevent overfitting. A model might achieve low training loss by memorizing training examples, but this doesn't guarantee good performance on new data. By saving the model weights that achieve the best validation loss, we capture the point where the model has learned generalizable patterns without overfitting to training-specific details.\n",
        "\n",
        "```python\n",
        "if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    best_model = model\n",
        "    torch.save(best_model.state_dict(), model_save_path)\n",
        "    print('Found and saved better weights for the model')\n",
        "```\n",
        "\n",
        "This checkpointing strategy ensures we retain the model configuration that will perform best on unseen test data, even if training continues and validation performance later degrades due to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT-TdBp8KGuI"
      },
      "outputs": [],
      "source": [
        "# Ensure reproducibility for the training process\n",
        "set_seeds(51) # You can change this number to get different results\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "\n",
        "# Define the path to save the model within your hard-drive\n",
        "path = Path(os.getcwd()) # Feel free to change the path\n",
        "\n",
        "model_save_path = path / 'best_lenet.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader)\n",
        "    val_loss = val_epoch(model, val_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "        # Save the best model\n",
        "        torch.save(best_model.state_dict(), model_save_path)\n",
        "        print('Found and saved better weights for the model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGC7oxwM9AGV"
      },
      "source": [
        "## Visualizing Training vs Validation Losses\n",
        "\n",
        "Plotting training and validation loss over epochs provides insights into model learning dynamics. These curves reveal whether the model is learning, overfitting, or underfitting the data.\n",
        "In healthy training, both curves decrease together, with training loss lower than validation loss. When training loss continues decreasing while validation loss plateaus or increases, this indicates overfitting where the model memorizes training data rather than learning generalizable patterns. If both curves plateau at high values, the model may be underfitting and require more capacity (e.g.more layers or more weights) or more training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ponD-bYqKvQy"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "# Find the epoch with the best validation loss\n",
        "best_epoch = np.argmin(val_losses) + 1 # Add 1 because epochs are 1-indexed\n",
        "\n",
        "# Add a vertical red line at the epoch with the best validation loss\n",
        "plt.axvline(x=best_epoch, color='red', linestyle='--', label=f'Best Val Loss Epoch {best_epoch}')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUY0t6oPcmJH"
      },
      "source": [
        "## Reload the Best LeNet Model Weights\n",
        "\n",
        "After training completes, we need to load the saved model weights that achieved the best validation performance. This ensures we use the model configuration that generalizes best to unseen data, rather than the final training state which may have overfitted.\n",
        "The reloading process involves creating a new model instance with the same architecture, then loading the saved state dictionary containing the optimal weights. We must ensure the model is moved to the correct device (CPU or GPU) and set to evaluation mode for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzA4y5mpOImr"
      },
      "outputs": [],
      "source": [
        "# Define the path where the best model was saved\n",
        "path = Path(os.getcwd())\n",
        "model_save_path = path / 'best_lenet.pth'\n",
        "\n",
        "# Instantiate a new model with the same architecture\n",
        "# Make sure you use the same model class that was trained\n",
        "loaded_model = ModernLeNet5()\n",
        "\n",
        "# Load the saved state dictionary into the new model instance\n",
        "# Make sure the model is on the correct device (CPU or GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "\n",
        "# Move the model to the device\n",
        "loaded_model = loaded_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "print(f\"Model loaded successfully from {model_save_path}\")\n",
        "print(f\"Model is on device: {next(loaded_model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VVt65BMRbZo"
      },
      "source": [
        "## Apply our Custom Model to the Test Set\n",
        "\n",
        "We have trained our LeNet-5 model and need to apply it to the test set to generate predictions and evaluate its performance. This step bridges the gap between PyTorch training and FiftyOne's analysis capabilities.\n",
        "\n",
        "**Why Store Predictions as FiftyOne Classifications?**\n",
        "\n",
        "Instead of storing raw predictions as strings or numbers, we use FiftyOne's [`Classification`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) objects, which provide several key advantages:\n",
        "\n",
        "**Structured Data Storage**: Classification objects encapsulate the predicted label, confidence score, and raw logits in a standardized format that FiftyOne understands.\n",
        "\n",
        "**Evaluation Integration**: FiftyOne's evaluation framework (`evaluate_classifications()`) can compare Classification objects against ground truth labels, generating metrics like confusion matrices, per-class accuracy, and performance reports.\n",
        "\n",
        "**Querying and Filtering**: With Classification objects, we can filter samples by confidence thresholds, find misclassifications, or identify uncertain predictions using FiftyOne's query language:\n",
        "\n",
        "```python\n",
        "# Find high-confidence predictions\n",
        "high_conf = dataset.match(F(\"predictions.confidence\") > 0.95)\n",
        "\n",
        "# Find misclassifications  \n",
        "errors = dataset.match(F(\"predictions.label\") != F(\"ground_truth.label\"))\n",
        "```\n",
        "\n",
        "**Visual Analysis**: The FiftyOne App can visualize Classification objects with confidence scores, making it simple to spot patterns in model behavior and identify errors.\n",
        "\n",
        "**Model Comparison**: Storing predictions in this standardized format enables comparison between different models (like our LeNet vs. CLIP's zero-shot classification) using the same evaluation framework.\n",
        "\n",
        "**Confidence-Based Analysis**: The embedded confidence scores allow for analysis like identifying samples where the model is uncertain, which correspond to edge cases or potential labeling errors in the dataset.\n",
        "\n",
        "This approach transforms raw model outputs into queryable metadata that integrates with FiftyOne's computer vision workflow, enabling insights into model performance and behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtL5ZfG6YYXe"
      },
      "outputs": [],
      "source": [
        "## Apply best_model to the test set, store logits and confidence\n",
        "\n",
        "# Create a PyTorch Dataset for the test set\n",
        "torch_test_set = CustomTorchImageDataset(test_dataset,\n",
        "                                      label_map=label_map, # Use the same label map as training\n",
        "                                      image_transforms=image_transforms) # Use the same transforms\n",
        "\n",
        "# Create a PyTorch DataLoader for the test set\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torch_test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Test DataLoader created successfully.\")\n",
        "print(f\"Test DataLoader has {len(test_loader)} batches.\")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Lists to store predictions and logits\n",
        "predictions = []\n",
        "all_logits = []\n",
        "\n",
        "# Run inference on the test set\n",
        "print(\"Applying best LeNet model to the test set...\")\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    for images, _ in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "all_logits = np.concatenate(all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on test set complete.\")\n",
        "print(f\"Shape of collected logits: {all_logits.shape}\")\n",
        "print(f\"Number of collected predictions: {len(predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications...\")\n",
        "\n",
        "for i, sample in enumerate(test_dataset):\n",
        "    # Get the predicted class index and corresponding class name\n",
        "    predicted_idx = predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "\n",
        "    # Get logits for this sample\n",
        "    sample_logits = all_logits[i]\n",
        "\n",
        "    # Calculate confidence scores (softmax applied to logits)\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    # Create FiftyOne Classification object with prediction\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()  # Store raw logits\n",
        "    )\n",
        "\n",
        "    # Store the Classification object in the sample\n",
        "    sample[\"lenet_classification\"] = classification\n",
        "\n",
        "    sample.save()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0LdkBKqY4FT"
      },
      "source": [
        "### Verify the Stored Data Structure for Predictions\n",
        "We should see `Classification` objects with label, confidence, and logits fields. We need these to perform analytics on our FiftyOne dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgBctL3WY61A"
      },
      "outputs": [],
      "source": [
        "print(\"\\n=== Verification ===\")\n",
        "sample = test_dataset.first()\n",
        "print(f\"Sample prediction type: {type(sample.lenet_classification)}\")\n",
        "print(f\"Sample prediction: {sample.lenet_classification}\")\n",
        "print(f\"Prediction label: {sample.lenet_classification.label}\")\n",
        "print(f\"Prediction confidence: {sample.lenet_classification.confidence}\")\n",
        "print(f\"Prediction logits shape: {len(sample.lenet_classification.logits)}\")\n",
        "\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWumhgwGZSNE"
      },
      "source": [
        "### Create a View Showing only LeNet's Mistakes\n",
        "\n",
        "The FiftyOne `Classification` object allows to do filtering on the samples where we have issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBzGnUgIaYBj"
      },
      "outputs": [],
      "source": [
        "# Create a view showing only LeNet's misclassifications\n",
        "misclassified_view = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "print(f\"Misclassified samples: {len(misclassified_view)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAUyD2N1Z039"
      },
      "source": [
        "## Evaluating LeNet's Classification Performance\n",
        "\n",
        "After applying our trained LeNet model to the test set, we need to evaluate its performance against the ground truth labels. This evaluation goes beyond simple accuracy to provide detailed insights into where and how the model succeeds or fails.\n",
        "FiftyOne's evaluation framework generates metrics including per-class precision, recall, and F1-scores, along with confusion matrices that reveal which digit pairs the model most often confuses. This analysis helps identify weaknesses and guides future improvements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbdrK2HVdL6t"
      },
      "outputs": [],
      "source": [
        "lenet_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"lenet_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"lenet_eval\")\n",
        "\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdP_CpjJhPRZ"
      },
      "outputs": [],
      "source": [
        "lenet_evaluation_results.print_report(digits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jahmUa2_SuM2"
      },
      "outputs": [],
      "source": [
        "lenet_evaluation_results.plot_confusion_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o0Y0qZVhAqOn"
      },
      "outputs": [],
      "source": [
        "# Compute quantiles of confidence\n",
        "test_dataset.quantiles(\"lenet_classification.confidence\", [0.01, 0.05, 0.25, 0.5, 0.75] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JleYduhreBg6"
      },
      "outputs": [],
      "source": [
        "low_confidence_view = test_dataset.match(F(\"lenet_classification.confidence\") < 0.69)\n",
        "low_confidence_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKnI5jJceXMK"
      },
      "outputs": [],
      "source": [
        "session.view = low_confidence_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eSfe8gg-yF_"
      },
      "source": [
        "## Evaluating Sample Hardness and Mistakenness through Logits\n",
        "\n",
        "**Quantifying Prediction Uncertainty and Label Quality**\n",
        "\n",
        "Model logits contain rich information beyond simple predictions. By analyzing the raw output scores before softmax conversion (the logits), we can compute metrics that reveal which samples the model finds challenging and which labels may be questionable.\n",
        "\n",
        "**Hardness** measures prediction uncertainty based on the model's confidence distribution. Samples with high hardness have flat probability distributions, indicating the model struggles to distinguish between classes. These often represent genuinely difficult cases or edge cases in the data.\n",
        "\n",
        "**Mistakenness** identifies samples where the model's confident predictions disagree with ground truth labels. High mistakenness scores suggest potential annotation errors rather than model failures, as the model may have learned correct patterns that conflict with incorrect labels.\n",
        "\n",
        "```python\n",
        "# Compute hardness based on prediction uncertainty\n",
        "fob.compute_hardness(test_dataset, label_field='lenet_classification')\n",
        "\n",
        "# Compute mistakenness to identify potential label errors  \n",
        "fob.compute_mistakenness(test_dataset,\n",
        "                        pred_field=\"lenet_classification\",\n",
        "                        label_field=\"ground_truth\")\n",
        "```\n",
        "\n",
        "These metrics transform raw model scores (logits) into actionable insights for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv9eUU8H7W8Q"
      },
      "outputs": [],
      "source": [
        "#Hardness is a measure computed based on model prediction output (through\n",
        "#logits) that summarizes a measure of the uncertainty the model had with the\n",
        "#sample. This makes hardness quantitative and can be used to detect things\n",
        "#like hard samples and annotation errors\n",
        "fob.compute_hardness(test_dataset,\n",
        "                     label_field='lenet_classification',\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnVwG6OHpQ7Z"
      },
      "outputs": [],
      "source": [
        "# Evaluate sample mistakenness (how likely the sample is mislabeled)\n",
        "# Samples with high mistakenness often have conflicting model output and ground truth\n",
        "fob.compute_mistakenness(test_dataset,\n",
        "                         pred_field=\"lenet_classification\",\n",
        "                         label_field=\"ground_truth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "B6UHt8iEBFsq"
      },
      "outputs": [],
      "source": [
        "# Compute quantiles of mistakenness\n",
        "mistakenness_quantiles = test_dataset.quantiles(\n",
        "    \"mistakenness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nMistakenness quantiles for train_dataset:\")\n",
        "print(mistakenness_quantiles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ykeRfSFCgah"
      },
      "outputs": [],
      "source": [
        "suspicious_test_samples_view = test_dataset.match(\n",
        "                             F(\"mistakenness\") > mistakenness_quantiles[-1]\n",
        "                             ).sort_by(\"mistakenness\", reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cHQbbD7Bms8"
      },
      "outputs": [],
      "source": [
        "# Set the session view to one of the views to visualize it in the App\n",
        "session.view = suspicious_test_samples_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL showing top mistakenness in the test set: {session.url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K2DNWgg-qSj"
      },
      "source": [
        "## Add Classifications to the Validation Dataset\n",
        "\n",
        "We need to have Classification objects on the validation set to evaluate the quality of the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUQeg3dY-xSP"
      },
      "outputs": [],
      "source": [
        "print(\"Applying best LeNet model to the validation set...\")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "\n",
        "# Lists to store predictions and logits\n",
        "val_predictions = []\n",
        "val_all_logits = []\n",
        "\n",
        "# Run inference on the validation set\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    for images, _ in tqdm(val_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        val_all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        val_predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "val_all_logits = np.concatenate(val_all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on validation set complete.\")\n",
        "print(f\"Shape of collected validation logits: {val_all_logits.shape}\")\n",
        "print(f\"Number of collected validation predictions: {len(val_predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications for validation set...\")\n",
        "\n",
        "for i, sample in enumerate(val_dataset):\n",
        "    # Get the predicted class index and corresponding class name\n",
        "    predicted_idx = val_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "\n",
        "    # Get logits for this sample\n",
        "    sample_logits = val_all_logits[i]\n",
        "\n",
        "    # Calculate confidence scores (softmax applied to logits)\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    # Create FiftyOne Classification object with prediction\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()  # Store raw logits\n",
        "    )\n",
        "\n",
        "    # Store the Classification object in the sample\n",
        "    sample[\"lenet_validation_classification\"] = classification\n",
        "\n",
        "    sample.save()\n",
        "\n",
        "print(\"Predictions and logits stored successfully as FiftyOne Classifications for validation set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmZvGqsLAwuC"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Evaluate sample hardness (how hard the sample is for the model)\n",
        "# Hardness is a measure computed based on model prediction output (through\n",
        "# logits) that summarizes a measure of the uncertainty the model had with the\n",
        "# sample. This makes hardness quantitative and can be used to detect things\n",
        "# like hard samples and annotation errors\n",
        "fob.compute_hardness(val_dataset,\n",
        "                     label_field='lenet_validation_classification',\n",
        "                     )\n",
        "\n",
        "# Evaluate sample mistakenness (how likely the sample is mislabeled)\n",
        "# Samples with high mistakenness often have conflicting model output and ground truth\n",
        "fob.compute_mistakenness(val_dataset,\n",
        "                         pred_field=\"lenet_validation_classification\",\n",
        "                         label_field=\"ground_truth\")\n",
        "\n",
        "# Compute quantiles of mistakenness for the validation set\n",
        "mistakenness_quantiles_val = val_dataset.quantiles(\n",
        "    \"mistakenness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nMistakenness quantiles for validation_dataset:\")\n",
        "print(mistakenness_quantiles_val)\n",
        "\n",
        "# Compute quantiles of hardness for the validation set\n",
        "hardness_quantiles_val = val_dataset.quantiles(\n",
        "    \"hardness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nHardness quantiles for validation_dataset:\")\n",
        "print(hardness_quantiles_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqMQrU09As8L"
      },
      "outputs": [],
      "source": [
        "# Create a view of samples with high mistakenness in the validation set\n",
        "suspicious_val_samples_view = val_dataset.match(\n",
        "                             F(\"mistakenness\") > mistakenness_quantiles_val[-1]\n",
        "                             ).sort_by(\"mistakenness\", reverse=True)\n",
        "\n",
        "# Set the session view to one of the views to visualize it in the App\n",
        "session.view = suspicious_val_samples_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL showing top mistakenness in the validation set: {session.url}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQVHjaBsCA9H"
      },
      "outputs": [],
      "source": [
        "bad_val_images_paths = set([\n",
        "    \"/root/fiftyone/mnist/train/data/005973.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/026623.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/025160.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/010383.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/031607.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/027027.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/013185.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/041291.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/032343.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/049961.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/031851.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/007348.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/035311.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/030417.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/008385.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/016749.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/031743.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/023869.jpg\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqxRHKEyAoot"
      },
      "outputs": [],
      "source": [
        "# Create a view of samples with high hardness in the validation set\n",
        "hard_val_samples_view = val_dataset.match(\n",
        "                         F(\"hardness\") > hardness_quantiles_val[-1]\n",
        "                         ).sort_by(\"hardness\", reverse=True)\n",
        "\n",
        "# Set the session view to visualize hard samples\n",
        "session.view = hard_val_samples_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL showing top hardness in the validation set: {session.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAQh1hXIrSS7"
      },
      "source": [
        "## Creating Embeddings from the LeNet-5 Model\n",
        "\n",
        "**Extracting Internal Feature Representations**\n",
        "\n",
        "Neural networks process information through multiple layers, with each layer transforming input data into increasingly abstract representations. While we typically focus on the final output layer for predictions, the intermediate layers contain valuable feature representations that reveal what the model has learned about the input data.\n",
        "\n",
        "LeNet-5's architecture builds features hierarchically: early convolutional layers detect edges and simple patterns, while deeper layers combine these into more complex digit-specific features. The fully connected layers before the final classification layer contain rich embeddings that capture the essential characteristics the model uses to distinguish between digit classes.\n",
        "\n",
        "By extracting these learned embeddings, we can analyze how our trained model represents each digit internally. Unlike pre-trained embeddings from models like CLIP, these features are optimized for our specific task of handwritten digit recognition. This makes them particularly valuable for understanding model behavior, identifying challenging samples, and comparing how different digits cluster in the learned feature space.\n",
        "\n",
        "The extraction process uses PyTorch hooks to capture intermediate layer outputs during inference, allowing us to access the 84-dimensional feature vectors from the penultimate layer that encode each sample's representation in the model's learned space.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxsL2g-Rstpq"
      },
      "outputs": [],
      "source": [
        "## Extract Embeddings from LeNet Model Using PyTorch Hooks\n",
        "\n",
        "def extract_lenet_embeddings(model, dataloader, device, layer_name='fc1'):\n",
        "    \"\"\"\n",
        "    Extract embeddings from a specified layer of the LeNet model using PyTorch hooks.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LeNet model\n",
        "        dataloader: PyTorch DataLoader\n",
        "        device: Device to run inference on\n",
        "        layer_name: Name of the layer to extract embeddings from\n",
        "                   Options: 'conv3', 'fc1', or 'fc2'\n",
        "\n",
        "    Returns:\n",
        "        numpy array of embeddings\n",
        "    \"\"\"\n",
        "    # Dictionary to store the embeddings\n",
        "    embeddings_dict = {}\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        \"\"\"Hook function to capture layer outputs\"\"\"\n",
        "        # Flatten the output if it's from conv layers\n",
        "        if len(output.shape) > 2:\n",
        "            embeddings_dict['embeddings'] = output.view(output.size(0), -1).cpu().detach()\n",
        "        else:\n",
        "            embeddings_dict['embeddings'] = output.cpu().detach()\n",
        "\n",
        "    # Register the hook on the specified layer\n",
        "    layer_map = {\n",
        "        'conv3': model.conv3,  # Shape: (batch_size, 120, 1, 1) -> flattened to (batch_size, 120)\n",
        "        'fc1': model.fc1,     # Shape: (batch_size, 84) - most common choice\n",
        "        'fc2': model.fc2      # Shape: (batch_size, 10) - final logits\n",
        "    }\n",
        "\n",
        "    if layer_name not in layer_map:\n",
        "        raise ValueError(f\"Invalid layer_name. Choose from: {list(layer_map.keys())}\")\n",
        "\n",
        "    target_layer = layer_map[layer_name]\n",
        "    hook_handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    print(f\"Extracting embeddings from {layer_name} layer...\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, _ in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass (hook will capture the embeddings)\n",
        "            _ = model(images)\n",
        "\n",
        "            # Store the captured embeddings\n",
        "            batch_embeddings = embeddings_dict['embeddings'].numpy()\n",
        "            all_embeddings.append(batch_embeddings)\n",
        "\n",
        "    # Remove the hook to clean up\n",
        "    hook_handle.remove()\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "\n",
        "    print(f\"Extracted embeddings shape: {final_embeddings.shape}\")\n",
        "    print(f\"Embedding dimension: {final_embeddings.shape[1]}\")\n",
        "\n",
        "    return final_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_L9gSqK6mg5"
      },
      "source": [
        "# Create a new DataLoader for the train_set  for inference\n",
        "\n",
        "It's important to ensure that shuffling is disabled for this training data `DataLoader`, otherwise our predictions will not match the corresponding data points when we store them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d73fcf8b"
      },
      "outputs": [],
      "source": [
        "# This uses the torch_train_set which is derived from train_dataset\n",
        "train_inference_loader = torch.utils.data.DataLoader(\n",
        "    torch_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,    # CRITICAL: Must be False for ordered predictions\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Lists to store predictions and logits for the training set\n",
        "train_predictions = []\n",
        "train_all_logits = []\n",
        "\n",
        "# Run inference on the training set using the NON-SHUFFLED loader\n",
        "print(\"Applying LeNet model to the train_dataset (using non-shuffled loader)...\")\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    # Use the new train_inference_loader\n",
        "    for images, _ in tqdm(train_inference_loader, desc=\"Processing train batches for inference\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        train_all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        train_predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "train_all_logits = np.concatenate(train_all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on train_dataset complete.\")\n",
        "print(f\"Shape of collected train logits: {train_all_logits.shape}\")\n",
        "print(f\"Number of collected train predictions: {len(train_predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications for train_dataset...\")\n",
        "for i, sample in enumerate(tqdm(train_dataset, desc=\"Storing train classifications\")):\n",
        "    predicted_idx = train_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx] # Assuming dataset_classes is consistent\n",
        "    sample_logits = train_all_logits[i]\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()\n",
        "    )\n",
        "    sample[\"lenet_train_classification\"] = classification\n",
        "    sample.save()\n",
        "\n",
        "print(\"Predictions and logits stored successfully as FiftyOne Classifications for train_dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1RwWM6aa19b"
      },
      "outputs": [],
      "source": [
        "lenet_train_evaluation_results = train_dataset.evaluate_classifications(\n",
        "    \"lenet_train_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"lenet_train_eval\"\n",
        ")\n",
        "session.refresh()\n",
        "lenet_train_evaluation_results.print_report(digits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MimuXxRsTvP4"
      },
      "outputs": [],
      "source": [
        "session.view = train_dataset.view()\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOivFC0taQk6"
      },
      "source": [
        "## Creating a View of the Mislabeled Images in the Training Data\n",
        "\n",
        "**Identifying Potential Annotation Errors Through Model Predictions**\n",
        "\n",
        "When a trained model disagrees with ground truth labels, this often reveals annotation errors rather than model failures. A well-trained neural network that has learned robust patterns from thousands of examples may be more reliable than human annotators on ambiguous cases. This principle becomes valuable for quality control in large datasets where manual verification of every sample is impractical.\n",
        "\n",
        "**The Logic Behind Mislabel Detection**\n",
        "\n",
        "Models learn statistical patterns across the entire dataset. When a model with high accuracy predicts a different label than the ground truth, several scenarios are possible: the model made an error, the annotation is incorrect, or the sample is ambiguous. By examining these disagreements, we can identify samples that warrant human review.\n",
        "\n",
        "For MNIST, known annotation errors exist in the original dataset. Research has identified several hundred ambiguous or mislabeled samples where even human experts disagree on the correct digit. Our trained LeNet model, having learned from 50,000+ examples, may detect these problematic cases more than the original annotators.\n",
        "\n",
        "Creating a filtered view of these disagreements allows us to focus human attention on the most questionable samples. This approach scales annotation quality control from reviewing entire datasets to examining only the cases where model and annotator disagree.\n",
        "\n",
        "This approach to finding annotation errors represents a powerful application of model predictions for dataset improvement, transforming disagreements between model and labels into opportunities for enhanced data quality.\n",
        "\n",
        "We focus on the mislabeled images from the training set, as these are the ones that we should first **augment** to improve the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auV2OrzEFzdp"
      },
      "outputs": [],
      "source": [
        "# A view of mislabeled images from the training set\n",
        "mislabeled_train_images_view = \\\n",
        "train_dataset.match(\n",
        "    F(\"lenet_train_classification.label\")!= F(\"ground_truth.label\"))\n",
        "\n",
        "mislabeled_train_images_view\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX7nYlhciQHW"
      },
      "outputs": [],
      "source": [
        "session.view = mislabeled_train_images_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9rqX4iDi6Tq"
      },
      "source": [
        "## Extract Embeddings from the Trained LeNet\n",
        "\n",
        "**Capturing Learned Feature Representations**\n",
        "\n",
        "Neural networks learn hierarchical feature representations through their layers. While the final layer produces class predictions, intermediate layers contain rich embeddings that capture the visual patterns the model has learned to distinguish digits. These learned features often differ from pre-trained embeddings like CLIP, as they are optimized for the specific task of handwritten digit recognition.\n",
        "\n",
        "Extracting embeddings from our trained LeNet model allows us to analyze what visual concepts the network has learned. The fully connected layer before the final classification layer (fc1) produces 84-dimensional vectors that represent each digit in the feature space that the model uses for decision-making.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkdkbeSFuwKI"
      },
      "outputs": [],
      "source": [
        "print(\"Creating non-shuffled DataLoader for embedding extraction...\")\n",
        "\n",
        "# Create a DataLoader specifically for inference with NO SHUFFLING\n",
        "train_inference_loader = torch.utils.data.DataLoader(\n",
        "    torch_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Non-shuffled inference DataLoader created with {len(train_inference_loader)} batches\")\n",
        "\n",
        "# Extract embeddings from the fc1 layer (84-dimensional representations)\n",
        "print(\"Extracting LeNet embeddings with proper sample ordering...\")\n",
        "\n",
        "lenet_embeddings = extract_lenet_embeddings(\n",
        "    model=loaded_model,\n",
        "    dataloader=train_inference_loader,  #  Use non-shuffled loader\n",
        "    device=device,\n",
        "    layer_name='fc1'  # 84-dimensional embeddings from fully connected layer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq7o80pLz5cd"
      },
      "outputs": [],
      "source": [
        "# Wrap embeddings to their associated filenames ()\n",
        "embeddings_dict = {img_path: emb for img_path, emb in zip(torch_train_set.image_paths, lenet_embeddings)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IErbAydN5hRz"
      },
      "source": [
        "## Store LeNet Embeddings in FiftyOne Dataset\n",
        "\n",
        "**Integrating Model Features with Dataset Metadata**\n",
        "\n",
        "After extracting embeddings from our trained LeNet model, we need to store them in our FiftyOne dataset to enable analysis and visualization. This integration allows us to leverage FiftyOne's powerful querying and visualization capabilities on the learned feature representations.\n",
        "\n",
        "Storing embeddings as sample fields transforms abstract neural network features into queryable fields. We can then compute similarity indices, create visualizations, and analyze how the model's learned representations relate to prediction accuracy and sample characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iXPUdi3u2v3"
      },
      "outputs": [],
      "source": [
        "print(\"Storing LeNet embeddings in FiftyOne dataset...\")\n",
        "\n",
        "# Store embeddings in each sample\n",
        "for index, sample in enumerate(tqdm(train_dataset, desc=\"Storing embeddings\")):\n",
        "    sample[\"lenet_embeddings\"] = lenet_embeddings[index]\n",
        "    sample.save()\n",
        "\n",
        "print(\"LeNet embeddings stored successfully in samples from train_dataset.\")\n",
        "\n",
        "# Verify storage\n",
        "sample = train_dataset.first()\n",
        "print(f\"Sample LeNet embedding shape: {sample.lenet_embeddings.shape}\")\n",
        "print(f\"Embedding type: {type(sample.lenet_embeddings)}\")\n",
        "\n",
        "# Persisting the change\n",
        "train_dataset.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y41hy9q04zov"
      },
      "outputs": [],
      "source": [
        "train_dataset.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vglV17Mq4Pi4"
      },
      "outputs": [],
      "source": [
        "train_dataset.get_field_schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqq_Ymg8brY"
      },
      "source": [
        "## Create a Similarity Index Based on the CNN embeddings\n",
        "\n",
        "After extracting embeddings from our trained LeNet model, we create a **similarity index**. This index allows for efficient searching of samples based on their learned feature representations. Similar to the CLIP similarity index, this enables us to quickly find training images that are visually similar according to what the LeNet model has learned, which is useful for exploring clusters and identifying related samples in the model's feature space.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsu1gMZe8Kvm"
      },
      "outputs": [],
      "source": [
        "print(\"Computing similarity index based on LeNet embeddings...\")\n",
        "\n",
        "lenet_similarity_index = fob.compute_similarity(\n",
        "    train_dataset,  # Using the training dataset\n",
        "    embeddings=\"lenet_embeddings\",  # Field containing the LeNet embeddings\n",
        "    brain_key=\"lenet_cosine_similarity_index\",  # Unique identifier for this similarity index\n",
        "    backend=\"sklearn\",  # Can also use \"pinecone\" for large datasets\n",
        "    metric=\"cosine\"  # Similarity metric\n",
        ")\n",
        "\n",
        "print(f\"LeNet similarity index computed successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neWE1NCOKjKV"
      },
      "outputs": [],
      "source": [
        "# Display similarity index information\n",
        "print(f\"LeNet Similarity Index Details:\")\n",
        "print(f\"- Total samples indexed: {len(train_dataset)}\")\n",
        "print(f\"- Embeddings field: {lenet_similarity_index.config.embeddings_field}\")\n",
        "print(f\"- Metric: {lenet_similarity_index.config.metric}\")\n",
        "\n",
        "# Example 1: Find similar images to a specific digit using LeNet embeddings\n",
        "# Let's pick a sample from the training dataset\n",
        "query_sample = train_dataset.first()\n",
        "print(f\"\"\"\\nQuerying for images similar to sample: {query_sample.id}\n",
        "            with label {query_sample.ground_truth.label}\n",
        "            using LeNet embeddings\"\"\")\n",
        "\n",
        "# Use the sample ID (string) instead of the sample object\n",
        "similar_view_lenet = train_dataset.sort_by_similarity(\n",
        "    query_sample.id,  # Pass the sample ID\n",
        "    brain_key=\"lenet_cosine_similarity_index\",\n",
        "    k=10,  # Return top 10 most similar\n",
        "    reverse=False  # Most similar first\n",
        ")\n",
        "\n",
        "print(f\"Found {len(similar_view_lenet)} most similar samples using LeNet embeddings\")\n",
        "\n",
        "# Update FiftyOne App view to show similar samples\n",
        "session.view = similar_view_lenet\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FISLQ-LuSlaS"
      },
      "source": [
        "## Evaluating Image Representativeness and Uniqueness through their Embeddings\n",
        "\n",
        "**Image representativeness** and **uniqueness** are metrics for understanding dataset quality and composition that work hand-in-hand with clustering analysis.\n",
        "\n",
        "**Representativeness** measures how well a sample captures the central characteristics of its cluster, highly representative images sit near cluster centers and exemplify the common visual patterns within each group.\n",
        "\n",
        "**Uniqueness** identifies samples that are distant from any cluster centers, often representing edge cases, rare scenarios, or potential annotation errors that clustering algorithms struggle to categorize.\n",
        "\n",
        "These scores are normalized to [0, 1] and become particularly insightful when viewed alongside clustering results. Representative samples serve as ideal cluster exemplars for understanding what each group represents, while highly unique samples often fall between clusters or form singleton groups, indicating unusual data points that may require special handling in our machine learning pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huEFWGbiu6kV"
      },
      "outputs": [],
      "source": [
        "## Compute Uniqueness and Representativeness for LeNet Embeddings\n",
        "\n",
        "print(\"Computing uniqueness scores based on LeNet embeddings...\")\n",
        "fob.compute_uniqueness(train_dataset, embeddings='lenet_embeddings')\n",
        "\n",
        "print(\"Computing representativeness scores based on LeNet embeddings...\")\n",
        "fob.compute_representativeness(train_dataset, embeddings='lenet_embeddings')\n",
        "\n",
        "print(\"Uniqueness and representativeness computation complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsJ4gY-7Fda7"
      },
      "outputs": [],
      "source": [
        "# Compute mistakenness on samples on the train_set\n",
        "# Mistakenness identifies samples where the model's confident predictions disagree with ground truth labels.\n",
        "# High mistakenness scores suggest potential annotation errors rather than model failures.\n",
        "fob.compute_mistakenness(train_dataset,\n",
        "                         pred_field=\"lenet_train_classification\",\n",
        "                         label_field=\"ground_truth\")\n",
        "\n",
        "print(\"Mistakenness computed successfully for train_dataset.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgK1sJDHHEgr"
      },
      "outputs": [],
      "source": [
        "# Compute quantiles of mistakenness\n",
        "mistakenness_quantiles = train_dataset.quantiles(\n",
        "    \"mistakenness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nMistakenness quantiles for train_dataset:\")\n",
        "print(mistakenness_quantiles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm1VTvYHPlyT"
      },
      "outputs": [],
      "source": [
        "# Hardness is a measure computed based on model prediction output (through\n",
        "# logits) that summarizes a measure of the uncertainty the model had with the\n",
        "# sample. This makes hardness quantitative and can be used to detect things\n",
        "# like hard samples and annotation errors\n",
        "fob.compute_hardness(train_dataset,\n",
        "                     label_field='lenet_train_classification',\n",
        "                     )\n",
        "\n",
        "print(\"Hardness computed successfully for train_dataset.\")\n",
        "\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlHELglv0NKr"
      },
      "outputs": [],
      "source": [
        "# Compute quantiles of hardness\n",
        "hardness_quantiles = train_dataset.quantiles(\n",
        "    \"hardness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nHardness quantiles for train_dataset:\")\n",
        "print(hardness_quantiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUt9Tx2z6LiF"
      },
      "source": [
        "## Create 2D Visualizations of LeNet Embeddings\n",
        "Reducing High-Dimensional Features to Visual Form\n",
        "Our LeNet embeddings exist in 84-dimensional space, making them impossible to visualize directly. Dimensionality reduction techniques transform these high-dimensional vectors into 2D points that preserve important relationships between samples.\n",
        "\n",
        "Principal Component Analysis (PCA) finds the directions of maximum variance in the data and projects samples onto the two most important axes. This linear transformation preserves global structure and shows which samples vary most from the dataset center.\n",
        "\n",
        "\n",
        "UMAP (Uniform Manifold Approximation and Projection) uses non-linear techniques to preserve local neighborhoods while revealing cluster structure. UMAP often separates distinct groups more clearly than PCA, making it valuable for identifying how the model groups similar digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIIRMjCi6C1S"
      },
      "outputs": [],
      "source": [
        "print(\"Creating 2D visualizations of LeNet embeddings...\")\n",
        "\n",
        "# PCA visualization\n",
        "pca_viz_lenet = fob.compute_visualization(\n",
        "    train_dataset,\n",
        "    method=\"pca\",\n",
        "    embeddings=\"lenet_embeddings\",\n",
        "    num_dims=2,\n",
        "    brain_key=\"pca_lenet_embeddings\"\n",
        ")\n",
        "\n",
        "# UMAP visualization\n",
        "umap_viz_lenet = fob.compute_visualization(\n",
        "    train_dataset,\n",
        "    method=\"umap\",\n",
        "    embeddings=\"lenet_embeddings\",\n",
        "    num_dims=2,\n",
        "    brain_key=\"umap_lenet_embeddings\"\n",
        ")\n",
        "\n",
        "print(\"2D visualizations created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB3nQoQyDGDU"
      },
      "source": [
        "#### PCA 2D Projection of the CNN Embedding Space\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/pca_lenet_embeddings.png?raw=true)\n",
        "\n",
        "\n",
        "#### UMAP 2D Projection of the CNN Embedding Space\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/umap_lenet_embeddings.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67c_ZJ0JHce"
      },
      "source": [
        "Inspect the projected embeddings on the FiftyOne app. Compare how the PCA and the UMAP projections look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mBoPU7nCUpq"
      },
      "outputs": [],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988VFPHSIGII"
      },
      "source": [
        "## Exploring Uniqueness and Representativeness with Dataset Aggregations\n",
        "\n",
        "**Understanding Score Distributions Through Statistical Analysis**\n",
        "\n",
        "We can use FiftyOne's aggregation methods to explore the distribution of uniqueness and representativeness values across our dataset. This analysis helps us identify meaningful thresholds for categorizing our samples based on their statistical properties.\n",
        "\n",
        "**Quantile Analysis for Threshold Selection**\n",
        "\n",
        "Quantiles divide our data into meaningful segments, revealing the distribution shape and enabling principled threshold selection. The 25th, 50th (median), 75th, and 95th percentiles help us understand where most samples fall and identify the truly exceptional cases.\n",
        "\n",
        "For uniqueness scores, the 95th percentile reveals the most unusual samples that fall far from any cluster centers. These often represent edge cases, annotation errors, or rare variants that deserve special attention. The 75th percentile for representativeness identifies samples that exemplify their clusters well, making them ideal candidates for visualization or as training exemplars.\n",
        "\n",
        "```python\n",
        "# Get quantiles for uniqueness scores\n",
        "uniqueness_quantiles = train_dataset.aggregate(fo.Quantiles(\"uniqueness\",\n",
        "                                          [0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "\n",
        "# Get quantiles for representativeness scores  \n",
        "representativeness_quantiles = train_dataset.aggregate(fo.Quantiles(\"representativeness\",\n",
        "                                    [0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "```\n",
        "\n",
        "These quantile boundaries enable us to create filtered views targeting specific sample types, such as the top 5% most unique samples or the most representative examples from each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz8rCYfFu_g6"
      },
      "outputs": [],
      "source": [
        "# Get quantiles for uniqueness based on LeNet embeddings\n",
        "\n",
        "quantiles_of_interest = [0.5, 0.75, 0.95, 0.99, 0.999]\n",
        "uniqueness_quantiles = train_dataset.quantiles(\"uniqueness\", quantiles_of_interest)\n",
        "quantile_labels = [f\"{label*100:.1f}% \" for label in quantiles_of_interest]\n",
        "print('Uniqueness quantiles')\n",
        "{label: value for label, value in zip(quantile_labels, uniqueness_quantiles)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmdFZa82BtDE"
      },
      "outputs": [],
      "source": [
        "quantiles_of_interest = [0.5, 0.75, 0.95, 0.99, 0.999]\n",
        "representativeness_quantiles = train_dataset.quantiles(\"uniqueness\", quantiles_of_interest)\n",
        "print('Representativeness quantiles')\n",
        "{label: value for label, value in zip(quantile_labels, representativeness_quantiles)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug4hUH2nXYDP"
      },
      "source": [
        "## Create Views for Most Unique and Most Representative Samples\n",
        "\n",
        "We can use filtered Views of the dataset to get the most unique values and launch the FiftyOne app from them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld0QsUJyCyiE"
      },
      "outputs": [],
      "source": [
        "# Most unique samples (top 0.1% - these are often edge cases or outliers)\n",
        "most_unique_samples_view = train_dataset.match(\n",
        "                             F(\"uniqueness\") > uniqueness_quantiles[-1]\n",
        "                             ).sort_by(\"uniqueness\", reverse=True)\n",
        "\n",
        "print(f\"Most unique samples: {len(most_unique_samples_view)} samples\")\n",
        "\n",
        "# Most representative samples (top 0.1% - these exemplify their clusters well)\n",
        "most_representative_samples_view = train_dataset.match(\n",
        "    F(\"representativeness\") > representativeness_quantiles[-1]\n",
        "    ).sort_by(\"representativeness\", reverse=True)\n",
        "\n",
        "print(f\"Most representative samples: {len(most_representative_samples_view)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5VZ7AE3vCCG"
      },
      "outputs": [],
      "source": [
        "## Visualize most unique images\n",
        "session.view = most_unique_samples_view\n",
        "session.refresh()\n",
        "print(f\"FiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOiK_Jrh_bZC"
      },
      "outputs": [],
      "source": [
        "# Get the sample with the highest uniqueness score\n",
        "most_unique_sample = most_unique_samples_view.first()\n",
        "\n",
        "print(f\"\"\"Most unique sample ID: {most_unique_sample.id}\n",
        "Uniqueness score: {most_unique_sample.uniqueness:.4f}\n",
        "Ground truth label: {most_unique_sample.ground_truth.label}\"\"\")\n",
        "\n",
        "# Find images most similar to the most unique one using the LeNet similarity index\n",
        "# We want the most similar samples, so reverse=False\n",
        "similar_to_unique_view = train_dataset.sort_by_similarity(\n",
        "    most_unique_sample.id,\n",
        "    brain_key=\"lenet_cosine_similarity_index\", # Use the LeNet index\n",
        "    k=20, # Get the top 20 similar samples (including the unique one itself)\n",
        "    reverse=False\n",
        ")\n",
        "\n",
        "print(f\"Found {len(similar_to_unique_view)} samples most similar to the most unique one.\")\n",
        "\n",
        "# Visualize the samples similar to the most unique one\n",
        "session.view = similar_to_unique_view\n",
        "session.refresh()\n",
        "print(session.url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9j3UwMpOCqm"
      },
      "outputs": [],
      "source": [
        "## Visualize most representative images\n",
        "session.view = most_representative_samples_view\n",
        "session.refresh()\n",
        "print(f\"FiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua4aI4kVHd7w"
      },
      "source": [
        "## Obtain PyTorch Datasets from the Most Unique and Representative Samples\n",
        "\n",
        "We can create new PyTorch datasets from the samples that are most unique or representative and use them to retrain the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2QdUDxGHt2h"
      },
      "outputs": [],
      "source": [
        "most_unique_torch_train_ds = CustomTorchImageDataset(most_unique_samples_view,\n",
        "                                                     label_map=label_map,\n",
        "                                                     image_transforms=image_transforms)\n",
        "most_representative_torch_train_ds = CustomTorchImageDataset(most_representative_samples_view,\n",
        "                                                             label_map=label_map,\n",
        "                                                             image_transforms=image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICAFI-g4rWZr"
      },
      "outputs": [],
      "source": [
        "new_train_set = ConcatDataset([most_unique_torch_train_ds, most_representative_torch_train_ds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_VLAyZEu2C3"
      },
      "source": [
        "**Suggested Exercise**\n",
        "Create a new DataLoader and retrain the model using **only** the most representative and unique samples. On the next sections there is code that can be used as guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waBtOaOoaaUm"
      },
      "source": [
        "## Augmenting the Misclassified Training Samples and Retrain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_uLxSnsm0rm"
      },
      "source": [
        "### Effective Augmentations for MNIST\n",
        "\n",
        "\n",
        "### Geometric Transformations:\n",
        "\n",
        "* Small rotations (10-15 degrees): Handwritten digits naturally vary in orientation\n",
        "* Small translations (2-3 pixels): Accounts for centering variations in digit positioning\n",
        "* Slight scaling (0.9-1.1x): Handles size variations in handwriting\n",
        "* Moderate elastic deformations are  useful for MNIST because they simulate the natural variations in handwriting style. Imagine stretching and compressing parts of a digit as different people might write them.\n",
        "### Why These Work\n",
        "The principle behind effective augmentation is creating realistic variations that preserve the digit's identity while exposing the model to plausible distortions. MNIST digits are centered and normalized, so augmentations should introduce controlled variability without making digits unrecognizable.\n",
        "### Augmentations to Avoid\n",
        "* Heavy distortions like large rotations (>20), extreme scaling, or aggressive elastic deformation can make digits ambiguous, a rotated \"6\" might look like a \"9\", or a heavily stretched \"1\" might resemble a \"7\".\n",
        "\n",
        "* Color-based augmentations (brightness, contrast) have limited benefit since MNIST is grayscale and already normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jujitJcyCvdN"
      },
      "source": [
        "## Retraining and Final Evaluation\n",
        "\n",
        "**Measuring the Impact of Augmented Data**\n",
        "\n",
        "After identifying misclassified training samples and creating augmented versions, we retrain our model to measure performance improvements. This process demonstrates how targeted data augmentation can address specific model weaknesses while maintaining overall performance.\n",
        "\n",
        "The retraining phase uses the original best model weights as a starting point, then fine-tunes on the combined dataset containing both original training data and augmented versions of problematic samples. We use a lower learning rate to preserve learned features while adapting to the new data.\n",
        "\n",
        "Final evaluation compares the retrained model against both the original LeNet performance and CLIP's zero-shot results. This comparison reveals the relative benefits of supervised learning with augmentation versus pre-trained model capabilities.\n",
        "\n",
        "```python\n",
        "# Load best model for retraining\n",
        "retrain_model = ModernLeNet5()\n",
        "retrain_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "# Retrain with combined dataset\n",
        "combined_train_dataset = ConcatDataset([torch_train_set, torch_augmented_dataset])\n",
        "```\n",
        "\n",
        "The evaluation metrics quantify whether targeted augmentation successfully improved model robustness and generalization on the held-out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWhQyNotngBu"
      },
      "outputs": [],
      "source": [
        "## Data Augmentation for Misclassified Training Samples\n",
        "\n",
        "print(f\"Number of misclassified training samples: {len(mislabeled_train_images_view)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZkHGpJjgniw"
      },
      "source": [
        "\n",
        "## Samples to Filter-out During Fine-tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX6UxRK3NoqR"
      },
      "outputs": [],
      "source": [
        "# Identify samples in the top 0.1 percentile of mistakenness (likely annotation errors)mistakenness_99th_percentile = train_dataset.quantiles(\"mistakenness\", [0.99])[0]\n",
        "mistakenness_99th_percentile = train_dataset.quantiles(\"mistakenness\", [0.999])[0]\n",
        "highly_mistaken_view = train_dataset.match(F(\"mistakenness\") > mistakenness_99th_percentile)\n",
        "\n",
        "# Identify samples in the top 1 percentile of hardness (likely genuinely hard samples)\n",
        "hardness_99th_percentile = train_dataset.quantiles(\"hardness\", [0.999])[0]\n",
        "highly_hard_view = train_dataset.match(F(\"hardness\") > hardness_99th_percentile)\n",
        "\n",
        "print(f\"Number of highly mistaken samples (top 1%): {len(highly_mistaken_view)}\")\n",
        "print(f\"Number of highly hard samples (top 1%): {len(highly_hard_view)}\")\n",
        "\n",
        "# Get the IDs of samples that are highly mistaken or highly hard\n",
        "highly_mistaken_ids = set(highly_mistaken_view.values(\"id\"))\n",
        "highly_hard_ids = set(highly_hard_view.values(\"id\"))\n",
        "\n",
        "# Combine the sets of IDs that are hard\n",
        "difficult_samples = highly_mistaken_ids.union(highly_hard_ids)\n",
        "\n",
        "print(f\"Total number of difficult samples (union of highly mistaken and highly hard): {len(difficult_samples)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9gijcmokiKk"
      },
      "outputs": [],
      "source": [
        "# Get the IDs of the misclassified samples\n",
        "misclassified_sample_ids = set(mislabeled_train_images_view.values(\"id\"))\n",
        "\n",
        "print(f\"Total number of misclassified samples in the training set: {len(misclassified_sample_ids)}\")\n",
        "\n",
        "# Find the intersection of difficult samples and misclassified samples\n",
        "intersection_difficult_misclassified_ids = difficult_samples.intersection(misclassified_sample_ids)\n",
        "\n",
        "print(f\"Number of samples that are both difficult and misclassified: {len(intersection_difficult_misclassified_ids)}\")\n",
        "\n",
        "# Create a FiftyOne view containing only these intersection samples\n",
        "intersection_difficult_misclassified_view = train_dataset.select(\n",
        "    list(intersection_difficult_misclassified_ids)\n",
        "    ).sort_by('hardness',\n",
        "              reverse=True)\n",
        "\n",
        "\n",
        "print(f\"Created a FiftyOne view for the intersection samples with {len(intersection_difficult_misclassified_view)} samples.\")\n",
        "\n",
        "\n",
        "# Inspect the samples in ids_to_remove, we can decide whether\n",
        "# whether we augment them or exclude them from the fine-tuning data\n",
        "session.view = intersection_difficult_misclassified_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFRSg5ufWfwe"
      },
      "source": [
        "### Finding the Needles in the Haystack\n",
        "\n",
        "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/mistaken_images.png)\n",
        "\n",
        "Using our metrics we have reduced significantly the search space and the manual inspection for our problematic samples. Isn't it interesting to find these samples in one of the most canonical datasets in machine learning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4jWhhtJTCEu"
      },
      "outputs": [],
      "source": [
        "filenames_of_samples_to_exclude = set([\n",
        "                                      \"/root/fiftyone/mnist/train/data/002902.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/003693.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/035247.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/018383.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/033507.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/039424.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/025547.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/047218.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/001921.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/004477.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/050606.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/030050.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/024614.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/014583.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/034521.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/050995.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/004477.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/033319.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/034405.jpg\"\n",
        "                                      \"/root/fiftyone/mnist/train/data/007260.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/001401.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/002149.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/000903.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/001921.jpg\"\n",
        "                                      ])\n",
        "\n",
        "# Use match() to filter by filepath\n",
        "samples_to_exclude_view = train_dataset.match(\n",
        "    fo.ViewField(\"filepath\").is_in(filenames_of_samples_to_exclude)\n",
        ")\n",
        "print('Samples to exclude from new training runs')\n",
        "session.view = samples_to_exclude_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvC4F-c9u4vJ"
      },
      "outputs": [],
      "source": [
        "# get the ids from samples_to_exclude_view\n",
        "ids_of_samples_to_exclude = samples_to_exclude_view.values(\"id\")\n",
        "ids_of_samples_to_exclude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHYCoyZIXn8h"
      },
      "source": [
        "## Create a New Training Dataset without the Excluded Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m04hU2qDXWtl"
      },
      "outputs": [],
      "source": [
        "new_train_dataset = train_dataset.exclude(ids_of_samples_to_exclude).clone()\n",
        "\n",
        "print(f\"Original train_dataset has {len(train_dataset)} samples.\")\n",
        "print(f\"Excluding {len(new_train_set)} samples.\")\n",
        "print(f\"New train_dataset view has {len(new_train_set)} samples.\")\n",
        "\n",
        "new_train_dataset.name = \"cleaned-up-mnist-training-set\"\n",
        "new_train_dataset.persistent = True\n",
        "print(f\"New persistent dataset created with {len(new_train_dataset)} samples.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUt5zP1eozkF"
      },
      "outputs": [],
      "source": [
        "# To work with this view in PyTorch, create a CustomTorchImageDataset from it\n",
        "new_torch_train_set = CustomTorchImageDataset(\n",
        "    new_train_dataset,  # Changed from new_train_set to new_train_dataset\n",
        "    label_map=label_map, # Use the same label map\n",
        "    image_transforms=image_transforms # Use the same transforms\n",
        ")\n",
        "\n",
        "print(f\"New PyTorch dataset created from the filtered view with {len(new_torch_train_set)} samples.\")\n",
        "\n",
        "# You can now use new_torch_train_set to create a new DataLoader for retraining\n",
        "# new_train_loader = create_deterministic_training_dataloader(\n",
        "#     new_torch_train_set,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=True,\n",
        "#     num_workers=num_workers,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "# print(f\"New train DataLoader created with {len(new_train_loader)} batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyPUYKG4VKBN"
      },
      "source": [
        "**Another possibility:** using these samples to train a model with an 'IDK' (I don't know class).\n",
        "\n",
        "Also consider that the validation and test set might have dirty samples too. Ideally we should repeat this process with them too.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK3EL0cHf6tA"
      },
      "source": [
        "## Define Augmentations\n",
        "\n",
        "**Configuring Transformations for MNIST Digit Recognition**\n",
        "\n",
        "Data augmentation applies controlled transformations to training images, creating variations that help models generalize to new handwriting styles. For MNIST, effective augmentations simulate natural variations in digit writing without changing the digit's identity.\n",
        "\n",
        "**Seeding for Reproducibility**: Set random seeds before defining augmentations to ensure consistent results across training runs. This enables reproducible experiments and fair model comparisons.\n",
        "\n",
        "**MNIST-Specific Transformations**: Small rotations, translations, and elastic deformations work well for handwritten digits. These transformations mimic natural handwriting variations while preserving digit readability. Avoid extreme distortions that could make a \"6\" look like a \"9\" or render digits unrecognizable.\n",
        "\n",
        "**Exploration**:\n",
        "To inspect the effect of the defined augmentations on the images, you can try [using the FiftyOne plugin for this](https://github.com/jacobmarks/fiftyone-albumentations-plugin).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8xShb1LCkxi"
      },
      "outputs": [],
      "source": [
        "# Install the augmentations plugin\n",
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw0Edh8ZEG_l"
      },
      "outputs": [],
      "source": [
        "mnist_augmentations_1 = A.Compose([\n",
        "\n",
        "    # Use Affine transform for shifting, scaling, and rotating\n",
        "    A.Affine(\n",
        "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},  # 10% translation\n",
        "        scale=(0.9, 1.1),     # 10% scaling\n",
        "        rotate=(-5, 5),     # 5 rotation\n",
        "        p=0.8\n",
        "    ),\n",
        "\n",
        "    # Elastic deformations to simulate handwriting style variations\n",
        "    A.ElasticTransform(\n",
        "        alpha=20,             # Strength of distortion\n",
        "        sigma=5,              # Smoothness of distortion\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        p=0.6\n",
        "    ),\n",
        "\n",
        "    # Mild perspective transformations\n",
        "    A.Perspective(scale=(0.01, 0.03), p=0.2),\n",
        "\n",
        "    # Mild grid distortion\n",
        "    A.GridDistortion(num_steps=2, distort_limit=0.05, p=0.2),\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVcUoxD3Cu15"
      },
      "outputs": [],
      "source": [
        "# Define second set of augmentations\n",
        "mnist_augmentations_2 = A.Compose([\n",
        "\n",
        "    # Equalize the image histogram (can help with contrast variations)\n",
        "    A.Equalize(p=0.3),\n",
        "\n",
        "    # Affine with shear (simulates slanted handwriting)\n",
        "    A.Affine(\n",
        "        shear=(-5, 5),  # Add shear\n",
        "        translate_percent={\"x\": (-0.03, 0.03), \"y\": (-0.03, 0.03)}, # Smaller translation\n",
        "        scale=(0.98, 1.02), # Smaller scaling\n",
        "        rotate=(-2, 2), # Smaller rotation\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        p=0.9\n",
        "    ),\n",
        "\n",
        "    # Mild perspective transformations\n",
        "    A.Perspective(scale=(0.01, 0.03), p=0.2),\n",
        "\n",
        "    # Mild grid distortion\n",
        "    A.GridDistortion(num_steps=2, distort_limit=0.05, p=0.2),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IptT5b8iC0Rs"
      },
      "outputs": [],
      "source": [
        "# Define a third set of augmentations for MNIST\n",
        "mnist_augmentations_3 = A.Compose([\n",
        "    # Use Affine transform for shifting, scaling, and rotating\n",
        "    A.Affine(\n",
        "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},  # 10% translation\n",
        "        scale=(0.95, 1.15),     # 15% scaling\n",
        "        rotate=(-7, 7),         # 7 rotation\n",
        "        p=0.8\n",
        "    ),\n",
        "\n",
        "    # Elastic deformation with different parameters\n",
        "    A.ElasticTransform(\n",
        "        alpha=15,\n",
        "        sigma=4,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        p=0.4\n",
        "    ),\n",
        "\n",
        "    # Coarse Dropout (simulates occlusions or pen lifts)\n",
        "    A.CoarseDropout(\n",
        "        p=0.3\n",
        "    ),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11PddjugRS9"
      },
      "source": [
        "## Create the torch Dataset for the Augmented Set\n",
        "\n",
        "**Building a Custom PyTorch Dataset for Augmented Training Data**\n",
        "\n",
        "Creating a PyTorch Dataset for augmented data requires handling multiple variations of problematic samples while maintaining compatibility with PyTorch's DataLoader system. Our `AugmentedMNISTDataset` class applies augmentations during training, ensuring each epoch sees different variations of misclassified samples.\n",
        "\n",
        "**Key Design Principles:**\n",
        "\n",
        "**Dynamic Augmentation**: Apply transformations during data loading rather than pre-generating samples. This saves disk space while providing variation potential.\n",
        "\n",
        "**Deterministic Access**: Use mathematical mapping for consistent sample ordering while allowing multiple augmented versions of each base sample. The formula `base_idx = idx // (augment_factor + 1)` determines which original sample to use.\n",
        "\n",
        "**Integration**: Inherit from PyTorch's `Dataset` interface for compatibility with DataLoader features like batching, shuffling, and multi-process loading.\n",
        "\n",
        "**Memory and Performance Considerations:**\n",
        "\n",
        "**Memory Usage**: Store file paths rather than loaded images to maintain small memory footprint regardless of augmentation factor.\n",
        "\n",
        "**Caching Strategy**: Consider implementing an LRU cache to store recent base images, reducing disk I/O during training.\n",
        "\n",
        "**Augmentation Randomness**: Each call to `__getitem__` may produce different results due to random augmentation parameters.\n",
        "\n",
        "**Error Resilience**: Robust error handling ensures training continues even if individual samples fail to load or augment.\n",
        "\n",
        "This design provides a foundation for targeted data augmentation while maintaining flexibility to experiment with different augmentation strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTZEWNCHDejq"
      },
      "outputs": [],
      "source": [
        "class AugmentedMNISTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch dataset that applies augmentations to misclassified MNIST samples.\n",
        "    Each sample can be augmented multiple times to create more training data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fiftyone_view,\n",
        "                 label_map,\n",
        "                 base_transforms,\n",
        "                 augmentations=None,\n",
        "                 augment_factor=5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fiftyone_view: FiftyOne view of misclassified samples\n",
        "            label_map: Mapping from string labels to indices\n",
        "            base_transforms: Base PyTorch transforms (normalization, etc.)\n",
        "            augmentations: Albumentations transform pipeline\n",
        "            augment_factor: How many augmented versions to create per sample\n",
        "        \"\"\"\n",
        "        self.image_paths = fiftyone_view.values(\"filepath\")\n",
        "        self.str_labels = fiftyone_view.values(\"ground_truth.label\")\n",
        "        self.label_map = label_map\n",
        "        self.base_transforms = base_transforms\n",
        "        self.augmentations = augmentations\n",
        "        self.augment_factor = augment_factor\n",
        "\n",
        "        print(f\"AugmentedMNISTDataset: {len(self.image_paths)} base samples\")\n",
        "        print(f\"With augmentation factor {augment_factor}: {len(self)} total samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths) * (self.augment_factor + 1)  # +1 for original\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Determine which base sample and whether to augment\n",
        "        base_idx = idx // (self.augment_factor + 1)\n",
        "        aug_idx = idx % (self.augment_factor + 1)\n",
        "\n",
        "        # Load image\n",
        "        image_path = self.image_paths[base_idx]\n",
        "        image = Image.open(image_path).convert('L')\n",
        "\n",
        "        # Convert to numpy for albumentations\n",
        "        image_np = np.array(image, dtype=np.uint8)\n",
        "\n",
        "        # Apply augmentation if not the first version (original)\n",
        "        if aug_idx > 0 and self.augmentations is not None:\n",
        "            augmented = self.augmentations(image=image_np)\n",
        "            image_np = augmented['image']\n",
        "\n",
        "        # Convert back to PIL for PyTorch transforms\n",
        "        image = Image.fromarray(image_np).convert(\"L\")\n",
        "\n",
        "        # Apply base transforms (normalization, tensor conversion)\n",
        "        if self.base_transforms:\n",
        "            image = self.base_transforms(image)\n",
        "\n",
        "        # Get label\n",
        "        label_str = self.str_labels[base_idx]\n",
        "        label_idx = self.label_map.get(label_str, -1)\n",
        "\n",
        "        return image, torch.tensor(label_idx, dtype=torch.long)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBQXUeAwDUvt"
      },
      "outputs": [],
      "source": [
        "# Calculate the 1th percentile (lowest percentile) of confidence scores for lenet_train_classification\n",
        "confidence_quantiles = train_dataset.quantiles(\"lenet_train_classification.confidence\", [0.01])\n",
        "q10_confidence = confidence_quantiles[0]\n",
        "\n",
        "print(f\"The 1th percentile of confidence is: {q10_confidence:.4f}\")\n",
        "\n",
        "# Create a view of samples that were correctly classified AND have confidence <= 1th percentile\n",
        "low_confidence_correct_view = train_dataset.match(\n",
        "    (F(\"lenet_train_classification.label\") == F(\"ground_truth.label\")) &\n",
        "    (F(\"lenet_train_classification.confidence\") <= q10_confidence)\n",
        ").sort_by(\"lenet_train_classification.confidence\")\n",
        "\n",
        "print(f\"\\nNumber of correctly classified samples with confidence <= {q10_confidence:.4f}: {len(low_confidence_correct_view)}\")\n",
        "\n",
        "# We can launch the FiftyOne App to visualize this view\n",
        "session.view = low_confidence_correct_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMh0_z9nNWxR"
      },
      "source": [
        "## Define Samples to Augment\n",
        "\n",
        "This section focuses on identifying specific subsets of samples from the training data that are most likely to benefit from augmentation. We will select samples that were previously misclassified, are highly unique or representative in the embedding space, or were correctly classified but with low confidence. Augmenting these samples aims to improve the model's performance on challenging or underrepresented examples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGM7NUVvtIVc"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"digit-samples-to-augment\"\n",
        "\n",
        "if dataset_name in fo.list_datasets():\n",
        "    print(f\"Dataset '{dataset_name}' already exists. Deleting it.\")\n",
        "    fo.delete_dataset(dataset_name)\n",
        "\n",
        "# Always create a new dataset after deleting or if it didn't exist\n",
        "dataset_to_augment = fo.Dataset(dataset_name)\n",
        "\n",
        "for sample in mislabeled_train_images_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "for sample in most_unique_samples_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "for sample in most_representative_samples_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "for sample in low_confidence_correct_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "dataset_to_augment.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yALZCSEAcbYm"
      },
      "outputs": [],
      "source": [
        "# Filter out excluded samples\n",
        "dataset_to_augment = dataset_to_augment.exclude(ids_of_samples_to_exclude).clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGH3ZAyrDM1A"
      },
      "outputs": [],
      "source": [
        "torch_augmented_dataset = AugmentedMNISTDataset(\n",
        "    dataset_to_augment,\n",
        "    label_map=label_map,\n",
        "    base_transforms=image_transforms,\n",
        "    augmentations=mnist_augmentations_1, # We can also try mnist_augmentations_2 or mnist_augmentations_3\n",
        "    augment_factor=9  # Create this number of augmented versions per input sample\n",
        ")\n",
        "\n",
        "print(f\"Original misclassified samples: {len(mislabeled_train_images_view)}\")\n",
        "print(f'')\n",
        "print(f\"Total augmented dataset size: {len(torch_augmented_dataset)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbY0VlychC9v"
      },
      "outputs": [],
      "source": [
        "# Combine the original dataset with its augmentation\n",
        "combined_dataset = ConcatDataset([new_torch_train_set, torch_augmented_dataset])\n",
        "print(f\"Combined dataset size: {len(combined_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpAymXsyhqC8"
      },
      "outputs": [],
      "source": [
        "# Create new DataLoader for combined dataset\n",
        "combined_train_loader = create_deterministic_training_dataloader(\n",
        "    combined_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Combined DataLoader has {len(combined_train_loader)} batches.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBokBRd7L9j_"
      },
      "source": [
        "## Visualize Sample Augmentations in FiftyOne\n",
        "\n",
        "**Inspecting Augmentation Quality Before Training**\n",
        "\n",
        "Visualizing augmented samples ensures transformations improve model training rather than corrupt the data. FiftyOne provides an interface to compare original images with their augmented variants, helping validate that augmentations preserve digit identity while adding useful variation.\n",
        "\n",
        "**Quality Control**: Review augmented samples to confirm that rotations, translations, and elastic deformations remain within acceptable bounds. A properly augmented \"3\" should still look like a \"3\" despite the applied transformations.\n",
        "\n",
        "**Parameter Tuning**: Visual inspection helps adjust augmentation parameters. If elastic deformations make digits unrecognizable or rotations create ambiguous orientations, reduce the transformation intensity before training.\n",
        "\n",
        "**Dataset Creation**: The visualization process creates temporary augmented samples that can be loaded into FiftyOne for side-by-side comparison with originals, enabling systematic evaluation of augmentation effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "508hMKjcFKfY"
      },
      "outputs": [],
      "source": [
        "# Create a small dataset with original and augmented versions for visualization\n",
        "def create_augmentation_samples(view, augmentations, num_samples=50):\n",
        "    \"\"\"Create FiftyOne samples showing original and augmented versions\"\"\"\n",
        "\n",
        "    dataset_name = \"augmented_images_training_set\"\n",
        "\n",
        "    if dataset_name in fo.list_datasets():\n",
        "        aug_viz_dataset = fo.load_dataset(dataset_name)\n",
        "    else:\n",
        "        aug_viz_dataset = fo.Dataset(dataset_name)\n",
        "\n",
        "    sample_paths = view.values(\"filepath\")[:num_samples]\n",
        "    sample_labels = view.values(\"ground_truth.label\")[:num_samples]\n",
        "\n",
        "    for i, (path, label) in enumerate(zip(sample_paths, sample_labels)):\n",
        "        # Load original image\n",
        "        original_image = Image.open(path).convert('L')\n",
        "        original_np = np.array(original_image)\n",
        "\n",
        "        # Create sample for original image\n",
        "        original_sample = fo.Sample(filepath=path)\n",
        "        original_sample.tags = [\"original\"]\n",
        "        original_sample[\"ground_truth\"] = fo.Classification(label=label)\n",
        "        original_sample[\"augmentation_type\"] = \"original\"\n",
        "        aug_viz_dataset.add_sample(original_sample)\n",
        "\n",
        "        # Create 3 augmented versions\n",
        "        for aug_idx in range(3):\n",
        "            # Apply augmentation\n",
        "            augmented = augmentations(image=original_np)['image']\n",
        "\n",
        "            # Save augmented image temporarily\n",
        "            aug_image = Image.fromarray(augmented).convert(\"L\")\n",
        "            temp_path = f\"/tmp/aug_{i}_{aug_idx}.png\"\n",
        "            aug_image.save(temp_path)\n",
        "\n",
        "            # Create FiftyOne sample for augmented image\n",
        "            aug_sample = fo.Sample(filepath=temp_path)\n",
        "            aug_sample.tags = [\"augmented\"]\n",
        "            aug_sample[\"ground_truth\"] = fo.Classification(label=label)\n",
        "            aug_sample[\"augmentation_type\"] = f\"augmented_{aug_idx + 1}\"\n",
        "            aug_sample[\"original_sample_id\"] = str(i)\n",
        "\n",
        "            aug_viz_dataset.add_sample(aug_sample)\n",
        "\n",
        "    return aug_viz_dataset\n",
        "\n",
        "print(\"Creating augmentation visualization dataset...\")\n",
        "\n",
        "# Create the visualization dataset\n",
        "aug_viz_dataset = create_augmentation_samples(\n",
        "    dataset_to_augment,\n",
        "    mnist_augmentations_1,\n",
        "    num_samples=50\n",
        ")\n",
        "\n",
        "print(f\"Created visualization dataset with {len(aug_viz_dataset)} samples\")\n",
        "print(f\"Original samples: {len(aug_viz_dataset.match_tags('original'))}\")\n",
        "print(f\"Augmented samples: {len(aug_viz_dataset.match_tags('augmented'))}\")\n",
        "\n",
        "# Launch FiftyOne App to visualize the augmentations\n",
        "session.view = aug_viz_dataset.view()\n",
        "\n",
        "print(f\"\\nAugmentation Visualization URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXV9LMYUXuVl"
      },
      "outputs": [],
      "source": [
        "# Create views for easy comparison\n",
        "original_view = aug_viz_dataset.match_tags(\"original\")\n",
        "augmented_view = aug_viz_dataset.match_tags(\"augmented\")\n",
        "\n",
        "print(f\"\\nTo compare:\")\n",
        "print(f\"- View original samples: session.view = original_view\")\n",
        "print(f\"- View augmented samples: session.view = augmented_view\")\n",
        "print(f\"- Group by original sample: Use 'original_sample_id' field to group related samples\")\n",
        "\n",
        "# Add some helpful aggregations\n",
        "print(f\"\\nSample distribution:\")\n",
        "print(f\"By augmentation type: {aug_viz_dataset.count_values('augmentation_type')}\")\n",
        "print(f\"By ground truth label: {aug_viz_dataset.count_values('ground_truth.label')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjfiVoK0HTSp"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/augmentation_mnist_vis.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0sIham2JVEw"
      },
      "source": [
        "## Fine-Tuning with Augmented Data\n",
        "\n",
        "The retraining process combines our original training data with the augmented versions of misclassified samples, allowing the model to learn from its previous mistakes. We use a lower learning rate to preserve the knowledge already learned while adapting to the new augmented examples.\n",
        "\n",
        "During retraining, we monitor both training and validation loss to ensure the model improves without overfitting. The validation set continues to serve as our guide for saving the best model weights, ensuring we capture improvements in generalization rather than just memorization of the augmented data.\n",
        "\n",
        "```python\n",
        "for epoch in range(retrain_epochs):\n",
        "    train_loss = train_epoch(retrain_model, combined_train_loader)\n",
        "    val_loss = val_epoch(retrain_model, val_loader)\n",
        "    \n",
        "    if val_loss < best_retrain_val_loss:\n",
        "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
        "```\n",
        "\n",
        "This targeted approach allows us to address specific model weaknesses while maintaining overall performance on the broader dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2MiRH66n-7r"
      },
      "outputs": [],
      "source": [
        "## Load Best Model for Retraining\n",
        "\n",
        "# Ensure reproducibility for the retraining process\n",
        "set_seeds(51) # Use the same seed as initial training for consistency\n",
        "\n",
        "# Load the previously saved best model\n",
        "best_model_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
        "retrain_model = ModernLeNet5()\n",
        "retrain_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "retrain_model = retrain_model.to(device)\n",
        "\n",
        "print(f\"Loaded best model from {best_model_path} for retraining\")\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "retrain_optimizer = Adam(retrain_model.parameters(), lr=0.0001,\n",
        "                       weight_decay=1e-3)\n",
        "# 10x stronger regularization\n",
        "# 30x smaller learning rate\n",
        "\n",
        "print(\"\\nStarting retraining with augmented data...\")\n",
        "\n",
        "retrain_epochs = 15\n",
        "retrain_losses = []\n",
        "retrain_val_losses = []\n",
        "\n",
        "# Track the best validation loss during retraining\n",
        "best_retrain_val_loss = float('inf')\n",
        "retrain_model_save_path = Path(os.getcwd()) / 'retrained_lenet.pth'\n",
        "\n",
        "for epoch in range(retrain_epochs):\n",
        "    print(f\"\\n--- Retrain Epoch {epoch+1}/{retrain_epochs} ---\")\n",
        "\n",
        "    # Training phase\n",
        "    retrain_model.train()\n",
        "    batch_losses = []\n",
        "\n",
        "    for images, labels in tqdm(combined_train_loader, desc=\"Retraining\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = retrain_model(images)\n",
        "        loss_value = ce_loss(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        retrain_optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        retrain_optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss_value.item())\n",
        "\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    retrain_losses.append(train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    retrain_model.eval()\n",
        "    val_batch_losses = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = retrain_model(images)\n",
        "            loss_value = ce_loss(logits, labels)\n",
        "            val_batch_losses.append(loss_value.item())\n",
        "\n",
        "    val_loss = np.mean(val_batch_losses)\n",
        "    retrain_val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save best model during retraining\n",
        "    if val_loss < best_retrain_val_loss:\n",
        "        best_retrain_val_loss = val_loss\n",
        "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
        "        print(\" Saved improved retrained model\")\n",
        "\n",
        "print(f\"\\nRetraining complete! Best model saved to {retrain_model_save_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh07-Kxclihj"
      },
      "source": [
        "## Plot Re-Training Progress\n",
        "\n",
        "**Visualizing Training Dynamics Across Phases**\n",
        "\n",
        "Training visualization reveals model learning patterns during both initial training and retraining with augmented data. Loss curves show convergence behavior, overfitting signals, and the impact of targeted data augmentation.\n",
        "\n",
        "**Two-Panel Analysis**\n",
        "\n",
        "The left panel shows retraining progress in isolation, tracking how the model adapts to augmented misclassified training samples.\n",
        "\n",
        "The right panel provides historical context by plotting the complete training timeline. The vertical line marks where retraining begins, allowing comparison between original learning dynamics and fine-tuning behavior.\n",
        "\n",
        "**Key Patterns to Observe**\n",
        "\n",
        "- **Convergence Speed**: Retraining typically converges faster than initial training due to pre-learned features\n",
        "- **Loss Magnitude**: Validation loss during retraining should remain close to or below original best values  \n",
        "- **Stability**: Smooth loss curves indicate stable learning, while oscillations suggest learning rate issues\n",
        "- **Gap Analysis**: Small train-validation gaps indicate good generalization\n",
        "\n",
        "**Interpreting Results**\n",
        "\n",
        "Successful retraining shows decreasing validation loss without diverging from training loss. If validation loss increases while training loss decreases, we have trained the model enough. We will be using the state of the model that achieved the lowest validation loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx6vDV6dlcNp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot retraining losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(retrain_losses, label='Retrain - Training Loss', marker='o')\n",
        "plt.plot(retrain_val_losses, label='Retrain - Validation Loss', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Retraining Progress ({retrain_epochs} Epochs)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot comparison with original training (if we have the data)\n",
        "plt.subplot(1, 2, 2)\n",
        "if 'train_losses' in locals() and 'val_losses' in locals():\n",
        "    epochs_orig = range(1, len(train_losses) + 1)\n",
        "    epochs_retrain = range(len(train_losses) + 1, len(train_losses) + 1 + len(retrain_losses))\n",
        "\n",
        "    plt.plot(epochs_orig, train_losses, label='Original Train', alpha=0.7)\n",
        "    plt.plot(epochs_orig, val_losses, label='Original Val', alpha=0.7)\n",
        "    plt.plot(epochs_retrain, retrain_losses, label='Retrain Train', marker='o', linewidth=2)\n",
        "    plt.plot(epochs_retrain, retrain_val_losses, label='Retrain Val', marker='s', linewidth=2)\n",
        "\n",
        "    plt.axvline(x=len(train_losses), color='red', linestyle='--', alpha=0.5, label='Retrain Start')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Complete Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJqpzGKCl9-_"
      },
      "source": [
        "## Applying the Fine-tuned Model to the Test Set\n",
        "\n",
        "Following is a brief description of what we are doing in this section.\n",
        "\n",
        "**Evaluating Augmented Model Performance**\n",
        "\n",
        "* Load the best retrained model weights and apply to the test dataset.\n",
        "\n",
        "* Generate predictions and logits for performance comparison against the original model.\n",
        "\n",
        "**Model Loading Process**\n",
        "\n",
        "* Instantiate a fresh model architecture and load the saved state dictionary from retraining.\n",
        "\n",
        "* Move to the GPU (if available) and set evaluation mode for inference without gradient computation.\n",
        "\n",
        "**Inference Pipeline**\n",
        "\n",
        "* Process test images through the fine-tuned model using the same preprocessing pipeline as training.\n",
        "\n",
        "* Extract both predicted class indices and raw logits for comprehensive evaluation.\n",
        "\n",
        "**Result Storage**\n",
        "\n",
        "Collect predictions and logits in arrays for analysis. This enables comparison between original model performance and improvements from targeted data augmentation on challenging samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGX6iRvzl7-e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nApplying retrained model to test set...\")\n",
        "\n",
        "# Load the best retrained model\n",
        "final_model = ModernLeNet5()\n",
        "final_model.load_state_dict(torch.load(retrain_model_save_path, map_location=device))\n",
        "final_model = final_model.to(device)\n",
        "final_model.eval()\n",
        "\n",
        "# Apply retrained model to test set\n",
        "retrained_predictions = []\n",
        "retrained_logits = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for images, _ in tqdm(test_loader, desc=\"Evaluating retrained model\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        logits = final_model(images)\n",
        "        retrained_logits.append(logits.cpu().numpy())\n",
        "\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        retrained_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Concatenate all results\n",
        "retrained_logits = np.concatenate(retrained_logits, axis=0)\n",
        "\n",
        "print(f\"Retrained model evaluation complete.\")\n",
        "print(f\"Predictions shape: {len(retrained_predictions)}\")\n",
        "print(f\"Logits shape: {retrained_logits.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRvv1QKXlmqW"
      },
      "source": [
        "## Store Retrained Model Predictions in FiftyOne\n",
        "\n",
        "Here's what we are doing next:\n",
        "\n",
        "**Integrating Fine-tuned Model Results**\n",
        "\n",
        "* Store retrained model predictions as [FiftyOne Classification](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) objects in the test dataset.\n",
        "\n",
        "* Each sample receives prediction label, confidence score, and raw logits for comprehensive analysis.\n",
        "\n",
        "**Classification Object Creation**\n",
        "\n",
        "Convert raw model outputs into structured FiftyOne format. Apply softmax to logits for confidence scores and map predicted indices to class labels using the established label mapping.\n",
        "\n",
        "**Dataset Integration**\n",
        "\n",
        "Add retrained predictions as a new field alongside existing CLIP and the original LeNet predictions. This enables direct comparison between all model variants within the same dataset framework.\n",
        "\n",
        "**Analysis Preparation**\n",
        "\n",
        "Structured storage enables filtering, querying, and evaluation using FiftyOne's built-in tools.\n",
        "\n",
        "* [FiftyOne's Views Cheat Sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html)\n",
        "* [FiftyOne's Filtering Cheat Sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
        "\n",
        "With this we compare model performance and identify samples where fine-tuning improved or degraded predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVxXrY0ulZQS"
      },
      "outputs": [],
      "source": [
        "print(\"Storing fine-tuned model predictions in FiftyOne...\")\n",
        "\n",
        "for i, sample in enumerate(tqdm(test_dataset, desc=\"Storing retrained predictions\")):\n",
        "    predicted_idx = retrained_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "    sample_logits = retrained_logits[i]\n",
        "\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()\n",
        "    )\n",
        "\n",
        "    sample[\"retrained_lenet_classification\"] = classification\n",
        "    sample.save()\n",
        "\n",
        "print(\"Retrained model predictions stored successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VNYKjHNlo2G"
      },
      "source": [
        "## Evaluate Retrained Model Performance\n",
        "\n",
        "**Comprehensive Assessment of Fine-tuned Model**\n",
        "\n",
        "Evaluate the retrained model using FiftyOne's classification evaluation framework. Generate detailed performance metrics including accuracy, precision, recall, and F1-scores for direct comparison with the original model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhPYCJOirl_J"
      },
      "source": [
        "**Performance Comparison Analysis**\n",
        "\n",
        "Compare original LeNet performance against the retrained model across key metrics. Calculate improvement deltas to quantify the impact of targeted data augmentation on model robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dk_WJoFlWZu"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluating retrained model performance...\")\n",
        "\n",
        "# Evaluate retrained model\n",
        "retrained_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"retrained_lenet_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"retrained_lenet_eval\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RETRAINED MODEL EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "retrained_evaluation_results.print_report(digits=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB4JTMyXrvOU"
      },
      "source": [
        "## Compare Original vs Retrained Performance\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ae2d684"
      },
      "source": [
        "Now that we have applied both the original and retrained LeNet models to the test set, we can compare their performance side-by-side. This comparison will highlight the impact of our targeted data augmentation strategy on the model's ability to generalize to unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isaBZU-oreCa"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get metrics for both models\n",
        "original_metrics = lenet_evaluation_results.metrics()\n",
        "retrained_metrics = retrained_evaluation_results.metrics()\n",
        "\n",
        "print(f\"{'Metric':<20} {'Original':<12} {'Retrained':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 56)\n",
        "\n",
        "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "for metric in metrics_to_compare:\n",
        "    if metric in original_metrics and metric in retrained_metrics:\n",
        "        orig_val = original_metrics[metric]\n",
        "        retrain_val = retrained_metrics[metric]\n",
        "        improvement = retrain_val - orig_val\n",
        "\n",
        "        print(f\"{metric:<20} {orig_val:<12.4f} {retrain_val:<12.4f} {improvement:+.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMl8hmbnls9i"
      },
      "source": [
        "## Analysis of Misclassified Samples After Retraining\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMbC8GmWr_Xl"
      },
      "source": [
        "**Misclassification Analysis**\n",
        "\n",
        "* Identify samples fixed by retraining versus those newly misclassified.\n",
        "\n",
        "* Compute net improvement in correct predictions to assess overall effectiveness of the augmentation strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8O6ygP1lSlK"
      },
      "outputs": [],
      "source": [
        "# Find samples that were misclassified before but correct now\n",
        "originally_wrong = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "now_correct = originally_wrong.match(\n",
        "    F(\"retrained_lenet_classification.label\") == F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "print(f\"\\nSamples fixed by retraining: {len(now_correct)}\")\n",
        "\n",
        "# Find samples that were correct before but wrong now\n",
        "originally_correct = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") == F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "now_wrong = originally_correct.match(\n",
        "    F(\"retrained_lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "print(f\"Samples broken by retraining: {len(now_wrong)}\")\n",
        "\n",
        "# Net improvement\n",
        "net_improvement = len(now_correct) - len(now_wrong)\n",
        "print(f\"Net improvement in correct predictions: {net_improvement}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MySUV8G8sO8U"
      },
      "source": [
        "## Launch FiftyOne App for Insights on the  Results\n",
        "\n",
        "\n",
        "* Launch FiftyOne App with comparative views showing all model predictions. Enable exploration of specific cases where fine-tuning improved or degraded performance for model debugging.\n",
        "\n",
        "* Create filtered view containing ground truth labels and predictions from both original and retrained models. This enables direct comparison between model variants within the same interface.\n",
        "\n",
        "* Access interactive confusion matrices, per-class metrics, and sample-level analysis through FiftyOne's evaluation framework. Filter by prediction differences to identify which samples benefited from augmentation.\n",
        "\n",
        "* Use FiftyOne's query language to create views of samples fixed by retraining, samples broken by retraining, or samples where both models agree or disagree with ground truth labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3wrq9VJ-fLA"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/samples_fixed_by_fine_tuning.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlKoxG9ZsNwM"
      },
      "outputs": [],
      "source": [
        "print(\"\\nLaunching FiftyOne App to explore results...\")\n",
        "\n",
        "# Create a view showing the comparison\n",
        "comparison_view = test_dataset.select_fields([\n",
        "    \"ground_truth\",\n",
        "    \"lenet_classification\",\n",
        "    \"retrained_lenet_classification\"\n",
        "])\n",
        "\n",
        "session.view = comparison_view\n",
        "session.refresh()\n",
        "print(f\"FiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaTxNzER9c58"
      },
      "outputs": [],
      "source": [
        "# View the samples that are fixed by fine-tuning\n",
        "session.view = now_correct\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZfdlKLJ1PQb"
      },
      "source": [
        "## Save the curated MNIST sets to your hard drive\n",
        "\n",
        "Time to save our work! Local copies protect against data loss and enable easy dataset distribution.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-xzB0gd1PQb"
      },
      "outputs": [],
      "source": [
        "# We list the datasets that we have available on this session.\n",
        "fo.list_datasets()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSkapOAV1PQb"
      },
      "outputs": [],
      "source": [
        "# Load the three separate datasets\n",
        "train_dataset = fo.load_dataset(\"mnist-training-set\")\n",
        "val_dataset = fo.load_dataset(\"mnist-validation-set\")\n",
        "test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
        "\n",
        "# Create a new merged dataset\n",
        "merged_dataset = fo.Dataset(\"curated-mnist\")\n",
        "\n",
        "# Add training samples with split tag\n",
        "for sample in train_dataset:\n",
        "    sample.tags.append(\"train\")\n",
        "    merged_dataset.add_sample(sample)\n",
        "\n",
        "# Add validation samples with split tag\n",
        "for sample in val_dataset:\n",
        "    sample.tags.append(\"validation\")\n",
        "    merged_dataset.add_sample(sample)\n",
        "\n",
        "# Add test samples with split tag\n",
        "for sample in test_dataset:\n",
        "    sample.tags.append(\"test\")\n",
        "    merged_dataset.add_sample(sample)\n",
        "\n",
        "# Save the merged dataset\n",
        "merged_dataset.persistent = True\n",
        "\n",
        "print(f\"Merged dataset created with {len(merged_dataset)} total samples\")\n",
        "print(f\"Train samples: {len(merged_dataset.match_tags('train'))}\")\n",
        "print(f\"Validation samples: {len(merged_dataset.match_tags('validation'))}\")\n",
        "print(f\"Test samples: {len(merged_dataset.match_tags('test'))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sstIlWKh1PQb"
      },
      "outputs": [],
      "source": [
        "# Save merged_dataset to disk using FiftyOneDataset type\n",
        "export_dir = Path.cwd() / \"Classification\" / \"curated_mnist_fiftyone\"\n",
        "export_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "merged_dataset.export(\n",
        "    export_dir=str(export_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    export_media=True  # This exports the actual image files along with metadata\n",
        ")\n",
        "\n",
        "print(f\"Merged dataset exported to: {export_dir}\")\n",
        "print(f\"Total samples exported: {len(merged_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoMJ0qu3peE1"
      },
      "source": [
        "## Key Insights\n",
        "Careful data augmentation has the potential to help with misclassified samples: By specifically targeting the samples that our model struggled with during initial training, we can create additional training examples that help the model learn to handle these edge cases more effectively. However we must do this carefully in order to not damage the model.\n",
        "\n",
        "Small rotations and elastic deformations are effective for handwritten digits: These transformations mimic natural variations in human handwriting without making digits ambiguous. The key is finding the right balance: enough variation to improve robustness, but not so much that a \"6\" looks like a \"9\".\n",
        "\n",
        "Fine-tuning with augmented data can improve model robustness: Rather than training from scratch, fine-tuning the already-trained model with augmented versions of problematic samples allows us to specifically address weaknesses while preserving the knowledge already learned.\n",
        "\n",
        "FiftyOne enables easy tracking of prediction changes across model versions: By storing predictions from both the original and retrained models in the same dataset, we can directly compare performance and identify which specific samples improved or degraded, enabling targeted analysis and further improvements.\n",
        "\n",
        "Now, take a look at the [public leaderboard scores on Kaggle for this problem](https://www.kaggle.com/competitions/digit-recognizer/leaderboard). How is it possible that some achieve 1.0 accuracy?  It's possible that many people are cheating and using data from the test set into their training (data leakage). This is evident due to the corrupted data that we have found on the test set, but this can't be proven unless we check their training workflow. **Can you achieve a 1.0 accuracy score after removing the low quality samples from the test set?** It's good to think about what we have done as an iteration in an open problem. The MNIST dataset could definitely be cleaner.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/impossible_due_to_mislabeled_data.png?raw=true)\n",
        "\n",
        "I recommend you to take a deep dive into the [Kaggle discussion forum on the problem](https://www.kaggle.com/competitions/digit-recognizer/discussion/61480).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM6Hf32Vqyix"
      },
      "source": [
        "## Suggested Exercises\n",
        "\n",
        "1. **Sample Quality Analysis**: Notice that we computed the uniqueness, representativeness, and hardness of the samples on the training set. Can you create filtered views of other batches of retraining data based on them?\n",
        "\n",
        "Try:\n",
        "* Selecting the most unique samples or those with highest hardness scores for additional augmentation\n",
        "* Filtering out the most ambiguous or questionable cases out of the fine-tuning training data\n",
        "\n",
        "2. **Active Learning**: Use the uniqueness, representativeness and hardness metrics to implement an [active learning pipeline](https://voxel51.com/blog/supercharge-your-annotation-workflow-with-active-learning) that selects the most informative samples for manual annotation or additional augmentation.\n",
        "\n",
        "3. **Dataset Exploration**: Apply these techniques to other classification datasets like [CIFAR-10](https://docs.voxel51.com/dataset_zoo/datasets.html#dataset-zoo-cifar10) or [Fashion-MNIST](https://docs.voxel51.com/dataset_zoo/datasets.html#dataset-zoo-fashion-mnist)\n",
        "\n",
        "4. **Architecture Comparison**: Implement and compare different CNN architectures (e.g. [Network in Network](https://arxiv.org/pdf/1312.4400), [ResNet](https://arxiv.org/pdf/1512.03385)) on MNIST\n",
        "\n",
        "\n",
        "5. **Custom Augmentations**: Design and test novel augmentation strategies specific to handwritten digits, try augmenting the dataset [adding colors to the digits](https://paperswithcode.com/dataset/colored-mnist) and see how this change impacts the model's performance.\n",
        "\n",
        "6. **CLIP Model Variants and Prompting**: Experiment with different CLIP model variants available in FiftyOne's Model Zoo and compare their zero-shot performance on MNIST. Test how different text prompts affect accuracy across model sizes.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "You explored two distinct paradigms for image classification on MNIST. CLIP demonstrated zero-shot capabilities through visual-language alignment, achieving a limited level accuracy (without requiring any training). LeNet-5 required supervised training but enabled targeted fine-tuning.\n",
        "\n",
        "The workflow revealed that embeddings expose semantic structure in our data, enabling clustering analysis, outlier detection, and similarity search. Metrics like uniqueness, representativeness, and hardness guided strategic data curation decisions. Visual debugging through FiftyOne exposed model weaknesses and validated improvements across training iterations.\n",
        "\n",
        "Data quality emerged as the critical factor. We identified mislabeled samples, found corrupted images in the test set, and discovered that targeted augmentation of difficult samples improved model robustness. The comparison between original and retrained models quantified these improvements and revealed which samples benefited from augmentation.\n",
        "\n",
        "This iterative approach applies beyond MNIST. The combination of embedding analysis, visual debugging, and strategic data curation forms a general framework for improving vision models. Foundation models provide strong baselines for exploration, while supervised models offer fine-grained control when domain-specific optimization matters."
      ],
      "metadata": {
        "id": "mclBxi4xwiks"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}