{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting Small Objects with SAHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Teaser](../_static/images/tutorials/small_object_detection.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object detection is one of the fundamental tasks in computer vision, but detecting small objects can be particularly challenging.\n",
    "\n",
    "In this walkthrough, you'll learn how to use a technique called [SAHI (Slicing Aided Hyper Inference)](https://ieeexplore.ieee.org/document/9897990) in conjunction with state-of-the-art object detection models to improve the detection of small objects. We'll apply SAHI with Ultralytics' YOLOv8 model to detect small objects in the VisDrone dataset, and then evaluate these predictions to better understand how slicing impacts detection performance.\n",
    "\n",
    "It covers the following:\n",
    "\n",
    "- Loading the VisDrone dataset from the Hugging Face Hub\n",
    "- Applying Ultralytics' YOLOv8 model to the dataset\n",
    "- Using SAHI to run inference on slices of the images\n",
    "- Evaluating model performance with and without SAHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this walkthrough, we'll be using the following libraries:\n",
    "\n",
    "- `fiftyone` for dataset exploration and manipulation\n",
    "- `huggingface_hub` for loading the VisDrone dataset\n",
    "- `ultralytics` for running object detection with YOLOv8\n",
    "- `sahi` for slicing aided hyper inference\n",
    "\n",
    "If you haven't already, install the latest versions of these libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -U fiftyone sahi ultralytics huggingface_hub --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get started! ðŸš€\n",
    "\n",
    "First, import the necessary modules from FiftyOne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "from fiftyone import ViewField as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's download some data. We'll be taking advantage of FiftyOne's [Hugging Face Hub integration](https://docs.voxel51.com/integrations/huggingface.html#huggingface-hub) to load a subset of the [VisDrone dataset](https://github.com/VisDrone/VisDrone-Dataset) directly from the [Hugging Face Hub](https://huggingface.co/docs/hub/en/index):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from Voxel51/VisDrone2019-DET\n",
      "Loading dataset\n",
      "Importing samples...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [33.1ms elapsed, 0s remaining, 3.0K samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset = fouh.load_from_hub(\"Voxel51/VisDrone2019-DET\", name=\"sahi-test\", max_samples=100, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before adding any predictions, let's take a look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![VisDrone](./images/sahi_dataset.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard Inference with YOLOv8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know what our data looks like, let's run our standard inference pipeline with a YOLOv8 (large-variant) model. We can load the model from Ultralytics and then apply this directly to our FiftyOne dataset using `apply_model()`, thanks to [FiftyOne's Ultralytics integration](https://docs.voxel51.com/integrations/ultralytics.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [25.0s elapsed, 0s remaining, 4.0 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "ckpt_path = \"yolov8l.pt\"\n",
    "model = YOLO(ckpt_path)\n",
    "## fiftyone will work directly with the Ultralytics.YOLO model\n",
    "\n",
    "dataset.apply_model(model, label_field=\"base_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if we want FiftyOne to handle the model downloading and location management for us, we can load the same model directly from the [FiftyOne Model Zoo](https://docs.voxel51.com/user_guide/model_zoo/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## comment this out if you want to use the model from the zoo\n",
    "# model = foz.load_zoo_model(\"yolov8l-coco-torch\")\n",
    "# ckpt_path = model.config.model_path\n",
    "# dataset.apply_model(model, label_field=\"base_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have predictions, we can visualize them in the App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Base Model Predictions](./images/sahi_base_model.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the model's predictions next to the ground truth, we can see a few things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, the classes detected by our *YOLOv8l* model are different from the ground truth classes in the VisDrone dataset. Our YOLO model was trained on the [COCO dataset](https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html#coco-2017), which has 80 classes, while the VisDrone dataset has 12 classes, including an `ignore_regions` class. To simplify the comparison, we'll focus on just the few most common classes in the dataset, and will map the VisDrone classes to the COCO classes as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\"pedestrians\": \"person\", \"people\": \"person\", \"van\": \"car\"}\n",
    "mapped_view = dataset.map_labels(\"ground_truth\", mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then filter our labels to only include the classes we're interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_fields(sample_collection):\n",
    "    \"\"\"Get the (detection) label fields of a Dataset or DatasetView.\"\"\"\n",
    "    label_fields = list(\n",
    "        sample_collection.get_field_schema(embedded_doc_type=fo.Detections).keys()\n",
    "    )\n",
    "    return label_fields\n",
    "\n",
    "def filter_all_labels(sample_collection):\n",
    "    label_fields = get_label_fields(sample_collection)\n",
    "\n",
    "    filtered_view = sample_collection\n",
    "\n",
    "    for lf in label_fields:\n",
    "        filtered_view = filtered_view.filter_labels(\n",
    "            lf, F(\"label\").is_in([\"person\", \"car\", \"truck\"]), only_matches=False\n",
    "        )\n",
    "    return filtered_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_view = filter_all_labels(mapped_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = filtered_view.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Filtered View](./images/sahi_base_model_predictions_filtered.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the classes are aligned and we've reduced the crowding in our images, we can see that while the model does a pretty good job of detecting objects, it struggles with the small objects, especially people in the distance. This can happen with large images, as most detection models are trained on fixed-size images. As an example, YOLOv8 is trained on images with maximum side length $640$. When we feed it an image of size $1920$ x $1080$, the model will downsample the image to $640$ x $360$ before making predictions. This downsampling can cause small objects to be missed, as the model may not have enough information to detect them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Small Objects with SAHI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically, one could train a model on larger images to improve detection of small objects, but this would require more memory and computational power. Another option is to introduce a sliding window approach, where we split the image into smaller patches, run the model on each patch, and then combine the results. This is the idea behind [Slicing Aided Hyper Inference](https://github.com/obss/sahi) (SAHI), which we'll use to improve the detection of small objects in the VisDrone dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "  <img src=\"https://raw.githubusercontent.com/obss/sahi/main/resources/sliced_inference.gif\" alt=\"Alt text\" style=\"width:100%\">\n",
    "  <figcaption style=\"text-align:center; color:gray;\">Illustration of Slicing Aided Hyper Inference. Image courtesy of SAHI Github Repo.</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SAHI technique is implemented in the `sahi` Python package that we installed earlier. SAHI is a framework which is compatible with many object detection models, including YOLOv8. We can choose the detection model we want to use and create an instance of any of the classes that subclass `sahi.models.DetectionModel`, including YOLOv8, YOLOv5, and even Hugging Face Transformers models. We will create our model object using SAHI's `AutoDetectionModel` class, specifying the model type and the path to the checkpoint file:\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi import AutoDetectionModel\n",
    "from sahi.predict import get_prediction, get_sliced_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_model = AutoDetectionModel.from_pretrained(\n",
    "    model_type='yolov8',\n",
    "    model_path=ckpt_path,\n",
    "    confidence_threshold=0.25, ## same as the default value for our base model\n",
    "    image_size=640,\n",
    "    device=\"cpu\", # or 'cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we generate sliced predictions, let's inspect the model's predictions on a trial image, using SAHI's `get_prediction()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sahi.prediction.PredictionResult object at 0x2b0e9c250>\n"
     ]
    }
   ],
   "source": [
    "result = get_prediction(dataset.first().filepath, detection_model, verbose=0)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, SAHI results objects have a `to_fiftyone_detections()` method, which converts the results to FiftyOne detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Detection: {\n",
      "    'id': '661858c20ae3edf77139db7a',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.6646394729614258,\n",
      "        0.7850866247106482,\n",
      "        0.06464214324951172,\n",
      "        0.09088355170355902,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.8933132290840149,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db7b',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.6196376800537109,\n",
      "        0.7399617513020833,\n",
      "        0.06670347849527995,\n",
      "        0.09494832356770834,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.8731599450111389,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db7c',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.5853352228800456,\n",
      "        0.7193766276041667,\n",
      "        0.06686935424804688,\n",
      "        0.07682359483506944,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.8595829606056213,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db7d',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.5635160446166992,\n",
      "        0.686444091796875,\n",
      "        0.06365642547607422,\n",
      "        0.06523607042100694,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.854781448841095,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db7e',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.7365047454833984,\n",
      "        0.8709894816080729,\n",
      "        0.07815799713134766,\n",
      "        0.06583930121527778,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.8482972383499146,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db7f',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.4387975692749023,\n",
      "        0.7973368326822917,\n",
      "        0.07478656768798828,\n",
      "        0.08685709635416666,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.8482537865638733,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db80',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.41441831588745115,\n",
      "        0.7553463971173322,\n",
      "        0.07797966003417969,\n",
      "        0.09232432047526042,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.8444766402244568,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db81',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.4094355583190918,\n",
      "        0.7256359524197049,\n",
      "        0.07238206863403321,\n",
      "        0.07048272026909722,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.798665463924408,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db82',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.5339123407999674,\n",
      "        0.6121687712492766,\n",
      "        0.07190316518147787,\n",
      "        0.07292734781901042,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.7936845421791077,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db83',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.03444666067759196,\n",
      "        0.5164913601345487,\n",
      "        0.03219949007034302,\n",
      "        0.06044175889756945,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.740483820438385,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db84',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.3923538525899251,\n",
      "        0.6745626378942419,\n",
      "        0.06798810958862304,\n",
      "        0.07528584798177083,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.6714914441108704,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db85',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.5216399192810058,\n",
      "        0.5886645846896701,\n",
      "        0.06560036341349283,\n",
      "        0.059334818522135416,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.6649367809295654,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db86',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.5096873283386231,\n",
      "        0.5273054334852431,\n",
      "        0.0551149050394694,\n",
      "        0.07670672381365741,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.6273276805877686,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db87',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.37222995758056643,\n",
      "        0.5739804303204572,\n",
      "        0.06103067398071289,\n",
      "        0.06263699001736112,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.5973840355873108,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db88',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.05044105052947998,\n",
      "        0.44830712212456597,\n",
      "        0.02773451805114746,\n",
      "        0.054146491156684025,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.5668562054634094,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db89',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        0.38649218877156577,\n",
      "        0.6419422290943287,\n",
      "        0.0629791259765625,\n",
      "        0.05787251790364583,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.525834858417511,\n",
      "    'index': None,\n",
      "}>, <Detection: {\n",
      "    'id': '661858c20ae3edf77139db8a',\n",
      "    'attributes': {},\n",
      "    'tags': [],\n",
      "    'label': 'car',\n",
      "    'bounding_box': [\n",
      "        3.7088990211486816e-05,\n",
      "        0.5483550460250289,\n",
      "        0.027724572022755942,\n",
      "        0.06541680230034722,\n",
      "    ],\n",
      "    'mask': None,\n",
      "    'confidence': 0.5090425610542297,\n",
      "    'index': None,\n",
      "}>]\n"
     ]
    }
   ],
   "source": [
    "print(result.to_fiftyone_detections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes our lives easy, so we can focus on the data and not the nitty gritty details of format conversions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAHI's `get_sliced_prediction()` function works in the same way as `get_prediction()`, with a few additional hyperparameters that let us configure how the image is sliced. In particular, we can specify the slice height and width, and the overlap between slices. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sliced_result = get_sliced_prediction(\n",
    "    dataset.skip(40).first().filepath,\n",
    "    detection_model,\n",
    "    slice_height = 320,\n",
    "    slice_width = 320,\n",
    "    overlap_height_ratio = 0.2,\n",
    "    overlap_width_ratio = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, we can compare the number of detections in the sliced predictions to the number of detections in the original predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detections predicted without slicing: 17\n",
      "Detections predicted with slicing: 73\n"
     ]
    }
   ],
   "source": [
    "num_sliced_dets = len(sliced_result.to_fiftyone_detections())\n",
    "num_orig_dets = len(result.to_fiftyone_detections())\n",
    "\n",
    "print(f\"Detections predicted without slicing: {num_orig_dets}\")\n",
    "print(f\"Detections predicted with slicing: {num_sliced_dets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the number of predictions increased substantially! We have yet to determine if the additional predictions are valid, or if we just have more false positives. We'll do this using [FiftyOne's Evaluation API](https://docs.voxel51.com/user_guide/evaluation.html) shortly. We also want to find a good set of hyperparameters for our slicing. To do all of these things, we're going to need to apply SAHI to the entire dataset. Let's do that now!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simplify the process, we'll define a function that adds predictions to a sample in a specified label field, and then we will iterate over the dataset, applying the function to each sample. This function will pass the sample's filepath and slicing hyperparameters to `get_sliced_prediction()`, and then add the predictions to the sample in the specified label field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_slicing(sample, label_field, **kwargs):\n",
    "    result = get_sliced_prediction(\n",
    "        sample.filepath, detection_model, verbose=0, **kwargs\n",
    "    )\n",
    "    sample[label_field] = fo.Detections(detections=result.to_fiftyone_detections())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll keep the slice overlap fixed at $0.2$, and see how the slice height and width affect the quality of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [13.6m elapsed, 0s remaining, 0.1 samples/s]    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:06:59 - INFO - eta.core.utils -    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [13.6m elapsed, 0s remaining, 0.1 samples/s]    \n"
     ]
    }
   ],
   "source": [
    "kwargs = {\"overlap_height_ratio\": 0.2, \"overlap_width_ratio\": 0.2}\n",
    "\n",
    "for sample in dataset.iter_samples(progress=True, autosave=True):\n",
    "    predict_with_slicing(sample, label_field=\"small_slices\", slice_height=320, slice_width=320, **kwargs)\n",
    "    predict_with_slicing(sample, label_field=\"large_slices\", slice_height=480, slice_width=480, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how these inference times are much longer than the original inference time. This is because we're running the model on multiple slices *per* image, which increases the number of forward passes the model has to make. This is a trade-off we're making to improve the detection of small objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's once again filter our labels to only include the classes we're interested in, and visualize the results in the FiftyOne App:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_view = filter_all_labels(mapped_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:18:33 - INFO - fiftyone.core.session.session -   Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(filtered_view, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sliced Model Predictions](./images/sahi_slices.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results certainly look promising! From a few visual examples, slicing seems to improve the coverage of ground truth detections, and smaller slices in particular seem to lead to more of the `person` detections being captured. But how can we know for sure? Let's run an evaluation routine to mark the detections as true positives, false positives, or false negatives, so that we can compare the sliced predictions to the ground truth. We'll use our filtered view's `evaluate_detections()` method to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating SAHI Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using FiftyOne's Evaluation API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sticking with our filtered view of the dataset, let's run an evaluation routine comparing our predictions from each of the prediction label fields to the ground truth labels. We will use the `evaluate_detections()` method, which will mark each detection as a true positive, false positive, or false negative. Here we use the default IoU threshold of $0.5$, but you can adjust this as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_results = filtered_view.evaluate_detections(\"base_model\", gt_field=\"ground_truth\", eval_key=\"eval_base_model\")\n",
    "large_slice_results = filtered_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_large_slices\")\n",
    "small_slice_results = filtered_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_slices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a report for each:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         car       0.81      0.55      0.66       692\n",
      "      person       0.94      0.16      0.28      7475\n",
      "       truck       0.66      0.34      0.45       265\n",
      "\n",
      "   micro avg       0.89      0.20      0.33      8432\n",
      "   macro avg       0.80      0.35      0.46      8432\n",
      "weighted avg       0.92      0.20      0.31      8432\n",
      "\n",
      "--------------------------------------------------\n",
      "Large slice results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         car       0.67      0.71      0.69       692\n",
      "      person       0.89      0.34      0.49      7475\n",
      "       truck       0.55      0.45      0.49       265\n",
      "\n",
      "   micro avg       0.83      0.37      0.51      8432\n",
      "   macro avg       0.70      0.50      0.56      8432\n",
      "weighted avg       0.86      0.37      0.51      8432\n",
      "\n",
      "--------------------------------------------------\n",
      "Small slice results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         car       0.66      0.75      0.70       692\n",
      "      person       0.84      0.42      0.56      7475\n",
      "       truck       0.49      0.46      0.47       265\n",
      "\n",
      "   micro avg       0.80      0.45      0.57      8432\n",
      "   macro avg       0.67      0.54      0.58      8432\n",
      "weighted avg       0.82      0.45      0.57      8432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Base model results:\")\n",
    "base_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Large slice results:\")\n",
    "large_slice_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Small slice results:\")\n",
    "small_slice_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that as we introduce more slices, the number of false positives increases, while the number of false negatives decreases. This is expected, as the model is able to detect more objects with more slices, but also makes more mistakes! You could apply more agressive confidence thresholding to combat this increase in false positives, but even without doing this the $F_1$-score has significantly improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Performance on Small Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dive a little bit deeper into these results. We noted earlier that the model struggles with small objects, so let's see how these three approaches fare on objects smaller than $32 \\times 32$ pixels. We can perform this filtering using FiftyOne's [ViewField](https://docs.voxel51.com/recipes/creating_views.html#View-expressions):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filtering for only small boxes\n",
    "\n",
    "box_width, box_height = F(\"bounding_box\")[2], F(\"bounding_box\")[3]\n",
    "rel_bbox_area = box_width * box_height\n",
    "\n",
    "im_width, im_height = F(\"$metadata.width\"), F(\"$metadata.height\")\n",
    "abs_area = rel_bbox_area * im_width * im_height\n",
    "\n",
    "small_boxes_view = filtered_view\n",
    "for lf in get_label_fields(filtered_view):\n",
    "    small_boxes_view = small_boxes_view.filter_labels(lf, abs_area < 32**2, only_matches=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = small_boxes_view.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Small Box View](./images/sahi_small_boxes_view.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:54:36 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1.1m elapsed, 0s remaining, 7.8 samples/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:55:44 - INFO - eta.core.utils -    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1.1m elapsed, 0s remaining, 7.8 samples/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:55:44 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1.2m elapsed, 0s remaining, 6.2 samples/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:56:59 - INFO - eta.core.utils -    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1.2m elapsed, 0s remaining, 6.2 samples/s]       \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating detections...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:56:59 - INFO - fiftyone.utils.eval.detection -   Evaluating detections...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1.4m elapsed, 0s remaining, 5.7 samples/s]       \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "04/12/2024 10:58:23 - INFO - eta.core.utils -    100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [1.4m elapsed, 0s remaining, 5.7 samples/s]       \n"
     ]
    }
   ],
   "source": [
    "small_boxes_base_results = small_boxes_view.evaluate_detections(\"base_model\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_base_model\")\n",
    "small_boxes_large_slice_results = small_boxes_view.evaluate_detections(\"large_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_large_slices\")\n",
    "small_boxes_small_slice_results = small_boxes_view.evaluate_detections(\"small_slices\", gt_field=\"ground_truth\", eval_key=\"eval_small_boxes_small_slices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Small Box â€” Base model results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         car       0.71      0.25      0.37       147\n",
      "      person       0.83      0.08      0.15      5710\n",
      "       truck       0.00      0.00      0.00        28\n",
      "\n",
      "   micro avg       0.82      0.08      0.15      5885\n",
      "   macro avg       0.51      0.11      0.17      5885\n",
      "weighted avg       0.82      0.08      0.15      5885\n",
      "\n",
      "--------------------------------------------------\n",
      "Small Box â€” Large slice results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         car       0.46      0.48      0.47       147\n",
      "      person       0.82      0.23      0.35      5710\n",
      "       truck       0.20      0.07      0.11        28\n",
      "\n",
      "   micro avg       0.78      0.23      0.36      5885\n",
      "   macro avg       0.49      0.26      0.31      5885\n",
      "weighted avg       0.80      0.23      0.36      5885\n",
      "\n",
      "--------------------------------------------------\n",
      "Small Box â€” Small slice results:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         car       0.42      0.53      0.47       147\n",
      "      person       0.79      0.31      0.45      5710\n",
      "       truck       0.21      0.18      0.19        28\n",
      "\n",
      "   micro avg       0.75      0.32      0.45      5885\n",
      "   macro avg       0.47      0.34      0.37      5885\n",
      "weighted avg       0.77      0.32      0.45      5885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Small Box â€” Base model results:\")\n",
    "small_boxes_base_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Small Box â€” Large slice results:\")\n",
    "small_boxes_large_slice_results.print_report()\n",
    "\n",
    "print(\"-\" * 50)\n",
    "print(\"Small Box â€” Small slice results:\")\n",
    "small_boxes_small_slice_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes the value of SAHI crystal clear! The recall when using SAHI is much higher for small objects without significant dropoff in precision, leading to improved F1-score. This is especially pronounced for `person` detections, where the $F_1$ score is tripled!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Edge Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know SAHI is effective at detecting small objects, let's look at the places where our predictions are most confident but do not align with the ground truth labels. We can do this by creating an evaluation patches view, filtering for predictions tagged as false positives and sorting by confidence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_conf_fp_view = filtered_view.to_evaluation_patches(eval_key=\"eval_small_slices\").match(F(\"type\")==\"fp\").sort_by(\"small_slices.detection.confidence\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.view = high_conf_fp_view.view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![False Positives View](./images/sahi_high_conf_fp_view.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at these results, we can see that in many cases, our predictions look quite reasonable, and it seems that the ground truth labels are missing! This is certainly not the case for every prediction, but there are definitely enough to motivate a relabeling of the ground truth data. This is where human-in-the-loop (HITL) workflows can be very useful, as they allow human annotators to review and correct the labels. After this process, we can reevaluate our trained models (with or without SAHI) on the updated dataset, and then train new models on the updated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough, we've covered how to add SAHI predictions to your data, and then rigorously evaluate the impacts of slicing on prediction quality. We've seen how slicing-aided hyper-inference (SAHI) can improve the recall and $F_1$-score for detection, especially for small objects, without needing to train a model on larger images.\n",
    "\n",
    "To maximize the effectiveness of SAHI, you may want to experiment with the following:\n",
    "\n",
    "- Slicing hyperparameters, such as slice height and width, and overlap. \n",
    "- Base object detection models, as SAHI is compatible with many models, including YOLOv5, and Hugging Face Transformers models.\n",
    "- Confidence thresholding (potentially on a class-by-class basis), to reduce the number of false positives.\n",
    "- Post-processing techniques, such as [non-maximum suppression (NMS)](https://docs.voxel51.com/api/fiftyone.utils.labels.html#fiftyone.utils.labels.perform_nms), to reduce the number of overlapping detections.\n",
    "- Human-in-the-loop (HITL) workflows, to correct ground truth labels.\n",
    "\n",
    "You will also want to determine which evaluation metrics make the most sense for your use case!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you found this tutorial helpful, you may also be interested in the following resources:\n",
    "\n",
    "- [Tutorial on Evaluating Object Detections](https://docs.voxel51.com/tutorials/evaluate_detections.html)\n",
    "- [Tutorial on Finding Object Detection Mistakes](https://docs.voxel51.com/tutorials/detection_mistakes.html)\n",
    "- [Tutorial on Fine-tuning YOLOv8 on Custom Data](https://docs.voxel51.com/tutorials/yolov8.html)\n",
    "- [FiftyOne Plugin for Comparing Models on Specific Detections](https://github.com/allenleetc/model-comparison)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fdev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
