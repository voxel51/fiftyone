{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "467a82be",
   "metadata": {},
   "source": [
    "# Detecting Subtle Anomalies in Agricultural Datasets\n",
    "\n",
    "Real-world agricultural imagery presents unique challenges for anomaly detection. Leaves in orchards and plantations are captured under **uncontrolled environmental conditions** ‚Äî changing illumination, variable backgrounds, and differences in distance or orientation. In addition, **image sizes and resolutions vary widely**, adding further complexity to building consistent models.\n",
    "\n",
    "In this notebook, we‚Äôll explore these challenges using the **RoCoLe dataset**, which contains images of coffee leaves under both healthy and diseased conditions. Many of these samples appear *visually similar*, even across classes, making it difficult to spot small-scale or early-stage anomalies. At the end of the notebook we will explore a tiled dataset and compare that approach for anomaly detection in agriculture. \n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. **Explore and curate** the original dataset with [FiftyOne](https://voxel51.com/fiftyone/), identifying a critical subset of visually similar healthy vs. unhealthy samples.  \n",
    "2. **Investigate anomaly detection** approaches when differences are subtle and localized.  \n",
    "3. **Compare two data preparation strategies:**\n",
    "   - **Masking the leaf region** ‚Äî isolating the target area to suppress background noise.  \n",
    "   - **Tiling the patches** ‚Äî splitting images into small, consistent crops to amplify local anomalies.  \n",
    "4. **Train and evaluate** a *PaDiM* model under both strategies, analyzing how masking and tiling affect anomaly localization and detection performance.\n",
    "\n",
    "---\n",
    "\n",
    "This walkthrough highlights how *data-centric techniques* ‚Äî careful curation, spatial priors, and representation consistency ‚Äî can help models detect small, context-sensitive anomalies in agricultural datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975c12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install --upgrade pip\n",
    "# %pip install fiftyone, torch, torchvision, umap\n",
    "# %pip install anomalib\n",
    "# %pip install anomalib[vlm_clip]\n",
    "# %pip install gdown\n",
    "# %pip install pycocotools\n",
    "# %pip install sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903099a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo # base library and app\n",
    "import fiftyone.brain as fob # ML methods\n",
    "import fiftyone.zoo as foz # zoo datasets and models\n",
    "from fiftyone import ViewField as F # helper for defining views\n",
    "import fiftyone.utils.huggingface as fouh # Hugging Face integration\n",
    "import fiftyone.types as fot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ed8546",
   "metadata": {},
   "source": [
    "## Download the original dataset from my Google Drive Folder. \n",
    "### üì¶ Importing the RoCoLe Dataset from Google Drive\n",
    "\n",
    "We‚Äôll begin by importing the **RoCoLe (Robusta Coffee Leaf)** dataset directly from Google Drive.  \n",
    "This dataset contains images of coffee leaves under **healthy** and **diseased** conditions, captured in natural environments with varying lighting, backgrounds, and scales.\n",
    "\n",
    "Using `gdown`, we‚Äôll download the compressed dataset (`rocole_original.zip`) and extract it locally.  \n",
    "This will make the image data available for exploration and analysis with FiftyOne in the next steps.\n",
    "\n",
    "\n",
    "- [Original dataset](https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/c5yvn32dzg-2.zip)\n",
    "- [Paper](https://www.sciencedirect.com/science/article/pii/S2352340919307693)\n",
    "- [My Google Drive link](https://drive.google.com/file/d/1FnObgIu_G2sQwUa5nfGMZxCVbiyoVa8E/view?usp=drive_link) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcffffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# Download the coffee dataset from Google Drive\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1FnObgIu_G2sQwUa5nfGMZxCVbiyoVa8E\"  # original\n",
    "gdown.download(url, output=\"rocole_original.zip\", quiet=False)\n",
    "!unzip rocole_original.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b40f22",
   "metadata": {},
   "source": [
    "## üìÇ Loading the RoCoLe Dataset into FiftyOne\n",
    "\n",
    "Once the dataset is extracted, we‚Äôll load it into FiftyOne to enable interactive exploration and visualization.\n",
    "\n",
    "The dataset follows the **COCO format**, which stores both **bounding box detections** and **segmentation masks** for each leaf image.  \n",
    "We‚Äôll use `fo.Dataset.from_dir()` to create a persistent FiftyOne dataset named `coffee_rocole_coco`.\n",
    "\n",
    "Key parameters:\n",
    "- `dataset_dir`: the local path to the dataset folder.  \n",
    "- `dataset_type`: specifies the annotation format (`COCODetectionDataset`).  \n",
    "- `label_types`: includes both `\"detections\"` and `\"segmentations\"` fields.  \n",
    "- `include_id=True`: ensures each annotation keeps its unique identifier.  \n",
    "\n",
    "Once loaded, the dataset will be saved persistently for future sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f62929",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"coffee_rocole\"  # change if you like\n",
    "dataset_dir  = \"rocole-DatasetNinja\"\n",
    "\n",
    "# Create (or overwrite) the dataset from a standard directory tree\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fo.types.COCODetectionDataset,  # or another supported type\n",
    "    name=dataset_name,\n",
    "    label_types=[\"detections\", \"segmentations\"],\n",
    "    # If your segmentations are polygons, keep them as polylines (optional):\n",
    "    # use_polylines=True,\n",
    "    include_id=True, \n",
    ")\n",
    "\n",
    "dataset.persistent = True  # keep it around between sessions\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a60a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, port=5151, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf69aa58",
   "metadata": {},
   "source": [
    "## Generating Image Embeddings with ResNet50 and Visualizing Similarities\n",
    "\n",
    "To better understand the relationships between images in the RoCoLe dataset, we‚Äôll extract **feature embeddings** using a pretrained **ResNet50** model from the FiftyOne Model Zoo.\n",
    "\n",
    "These embeddings capture high-level visual information about each image ‚Äî color, texture, and structure ‚Äî which will later help us identify clusters of **similar** or **anomalous** samples.\n",
    "\n",
    "Steps performed in this cell:\n",
    "1. **Load the ResNet50 model** trained on ImageNet.  \n",
    "2. **Compute embeddings** for all images in the dataset and store them in a new field called `\"resnet50_embeddings\"`.  \n",
    "3. Use **UMAP** (Uniform Manifold Approximation and Projection) to reduce the embedding dimensions for visualization.  \n",
    "   The result, stored under the `brain_key` `\"resnet50_vis\"`, allows us to plot images in a 2D similarity space within the FiftyOne App.\n",
    "\n",
    "This embedding visualization helps identify patterns such as clusters of healthy vs. unhealthy leaves and outliers that may represent subtle anomalies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4e2459",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\n",
    "    \"resnet50-imagenet-torch\"\n",
    ")  # load the ResNet50 model from the zoo\n",
    "\n",
    "# Compute embeddings for the dataset ‚Äî this might take a while on a CPU\n",
    "dataset.compute_embeddings(model=model, embeddings_field=\"resnet50_embeddings\")\n",
    "\n",
    "# Dimensionality reduction using UMAP on the embeddings\n",
    "fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=\"resnet50_embeddings\",\n",
    "    method=\"umap\",\n",
    "    brain_key=\"resnet50_vis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32f5be4",
   "metadata": {},
   "source": [
    "## Exploring Leaf Patches with FiftyOne Brain Visualization\n",
    "\n",
    "To study **localized variations** within each leaf image, we can analyze the **individual patches** (detections) rather than the full images.  \n",
    "This approach helps focus on the leaf areas themselves, reducing background influence from soil, branches, or lighting differences.\n",
    "\n",
    "In this step:\n",
    "- We use `fiftyone.brain.compute_visualization()` to generate a **2D embedding** of all detected patches.  \n",
    "- The **`patches_field`** parameter (`\"detections\"`) tells FiftyOne to work at the patch level rather than full images.  \n",
    "- UMAP (`method=\"umap\"`) is applied again for dimensionality reduction, producing an interpretable **similarity map** of leaf patches.  \n",
    "- The results are stored under the `brain_key` `\"patches_viz\"`, allowing us to visualize the patch-level embedding space in the FiftyOne App.\n",
    "\n",
    "This helps us discover clusters of visually similar patches and potentially identify **subtle anomalies** that differ in color, texture, or structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a7eb59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    patches_field=\"detections\",  # or your segmentation field\n",
    "    brain_key=\"patches_viz\",\n",
    "    num_dims=2,\n",
    "    method=\"umap\",\n",
    "    verbose=True,\n",
    "    seed=51,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f4db41",
   "metadata": {},
   "source": [
    "## Computing Patch Similarity with CLIP Embeddings\n",
    "\n",
    "Next, we‚Äôll compute a **similarity index** between all leaf patches to enable semantic search and comparison.  \n",
    "While the previous UMAP visualization helped us see global relationships, this step lets us **quantitatively measure how similar** different leaf regions are to one another.\n",
    "\n",
    "Using `fiftyone.brain.compute_similarity()`:\n",
    "- The **`patches_field`** (`\"detections\"`) specifies which regions to compare.  \n",
    "- The **CLIP model** (`\"clip-vit-base32-torch\"`) encodes each patch into a semantic embedding space, capturing both visual and contextual features.  \n",
    "- The resulting **similarity index** is stored under the `brain_key` `\"gt_sim\"`.  \n",
    "\n",
    "This allows us to:\n",
    "- Search for patches similar to a given region of interest.  \n",
    "- Identify visually related anomalies or healthy patterns.  \n",
    "- Explore subtle visual cues that may not be obvious in the full image context.\n",
    "\n",
    "Overall, this similarity computation provides a foundation for **content-based retrieval** and **fine-grained anomaly discovery** in the RoCoLe dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a73003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index ground truth objects by similarity\n",
    "fob.compute_similarity(\n",
    "    dataset,\n",
    "    patches_field=\"detections\",  # your field containing patches\n",
    "    model=\"clip-vit-base32-torch\", # or another supported model\n",
    "    brain_key=\"gt_sim\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7400d1db",
   "metadata": {},
   "source": [
    "## Preparing Similar Leaf Patches for Anomaly Detection\n",
    "\n",
    "To train and evaluate an anomaly detection model effectively, we‚Äôll first isolate a subset of **visually similar patches** ‚Äî regions that look alike but belong to either **healthy** or **unhealthy** classes.  \n",
    "This helps us focus on cases where anomalies are subtle and occur within visually homogeneous groups (e.g., similar textures, lighting, and orientation).\n",
    "\n",
    "Steps in this cell:\n",
    "\n",
    "1. **Convert detections into patch samples**  \n",
    "   Using `dataset.to_patches(\"detections\")`, we extract each annotated leaf patch as a standalone sample. This enables patch-level training and visualization.\n",
    "\n",
    "2. **Select a query patch**  \n",
    "   We pick one patch (`query_patch_id`) as the reference and retrieve its most visually similar counterparts using the previously computed similarity index (`\"gt_sim\"`).\n",
    "\n",
    "3. **Create two specialized subsets:**\n",
    "   - **`healthy_view`** ‚Üí patches labeled as *healthy*.  \n",
    "   - **`anormal_view`** ‚Üí patches labeled as *non-healthy* (i.e., diseased or anomalous).  \n",
    "\n",
    "This setup prepares the foundation for our **PaDiM anomaly detection experiments**, where both groups share visual similarity, but subtle differences reveal the presence of disease or damage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5b2838",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_view = dataset.to_patches(\"detections\")\n",
    "session.view = patches_view\n",
    "\n",
    "# Use your specific patch ID\n",
    "# query_patch_id = \"68f2a0732aa8709f10a96803\"\n",
    "query_patch_id = patches_view.take(1).first().id \n",
    "\n",
    "# Sort patches by similarity to the chosen patch\n",
    "similar_patches_view = patches_view.sort_by_similarity(\n",
    "    query_patch_id, k=700, brain_key=\"gt_sim\"\n",
    ")\n",
    "\n",
    "# --- Create the healthy (normal) view ---\n",
    "healthy_view = similar_patches_view.match(\n",
    "    F(\"detections.label\") == \"healthy\"\n",
    ")\n",
    "\n",
    "# --- Create the anormal (non-healthy) view ---\n",
    "anormal_view = similar_patches_view.match(\n",
    "    F(\"detections.label\") != \"healthy\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb78044c",
   "metadata": {},
   "source": [
    "## Exporting Healthy and Anomalous Patches for Training\n",
    "\n",
    "With our subsets of visually similar **healthy** and **non-healthy** (anomalous) leaf patches prepared, we now export them as separate image directories.  \n",
    "This structure is ideal for training anomaly detection models such as **PaDiM**, which require folder-based datasets with distinct ‚Äúnormal‚Äù and ‚Äúanomalous‚Äù categories.\n",
    "\n",
    "In this step:\n",
    "\n",
    "1. **Define output directories** for both classes:\n",
    "   - `normal_dir` ‚Üí healthy leaf patches  \n",
    "   - `anormal_dir` ‚Üí diseased or damaged leaf patches  \n",
    "\n",
    "2. **Export each subset** using FiftyOne‚Äôs `export()` method with `ImageDirectory` format.  \n",
    "   This creates two independent, ready-to-train datasets compatible with most anomaly detection pipelines.\n",
    "\n",
    "The resulting directory layout will look like:\n",
    "\n",
    "```\n",
    "rocole_patches/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ normal/\n",
    "‚îÇ ‚îú‚îÄ‚îÄ patch_0001.png\n",
    "‚îÇ ‚îú‚îÄ‚îÄ patch_0002.png\n",
    "‚îÇ ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ\n",
    "‚îî‚îÄ‚îÄ anormal/\n",
    "‚îú‚îÄ‚îÄ patch_1001.png\n",
    "‚îú‚îÄ‚îÄ patch_1002.png\n",
    "‚îî‚îÄ‚îÄ ...\n",
    "```\n",
    "\n",
    "These exports provide the clean, structured data we‚Äôll use for training and evaluating PaDiM under different preprocessing strategies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb5f828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.expanduser(\"rocole_patches\")\n",
    "\n",
    "# Option B (Databricks-friendly): uncomment this instead\n",
    "# base_dir = \"/dbfs/FileStore/rocole_patches\"\n",
    "\n",
    "normal_dir  = os.path.join(base_dir, \"normal\")\n",
    "anormal_dir = os.path.join(base_dir, \"anormal\")\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(normal_dir, exist_ok=True)\n",
    "os.makedirs(anormal_dir, exist_ok=True)\n",
    "\n",
    "# --- Export ---\n",
    "healthy_view.export(\n",
    "    export_dir=normal_dir,\n",
    "    dataset_type=fo.types.ImageDirectory,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "anormal_view.export(\n",
    "    export_dir=anormal_dir,\n",
    "    dataset_type=fo.types.ImageDirectory,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8878af2",
   "metadata": {},
   "source": [
    "# Create the anomalib detection model for detecting healthy and unhealthy leaves.\n",
    "\n",
    "## Training the PaDiM Anomaly Detection Model\n",
    "\n",
    "With our **healthy (normal)** and **anomalous (non-healthy)** leaf patch datasets prepared, we can now train a **PaDiM** (Patch Distribution Modeling) anomaly detection model using the [Anomalib](https://github.com/openvinotoolkit/anomalib) framework.\n",
    "\n",
    "PaDiM is well-suited for this task because it models the distribution of feature embeddings extracted from pretrained backbones, allowing it to detect subtle deviations in texture, color, or structure ‚Äî exactly the kind of anomalies present in coffee leaf diseases.\n",
    "\n",
    "### Steps in this section:\n",
    "\n",
    "1. **Environment setup**\n",
    "   - Clear GPU and memory caches for a clean training start.\n",
    "   - Ensure `PYTORCH_CUDA_ALLOC_CONF` is configured for efficient memory handling.\n",
    "\n",
    "2. **Data preparation**\n",
    "   - Create a `Folder` datamodule pointing to our exported patch directories (`normal` and `anormal`).\n",
    "   - Anomalib automatically resizes all inputs to **256√ó256**, ensuring consistent patch size.\n",
    "   - Adjust `train_batch_size` and `num_workers` to balance speed and memory usage.\n",
    "\n",
    "3. **Model configuration**\n",
    "   - Use a lightweight **ResNet18** backbone for PaDiM.\n",
    "   - Extract mid-level features from layers `[\"layer2\", \"layer3\"]` (or just `\"layer3\"` for limited memory).\n",
    "   - `n_features` controls the feature dimensionality and memory footprint.\n",
    "\n",
    "4. **Training**\n",
    "   - Instantiate the **Anomalib Engine**, leveraging GPU acceleration.\n",
    "   - Call `engine.fit()` to begin model training on the RoCoLe patch dataset.\n",
    "\n",
    "Once training completes, you‚Äôll have a PaDiM model specialized in identifying **subtle leaf anomalies** under visually similar conditions ‚Äî a core challenge in real-world agricultural inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c574896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- env hygiene (optional but helpful) ---\n",
    "import os, gc, torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# --- Anomalib: datamodule + model + trainer ---\n",
    "from anomalib.data import Folder\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.models.image.padim.lightning_model import Padim  # direct import avoids WinCLIP deps\n",
    "\n",
    "# Datamodule applies a 256x256 resize internally\n",
    "datamodule = Folder(\n",
    "    name=\"rocole\",\n",
    "    root=\"rocole_patches\",\n",
    "    normal_dir=\"normal\",\n",
    "    abnormal_dir=\"anormal\",\n",
    "    train_batch_size=4,      # reduce until it fits (8 ‚Üí 4 ‚Üí 2 ‚Üí 1)\n",
    "    eval_batch_size=4,\n",
    "    num_workers=4,\n",
    ")\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "# Lightweight PaDiM (fits easier)\n",
    "model = Padim(\n",
    "    backbone=\"resnet18\",\n",
    "    layers=[\"layer2\", \"layer3\"],    # or [\"layer3\"] if memory is tight\n",
    "    n_features=50,                  # 32‚Äì100; lower uses less memory\n",
    "    pre_trained=True,\n",
    ")\n",
    "\n",
    "engine = Engine(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "engine.fit(model=model, datamodule=datamodule)\n",
    "# (optional) run validation/test afterwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dfea3f",
   "metadata": {},
   "source": [
    "## Running Inference on the RoCoLe Patch Dataset\n",
    "\n",
    "After training, we‚Äôll use the trained **PaDiM** model to perform inference on all patches in the dataset.  \n",
    "This step generates **anomaly predictions** ‚Äî identifying which patches are normal and which show potential defects or disease symptoms.\n",
    "\n",
    "### Key steps in this cell:\n",
    "\n",
    "1. **Define label mapping**\n",
    "   - `label_map = {0: \"normal\", 1: \"anormal\"}`  \n",
    "     This maps the binary model outputs to human-readable class labels.\n",
    "\n",
    "2. **Specify prediction field**\n",
    "   - `pred_field = \"predictions\"` will store the model output for each sample.\n",
    "\n",
    "3. **Run inference**\n",
    "   - Using `engine.predict()`, we feed the dataset through the trained model.\n",
    "   - The `return_predictions=True` flag ensures that predictions are returned as a list of batch dictionaries, containing:\n",
    "     - `pred_label` ‚Üí model‚Äôs binary decision (0 = normal, 1 = anomalous)  \n",
    "     - `pred_score` ‚Üí anomaly confidence score  \n",
    "     - optional spatial outputs such as anomaly maps or masks.\n",
    "\n",
    "This inference step enables us to evaluate how well the trained PaDiM model distinguishes **subtle abnormalities** in visually similar leaf patches.\n",
    "\n",
    "Model should be saved in ```/results/Padim/rocole/latest/weights/lightning/model.ckpt```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ba99b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map    = {0: \"normal\", 1: \"anormal\"}\n",
    "pred_field   = \"predictions\"\n",
    "\n",
    "# 4) Run prediction via datamodule (returns list of batch dicts)\n",
    "pred_batches = engine.predict(\n",
    "    model=model,\n",
    "    datamodule=datamodule,\n",
    "    return_predictions=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c4bb27",
   "metadata": {},
   "source": [
    "## Inspecting the Model‚Äôs Prediction Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a038ecf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(pred_batches))\n",
    "print(pred_batches)\n",
    "# or check only the first element\n",
    "print(type(pred_batches[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6fdf5",
   "metadata": {},
   "source": [
    "## Re-importing the Exported Patches into FiftyOne\n",
    "\n",
    "After running inference with Anomalib, we‚Äôll re-import the exported **RoCoLe patch dataset** into FiftyOne.  \n",
    "This allows us to visualize and analyze the predictions interactively alongside their corresponding patch images.\n",
    "\n",
    "In this step:\n",
    "1. **Load the directory tree** of patch images (`rocole_patches/`) into a new FiftyOne dataset named `\"coffee_rocole_original_patches2\"`.  \n",
    "   - The dataset uses the `ImageClassificationDirectoryTree` type, which automatically assigns labels (`normal` or `anormal`) based on folder names.  \n",
    "   - The label field is stored under `\"detections\"` for consistency with earlier steps.  \n",
    "\n",
    "2. **Make the dataset persistent**, so it remains available between sessions for further visualization, evaluation, and annotation.\n",
    "\n",
    "This re-imported dataset (`dataset2`) will serve as the base for attaching the **PaDiM model‚Äôs predictions**, visualizing anomaly scores, and comparing ground-truth vs. predicted labels within the FiftyOne App.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138e57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.types as fot\n",
    "\n",
    "#dataset = fo.load_dataset(\"coffee_rocole_patches\")\n",
    "dataset_name = \"coffee_rocole_original_patches\"  # change if you like\n",
    "dataset_dir  = \"rocole_patches\"\n",
    "\n",
    "# Create (or overwrite) the dataset from a standard directory tree\n",
    "dataset2 = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.ImageClassificationDirectoryTree,\n",
    "    name=dataset_name,\n",
    "    label_field=\"detections\",   # the field where labels will go\n",
    ")\n",
    "\n",
    "dataset2.persistent = True  # keep it around between sessions\n",
    "\n",
    "print(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a75411",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset2, port=5152, auto=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39cf636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\"\n",
    ")  # load the CLIP model from the zoo\n",
    "\n",
    "# Compute embeddings for the dataset\n",
    "dataset2.compute_embeddings(\n",
    "    model=model, embeddings_field=\"clip_embeddings\", batch_size=64\n",
    ")\n",
    "\n",
    "# Dimensionality reduction using UMAP on the embeddings\n",
    "fob.compute_visualization(\n",
    "    dataset2, embeddings=\"clip_embeddings\", method=\"umap\", brain_key=\"clip_vis_2\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012d9be0",
   "metadata": {},
   "source": [
    "## Generating Spatial Heatmaps with the NVLabs C-RADIO Model\n",
    "\n",
    "To further localize and visualize potential anomaly regions within each patch, we‚Äôll use the **C-RADIO V3-H** model developed by NVIDIA Research (NVLabs).  \n",
    "This model produces **spatial heatmaps** that highlight regions of high anomaly probability, serving as an interpretable bridge between raw pixel space and model decision space.\n",
    "\n",
    "### Steps performed in this cell:\n",
    "\n",
    "1. **Register the model source**  \n",
    "   The C-RADIO implementation is hosted on GitHub, so we register its repository as a custom FiftyOne Model Zoo source.\n",
    "\n",
    "2. **Load the pre-trained model**  \n",
    "   Using `foz.load_zoo_model()`, we load the `\"nv_labs/c-radio_v3-h\"` model with spatial output enabled.  \n",
    "   - `output_type=\"spatial\"` ensures pixel-level anomaly scores.  \n",
    "   - `apply_smoothing=True` and `smoothing_sigma=0.51` slightly blur the heatmap to reduce noise.  \n",
    "   - `feature_format=\"NCHW\"` aligns tensor shape with model expectations.\n",
    "\n",
    "3. **Apply the model**  \n",
    "   The model is applied to all samples in `dataset2`, storing results in a new field called `\"radio_heatmap\"`.  \n",
    "   Each sample now includes a **spatial anomaly map** indicating regions where the model detects deviations from normal patterns.\n",
    "\n",
    "These heatmaps are crucial for our next steps, where we‚Äôll combine them with geometric priors (like center keypoints or bounding boxes) to guide region segmentation using SAM2 and refine anomaly localization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaed70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "# Register the RADIO model source (run this once per environment)\n",
    "foz.register_zoo_model_source(\n",
    "    \"https://github.com/harpreetsahota204/NVLabs_CRADIOV3\",\n",
    ")\n",
    "# Load a RADIO model with spatial output\n",
    "spatial_model = foz.load_zoo_model(\n",
    "    \"nv_labs/c-radio_v3-h\",\n",
    "    output_type=\"spatial\",\n",
    "    apply_smoothing=True,      # Optional: smooth the heatmap\n",
    "    smoothing_sigma=0.51,      # Smoothing strength\n",
    "    feature_format=\"NCHW\"\n",
    ")\n",
    "\n",
    "# Apply the model to generate heatmaps\n",
    "dataset2.apply_model(spatial_model, \"radio_heatmap\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acd496",
   "metadata": {},
   "source": [
    "## Running PaDiM Inference and Writing Results to FiftyOne\n",
    "\n",
    "This function defines a complete inference pipeline that integrates **Anomalib‚Äôs PaDiM model** with **FiftyOne datasets**.  \n",
    "It handles model loading, prediction, tensor-to-array conversion, and structured field writing ‚Äî all in a single, reusable function.\n",
    "\n",
    "### Overview\n",
    "\n",
    "The main function, `run_inference()`, performs the following steps:\n",
    "\n",
    "1. **Model Resolution**\n",
    "   - Dynamically imports the correct `Padim` class across multiple Anomalib versions.\n",
    "   - Loads the trained model from a specified checkpoint (`.ckpt`).\n",
    "\n",
    "2. **Per-sample Inference**\n",
    "   - Iterates through all samples in a FiftyOne dataset or view (`sample_collection`).\n",
    "   - Runs inference on each image using `engine.predict()`.\n",
    "   - Collects both scalar and spatial predictions from Anomalib:\n",
    "     - `pred_score`: anomaly confidence score.  \n",
    "     - `pred_label`: binary anomaly flag (0 = normal, 1 = anomaly).  \n",
    "     - `anomaly_map`: pixel-wise heatmap showing anomalous intensity.  \n",
    "     - `pred_mask`: optional segmentation mask for localized defects.\n",
    "\n",
    "3. **Automatic Field Writing**\n",
    "   - Saves the following fields to each FiftyOne sample:\n",
    "     - `pred_anomaly_score_<key>` ‚Üí anomaly confidence (float).  \n",
    "     - `pred_anomaly_<key>` ‚Üí classification label (‚Äúnormal‚Äù or ‚Äúanomaly‚Äù).  \n",
    "     - `pred_anomaly_map_<key>` ‚Üí spatial heatmap (as `fo.Heatmap`).  \n",
    "     - `pred_defect_mask_<key>` ‚Üí segmentation mask (as `fo.Segmentation`).  \n",
    "\n",
    "4. **Helper Utilities**\n",
    "   - `_resolve_padim_cls()` ensures compatibility with all Anomalib versions.  \n",
    "   - `_to_numpy2d()` converts tensors and nested dataclasses to clean 2D NumPy arrays.  \n",
    "   - `_to_float()` safely converts torch scalars to Python floats.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "from anomalib.engine import Engine\n",
    "engine = Engine(accelerator=\"gpu\", devices=1)\n",
    "\n",
    "dataset2 = fo.load_dataset(\"coffee_rocole_original_patches2\")\n",
    "ckpt = \"/home/paula/projects/databricks/results/Padim/rocole/latest/weights/lightning/model.ckpt\"\n",
    "\n",
    "run_inference(engine, dataset2, ckpt, key=\"padim\", threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- requirements: anomalib, torch, fiftyone ---\n",
    "# Assumes you already created `engine = Engine()` (or similar) elsewhere.\n",
    "\n",
    "import importlib\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "\n",
    "def _resolve_padim_cls():\n",
    "    \"\"\"\n",
    "    Resolve the Padim class across multiple anomalib versions.\n",
    "    \"\"\"\n",
    "    candidates = (\n",
    "        \"anomalib.models.image.padim.lightning_model.Padim\",  # newer\n",
    "        \"anomalib.models.image.padim.Padim\",                  # alt re-export\n",
    "        \"anomalib.models.padim.lightning_model.Padim\",        # older\n",
    "        \"anomalib.models.padim.Padim\",                        # older\n",
    "        \"anomalib.models.Padim\",                              # very old\n",
    "    )\n",
    "    last_err = None\n",
    "    for path in candidates:\n",
    "        try:\n",
    "            m, c = path.rsplit(\".\", 1)\n",
    "            return getattr(importlib.import_module(m), c)\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "    raise ModuleNotFoundError(f\"Could not import Padim from known paths. Last error: {last_err}\")\n",
    "\n",
    "def _as_numpy(x):\n",
    "    \"\"\"Try the common, non-recursive ways to get a numpy array.\"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "\n",
    "    # Already numpy\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x\n",
    "\n",
    "    # torch.Tensor -> numpy\n",
    "    try:\n",
    "        import torch  # noqa: F401\n",
    "        if hasattr(x, \"detach\") and hasattr(x, \"cpu\") and hasattr(x, \"numpy\"):\n",
    "            return x.detach().cpu().float().numpy()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # x.numpy() or x.to_numpy()\n",
    "    for m in (\"numpy\", \"to_numpy\", \"to_ndarray\", \"__array__\"):\n",
    "        try:\n",
    "            meth = getattr(x, m, None)\n",
    "            if callable(meth):\n",
    "                arr = meth()\n",
    "                if isinstance(arr, np.ndarray):\n",
    "                    return arr\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    # Fallback: try np.asarray\n",
    "    try:\n",
    "        return np.asarray(x)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def _to_numpy2d(x, *, dtype=None, normalize=False, _max_depth=5):\n",
    "    \"\"\"\n",
    "    Convert various objects (torch.Tensor, PIL, Anomalib wrappers) into a 2D NumPy array.\n",
    "    - For heatmaps: use dtype=np.float32 and normalize=True (scales to [0, 1])\n",
    "    - For masks:    use dtype=np.int32   and normalize=False\n",
    "    Returns None if conversion is not possible or not 2D.\n",
    "    \"\"\"\n",
    "    if x is None:\n",
    "        return None\n",
    "\n",
    "    # First, try direct conversions (safe, non-recursive)\n",
    "    arr = _as_numpy(x)\n",
    "\n",
    "    # If still not numpy, carefully unwrap a few whitelisted attributes with cycle protection\n",
    "    if not isinstance(arr, np.ndarray):\n",
    "        visited = set()\n",
    "        def _safe_peel(obj, depth=0):\n",
    "            if obj is None or depth > _max_depth:\n",
    "                return None\n",
    "            oid = id(obj)\n",
    "            if oid in visited:\n",
    "                return None\n",
    "            visited.add(oid)\n",
    "\n",
    "            # Try converting this object directly\n",
    "            arr_local = _as_numpy(obj)\n",
    "            if isinstance(arr_local, np.ndarray):\n",
    "                return arr_local\n",
    "\n",
    "            # Only inspect a small, safe set of attributes that commonly hold the array/tensor\n",
    "            for attr in (\"anomaly_map\", \"heatmap\", \"mask\", \"data\", \"image\", \"value\", \"array\", \"tensor\"):\n",
    "                try:\n",
    "                    if hasattr(obj, attr):\n",
    "                        child = getattr(obj, attr)\n",
    "                        if child is obj:\n",
    "                            continue\n",
    "                        out = _safe_peel(child, depth + 1)\n",
    "                        if isinstance(out, np.ndarray):\n",
    "                            return out\n",
    "                except Exception:\n",
    "                    # avoid properties that raise internally\n",
    "                    continue\n",
    "            return None\n",
    "\n",
    "        arr = _safe_peel(x)\n",
    "\n",
    "    if arr is None:\n",
    "        return None\n",
    "\n",
    "    # Squeeze stray dims\n",
    "    arr = np.squeeze(arr)\n",
    "\n",
    "    # If 3D, collapse a singleton or pick the first channel\n",
    "    if arr.ndim == 3:\n",
    "        if arr.shape[0] in (1, 3):  # (C, H, W) with C=1 or 3\n",
    "            arr = arr[0]\n",
    "        else:                       # (H, W, 1)\n",
    "            arr = arr[..., 0]\n",
    "\n",
    "    if arr.ndim != 2:\n",
    "        return None\n",
    "\n",
    "    if dtype is not None:\n",
    "        arr = arr.astype(dtype, copy=False)\n",
    "\n",
    "    if normalize:\n",
    "        x_min, x_max = float(np.min(arr)), float(np.max(arr))\n",
    "        rng = (x_max - x_min) or 1.0\n",
    "        arr = (arr - x_min) / rng\n",
    "        arr = arr.astype(np.float32, copy=False)\n",
    "\n",
    "    return arr\n",
    "\n",
    "def _to_float(x):\n",
    "    return float(x.detach().cpu()) if hasattr(x, \"detach\") else float(x)\n",
    "\n",
    "# ---------------------------\n",
    "# Main function\n",
    "# ---------------------------\n",
    "\n",
    "def run_inference(engine, sample_collection, ckpt_path, key=\"padim\", threshold=0.5):\n",
    "    \"\"\"\n",
    "    Runs PaDiM inference via Anomalib Engine over a FiftyOne sample collection.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    engine : anomalib.engine.Engine\n",
    "        An initialized Anomalib Engine instance.\n",
    "    sample_collection : fiftyone.core.collections.SampleCollection\n",
    "        Any FiftyOne sample collection (Dataset or View).\n",
    "    ckpt_path : str\n",
    "        Path to a PaDiM Lightning checkpoint file (.ckpt).\n",
    "    key : str, default \"padim\"\n",
    "        Suffix used for field names written to each sample.\n",
    "    threshold : float, default 0.5\n",
    "        Score threshold at/above which samples are labeled \"anomaly\".\n",
    "\n",
    "    Writes per-sample fields:\n",
    "        - pred_anomaly_score_<key> : float\n",
    "        - pred_anomaly_<key>       : fo.Classification(\"normal\"/\"anomaly\")\n",
    "        - pred_anomaly_map_<key>   : fo.Heatmap (float32 [0,1])  [if available]\n",
    "        - pred_defect_mask_<key>   : fo.Segmentation (int32)     [if available]\n",
    "    \"\"\"\n",
    "    # 1) Load the Anomalib model object from your checkpoint (do this once)\n",
    "    Padim = _resolve_padim_cls()\n",
    "    padim_model = Padim.load_from_checkpoint(ckpt_path)\n",
    "\n",
    "    # 2) Iterate samples and call predict(), passing the model object each time\n",
    "    for sample in sample_collection.iter_samples(autosave=True, progress=True):\n",
    "        preds = engine.predict(\n",
    "            model=padim_model,            # Anomalib model object (has .name)\n",
    "            data_path=sample.filepath,    # Single image path is OK\n",
    "            return_predictions=True,\n",
    "        )\n",
    "        actual = preds[0]                 # anomalib.data.dataclasses.torch.image.ImageBatch\n",
    "\n",
    "        # Scalar score + label\n",
    "        score = _to_float(actual.pred_score)\n",
    "        label = \"anomaly\" if score >= threshold else \"normal\"\n",
    "\n",
    "        # Write simple fields\n",
    "        sample[f\"pred_anomaly_score_{key}\"] = score\n",
    "        sample[f\"pred_anomaly_{key}\"] = fo.Classification(label=label)\n",
    "\n",
    "        # Heatmap (float32 in [0,1])\n",
    "        amap_np = _to_numpy2d(getattr(actual, \"anomaly_map\", None),\n",
    "                              dtype=np.float32, normalize=True)\n",
    "        if amap_np is not None:\n",
    "            sample[f\"pred_anomaly_map_{key}\"] = fo.Heatmap(map=amap_np)\n",
    "\n",
    "        # Segmentation mask (int32)\n",
    "        pmask_np = _to_numpy2d(getattr(actual, \"pred_mask\", None),\n",
    "                               dtype=np.int32, normalize=False)\n",
    "        if pmask_np is not None:\n",
    "            sample[f\"pred_defect_mask_{key}\"] = fo.Segmentation(mask=pmask_np)\n",
    "\n",
    "# ---------------------------\n",
    "# Example usage\n",
    "# ---------------------------\n",
    "# from anomalib.engine import Engine\n",
    "# engine = Engine()  # configure as needed\n",
    "# dataset2 = fo.load_dataset(\"your_dataset_name\")  # or your existing collection\n",
    "# ckpt = \"/home/paula/projects/databricks/results/Padim/rocole/latest/weights/lightning/model.ckpt\"\n",
    "# run_inference(engine, dataset2, ckpt, key=\"padim\", threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94df990e",
   "metadata": {},
   "source": [
    "## Inspecting PaDiM Predictions on a Single Sample\n",
    "\n",
    "Before running batch-level evaluation, it‚Äôs useful to explore the output of the **PaDiM model** on an individual patch.  \n",
    "This helps verify the prediction structure, inspect raw outputs, and understand which attributes are available for downstream visualization.\n",
    "\n",
    "### What this cell does:\n",
    "\n",
    "1. **Select a test image**  \n",
    "   We take the first image from `dataset2` and load it with Pillow (`PIL.Image.open()`).\n",
    "\n",
    "2. **Run inference**  \n",
    "   Using `engine.predict()`, we pass the image through the trained PaDiM model.  \n",
    "   Setting `return_predictions=True` returns a detailed prediction object rather than writing directly to disk.\n",
    "\n",
    "3. **Inspect the returned data**\n",
    "   - Print the overall type of the `preds` object and its first element.  \n",
    "   - Explore its attributes and internal fields (e.g., `image_path`, `pred_score`, `pred_label`, `anomaly_map`, `pred_mask`).  \n",
    "   - Convert tensor-based fields to native Python types for readability.\n",
    "\n",
    "4. **Understand the model outputs**\n",
    "   - `pred_score`: Anomaly confidence score ‚Äî higher values indicate greater deviation from normality.  \n",
    "   - `pred_label`: Binary classification (0 = *normal*, 1 = *anomalous*).  \n",
    "   - `anomaly_map`: Pixel-wise anomaly intensity (if available).  \n",
    "   - `pred_mask`: Segmentation mask of anomalous regions (if produced by the model).\n",
    "\n",
    "This inspection helps confirm that the trained PaDiM model is returning the expected outputs and provides a foundation for visualizing heatmaps and integrating predictions into FiftyOne.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a593f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "## get the first sample from the test split\n",
    "test_image = Image.open(dataset2.first().filepath)\n",
    "Padim = _resolve_padim_cls()\n",
    "padim_model = Padim.load_from_checkpoint(\"results/Padim/rocole/latest/weights/lightning/model.ckpt\")\n",
    "\n",
    "\n",
    "preds = engine.predict(\n",
    "    model=padim_model,                      # optional if using ckpt/config  # \"best\", \"last\", or a path\n",
    "    data_path=dataset2.first().filepath,   # single file is OK\n",
    "    return_predictions=True\n",
    ")\n",
    "\n",
    "print(preds)\n",
    "# 1. See type\n",
    "print(type(preds))\n",
    "\n",
    "# 2. If it's a list, check the first element\n",
    "print(type(preds[0]))\n",
    "\n",
    "# 3. Look at available attributes/keys\n",
    "print(dir(preds[0]))          # if it's an object\n",
    "\n",
    "# 4. Pretty-print contents\n",
    "import pprint\n",
    "pprint.pprint(preds[0])\n",
    "\n",
    "sample = preds[0]  # anomalib.data.dataclasses.torch.image.ImageBatch\n",
    "\n",
    "# --- quick glance at what's inside ---\n",
    "print(\"available fields:\", list(sample.__dataclass_fields__.keys()))\n",
    "\n",
    "# --- the most useful attributes ---\n",
    "print(\"path:\", sample.image_path)          # str (path to the image)\n",
    "print(\"pred_score:\", sample.pred_score)     # float / tensor scalar\n",
    "print(\"pred_label:\", sample.pred_label)     # 0 (normal) or 1 (anomalous)\n",
    "\n",
    "# Pixel-wise outputs (may be None depending on model/config)\n",
    "print(\"pred_mask is None?\", sample.pred_mask is None)   # segmentation-style mask\n",
    "print(\"anomaly_map is None?\", sample.anomaly_map is None)  # raw heatmap\n",
    "\n",
    "# --- convert to Python scalars if tensors ---\n",
    "to_float = (lambda x: float(x.detach().cpu()) if hasattr(x, \"detach\") else float(x))\n",
    "print(\"pred_score (float):\", to_float(sample.pred_score))\n",
    "print(\"pred_label (int):\", int(to_float(sample.pred_label)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624ede7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_inference(engine=engine, sample_collection=dataset2, ckpt_path= \"results/Padim/rocole/latest/weights/lightning/model.ckpt\", key=\"padim\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa49148",
   "metadata": {},
   "source": [
    "## Evaluating PaDiM Classification Performance  \n",
    "\n",
    "With PaDiM predictions now in the dataset, we can **evaluate classification performance** against ground truth labels while aligning label names for consistency.\n",
    "\n",
    "**Steps:**\n",
    "1. **Setup:**  \n",
    "   - `GT_FIELD = \"detections\"` ‚Üí true labels (`normal`, `anormal`)  \n",
    "   - `PRED_FIELD = \"pred_anomaly_padim\"` ‚Üí model predictions  \n",
    "   - `EVAL_KEY = \"padim_eval\"` ‚Üí stores evaluation results  \n",
    "\n",
    "2. **Normalize labels:**  \n",
    "   ```python\n",
    "   label_map = {\"anomaly\": \"anormal\"}\n",
    "   mapped_view = dataset2.map_labels(PRED_FIELD, label_map)\n",
    "   ```\n",
    "3. **Run binary evaluation:**\n",
    "   ```python\n",
    "   results = mapped_view.evaluate_classifications(\n",
    "    PRED_FIELD,\n",
    "    gt_field=GT_FIELD,\n",
    "    eval_key=EVAL_KEY,\n",
    "    method=\"binary\",\n",
    "    classes=[\"normal\", \"anormal\"],)\n",
    "    results.print_report()\n",
    "    ```\n",
    "This computes metrics like accuracy, precision, recall, F1-score, and confusion matrix, viewable in the FiftyOne App or programmatically. It quantifies how well PaDiM distinguishes between healthy and anomalous leaf patches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636973c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_FIELD   = \"detections\"\n",
    "PRED_FIELD = \"pred_anomaly_padim\"\n",
    "EVAL_KEY   = \"padim_eval\"\n",
    "\n",
    "# 1) Normalize prediction labels using map_labels\n",
    "label_map = {\"anomaly\": \"anormal\"}\n",
    "mapped_view = dataset2.map_labels(PRED_FIELD, label_map)\n",
    "\n",
    "# 2) Evaluate as binary (positive = \"anormal\")\n",
    "results = mapped_view.evaluate_classifications(\n",
    "    PRED_FIELD,\n",
    "    gt_field=GT_FIELD,\n",
    "    eval_key=EVAL_KEY,\n",
    "    method=\"binary\",\n",
    "    classes=[\"normal\", \"anormal\"],\n",
    ")\n",
    "\n",
    "results.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4915cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df7f5ed",
   "metadata": {},
   "source": [
    "## Leaf-Only Masking with SAM 2 (Center Seed Prompt)\n",
    "\n",
    "To prevent **false anomalies outside the leaf**, we use SAM 2 to generate a **leaf mask** per image and later restrict anomaly maps to this region.  \n",
    "Here we prompt SAM 2 with a **single keypoint at the image center**, asking it to segment the leaf around that point.\n",
    "\n",
    "### What this cell does\n",
    "1. **Create a center keypoint per sample** (`center_seed`)  \n",
    "   - We add a `Keypoints` label with one point at the center of each image.\n",
    "2. **Run SAM 2 from the FiftyOne Model Zoo**  \n",
    "   - We apply the `\"segment-anything-2-hiera-tiny-image-torch\"` model, using the keypoint prompt to produce a leaf segmentation.  \n",
    "   - The resulting mask(s) are stored in `sam2_mask`.\n",
    "\n",
    "> **Important notes**\n",
    "> - SAM 2‚Äôs keypoint prompts expect **pixel coordinates**, not normalized.  \n",
    ">   If you created `points=[(0.5, 0.5)]`, convert to `(W//2, H//2)` per image.  \n",
    "> - When prompting with keypoints, use `prompt_field=\"<your_field>_keypoints\"` in `apply_model`.  \n",
    ">   For the field `center_seed`, set `prompt_field=\"center_seed_keypoints\"`.\n",
    "\n",
    "After this step, you can **mask anomaly maps** or **ignore predictions** outside the leaf to reduce background-induced false positives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6fc5300",
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in dataset2.iter_samples(progress=True, autosave=True):\n",
    "    # Center point in relative coordinates\n",
    "    center_x = 0.5\n",
    "    center_y = 0.5\n",
    "\n",
    "    # Create a Keypoints label with one Keypoint at the center\n",
    "    sample[\"center_seed\"] = fo.Keypoints(\n",
    "        keypoints=[\n",
    "            fo.Keypoint(label=\"center_seed\", points=[(center_x, center_y)])\n",
    "        ]\n",
    "    )\n",
    "    # No need to call sample.save() if autosave=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e5e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n",
    "dataset2.apply_model(model, label_field=\"sam2_mask\", prompt_field=\"center_seed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf05bd",
   "metadata": {},
   "source": [
    "## Leaf Masking with SAM 2 (Five Keypoint Prompts)\n",
    "\n",
    "To improve segmentation coverage and reduce **false anomalies at leaf edges**, we expand the SAM 2 prompt from one to **five keypoints** ‚Äî one at the center and four around it (up, down, left, right).  \n",
    "This helps SAM 2 better capture the entire leaf contour, especially when leaves are partially occluded or asymmetrical.\n",
    "\n",
    "### What this cell does\n",
    "1. **Generate five keypoints per image**  \n",
    "   - Defines relative coordinates (0‚Äì1) for the center and four surrounding points (10% offset).  \n",
    "   - Saves them in a new field `center_seed4` as a `Keypoints` label.  \n",
    "\n",
    "2. **Run SAM 2 segmentation**  \n",
    "   - Loads the `\"segment-anything-2-hiera-tiny-image-torch\"` model from the FiftyOne Zoo.  \n",
    "   - Applies the model using the new `center_seed4` keypoints as prompts.  \n",
    "   - Segmentation masks are stored in the `sam2_pred` field.  \n",
    "\n",
    "> **Tip:**  \n",
    "> When prompting SAM 2 with multiple keypoints, keep them evenly distributed across the target region for more stable mask generation.  \n",
    "\n",
    "This step provides **denser and more robust leaf masks**, improving downstream anomaly detection by excluding irrelevant background regions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8855027",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import fiftyone as fo\n",
    "\n",
    "for sample in dataset2.iter_samples(progress=True, autosave=True):\n",
    "    # relative coordinates (0‚Äì1)\n",
    "    cx, cy = 0.5, 0.5\n",
    "    dy = 0.1  # 10% vertical offset\n",
    "    dx = 0.1  # 10% horizontal offset (optional for symmetry)\n",
    "\n",
    "    # center + four around center\n",
    "    points = [\n",
    "        (cx, cy),                # center\n",
    "        (cx, cy - dy),           # up\n",
    "        (cx, cy + dy),           # down\n",
    "        (cx - dx, cy),           # left\n",
    "        (cx + dx, cy),           # right\n",
    "    ]\n",
    "\n",
    "    sample[\"center_seed4\"] = fo.Keypoints(\n",
    "        keypoints=[fo.Keypoint(label=\"center_seed4\", points=points)]\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb97d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = foz.load_zoo_model(\"segment-anything-2-hiera-tiny-image-torch\")\n",
    "dataset2.apply_model(model, label_field=\"sam2_pred\", prompt_field=\"center_seed4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e14319",
   "metadata": {},
   "source": [
    "## Post-Processing: AND SAM2 Mask with PaDiM Mask & Classify by Overlap\n",
    "\n",
    "To reduce **background false positives** and only count anomalies that fall **within the leaf**, this step:\n",
    "1. Builds a **full-image SAM2 leaf mask** from detections (`sam2_pred`).\n",
    "2. Resizes the **PaDiM defect mask** (`pred_defect_mask_padim`) to image size.\n",
    "3. Computes the **logical AND** between both masks to retain only defects **inside** the leaf area.\n",
    "4. Converts the overlap ratio into a **binary classification** (`normal` vs. `anormal`).\n",
    "\n",
    "### Configuration\n",
    "- `SAM_FIELD = \"sam2_pred\"` ‚Äî SAM2 masks (detections with per-detection masks).  \n",
    "- `PADIM_FIELD = \"pred_defect_mask_padim\"` ‚Äî PaDiM segmentation mask.  \n",
    "- `OUT_AND_FIELD = \"mask_and_sam2_padim\"` ‚Äî output segmentation (AND result).  \n",
    "- `OUT_CLS_FIELD = \"pred_by_overlap\"` ‚Äî output classification based on overlap.  \n",
    "- `THRESH = 0.80` ‚Äî minimum **coverage** of PaDiM mask by SAM2 (AND/PaDiM area) to call it **anormal**.  \n",
    "- `OUT_DIR = \"/tmp/masks_and\"` ‚Äî folder to export the AND masks as PNGs.\n",
    "\n",
    "### What this cell does\n",
    "1. **Reconstructs SAM2 full-image mask**  \n",
    "   - Initializes an empty `H√óW` boolean mask.  \n",
    "   - For each detection in `sam2_pred`, grabs `det.get_mask()` (which is relative to the detection‚Äôs bounding box), **resizes it** to the box size, and **pastes** it back into the full-image coordinates.\n",
    "\n",
    "2. **Normalizes PaDiM mask shape**  \n",
    "   - Reads the PaDiM mask, binarizes it, and **resizes** (nearest neighbor) to `(H, W)` if needed to match image size.\n",
    "\n",
    "3. **Combines masks & saves output**  \n",
    "   - Computes `and_mask = sam_mask & padim_mask`.  \n",
    "   - Saves `and_mask` via `fo.Segmentation(...).export_mask()` to `OUT_DIR`.  \n",
    "   - Stores the segmentation in `mask_and_sam2_padim`.\n",
    "\n",
    "4. **Computes coverage & classifies**  \n",
    "   - `coverage = intersection(and_mask) / area(padim_mask)` (if PaDiM area > 0).  \n",
    "   - If `coverage >= 0.80` ‚Üí `anormal`, else `normal`.  \n",
    "   - Writes a `Classification(label, confidence=coverage)` into `pred_by_overlap`.\n",
    "\n",
    "### Why this helps\n",
    "- Ensures anomalies are only counted **within the leaf** region predicted by SAM2.  \n",
    "- Drops spurious detections on **background** or **borders**, improving precision.\n",
    "\n",
    "### Notes & caveats\n",
    "- Use **nearest-neighbor** resizing for binary masks (already set via `order=0`) to avoid soft edges.  \n",
    "- If SAM2 misses parts of the leaf, coverage may be underestimated (potential false negatives).  \n",
    "- If PaDiM area is tiny/noisy, a high threshold (0.80) can be strict; consider tuning per dataset.  \n",
    "- The SAM2 detection mask is in **box-local coordinates**; this code correctly pastes it back using the detection‚Äôs bounding box.\n",
    "\n",
    "### Next steps (optional)\n",
    "- Evaluate `pred_by_overlap` against ground truth (e.g., `detections`) with:\n",
    "  ```python\n",
    "  ds.evaluate_classifications(\n",
    "      \"pred_by_overlap\",\n",
    "      gt_field=\"detections\",\n",
    "      eval_key=\"overlap_eval\",\n",
    "      method=\"binary\",\n",
    "      classes=[\"normal\", \"anormal\"],\n",
    "  ).print_report()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6762144c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import fiftyone as fo\n",
    "from skimage.transform import resize\n",
    "\n",
    "# ==== CONFIG ====\n",
    "ds = dataset2\n",
    "SAM_FIELD   = \"sam2_pred\"\n",
    "PADIM_FIELD = \"pred_defect_mask_padim\"\n",
    "OUT_AND_FIELD = \"mask_and_sam2_padim\"\n",
    "OUT_CLS_FIELD = \"pred_by_overlap\"\n",
    "THRESH = 0.80\n",
    "OUT_DIR = \"/tmp/masks_and\"\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "ds.compute_metadata()\n",
    "\n",
    "for s in ds.iter_samples(progress=True, autosave=True):\n",
    "    sam_dets = s[SAM_FIELD] if SAM_FIELD in s else None\n",
    "    padim_seg = s[PADIM_FIELD] if PADIM_FIELD in s else None\n",
    "    \n",
    "    if sam_dets is None or padim_seg is None:\n",
    "        continue\n",
    "    \n",
    "    if not sam_dets.detections:\n",
    "        continue\n",
    "    \n",
    "    # Get image dimensions\n",
    "    H, W = s.metadata.height, s.metadata.width\n",
    "    \n",
    "    # Create full-size SAM mask\n",
    "    sam_mask = np.zeros((H, W), dtype=bool)\n",
    "    for det in sam_dets.detections:\n",
    "        det_mask = det.get_mask()\n",
    "        if det_mask is not None:\n",
    "            bbox = det.bounding_box\n",
    "            x, y, w, h = bbox\n",
    "            x0, y0 = int(x * W), int(y * H)\n",
    "            x1, y1 = int((x + w) * W), int((y + h) * H)\n",
    "            \n",
    "            det_mask_resized = resize(\n",
    "                det_mask.astype(np.uint8),\n",
    "                (y1 - y0, x1 - x0),\n",
    "                order=0,\n",
    "                preserve_range=True,\n",
    "                anti_aliasing=False\n",
    "            ).astype(bool)\n",
    "            sam_mask[y0:y1, x0:x1] |= det_mask_resized\n",
    "    \n",
    "    # Get PaDiM mask\n",
    "    padim_mask = padim_seg.get_mask()\n",
    "    if padim_mask is None:\n",
    "        continue\n",
    "    \n",
    "    padim_mask = (padim_mask > 0).astype(bool)\n",
    "    \n",
    "    # **RESIZE PaDiM mask to match image dimensions**\n",
    "    if padim_mask.shape != (H, W):\n",
    "        padim_mask = resize(\n",
    "            padim_mask.astype(np.uint8),\n",
    "            (H, W),\n",
    "            order=0,  # nearest neighbor for binary masks\n",
    "            preserve_range=True,\n",
    "            anti_aliasing=False\n",
    "        ).astype(bool)\n",
    "    \n",
    "    # AND operation (now both masks are same size)\n",
    "    and_mask = sam_mask & padim_mask\n",
    "    \n",
    "    # Save using export_mask()\n",
    "    out_path = os.path.join(OUT_DIR, f\"{s.id}_and.png\")\n",
    "    and_seg = fo.Segmentation(mask=and_mask.astype(np.uint8) * 255)\n",
    "    and_seg.export_mask(out_path, update=True)\n",
    "    s[OUT_AND_FIELD] = and_seg\n",
    "    \n",
    "    # Calculate overlap\n",
    "    padim_area = np.count_nonzero(padim_mask)\n",
    "    inter_area = np.count_nonzero(and_mask)\n",
    "    cov = (inter_area / padim_area) if padim_area > 0 else 0.0\n",
    "    \n",
    "    # Classification\n",
    "    label = \"anormal\" if cov >= THRESH else \"normal\"\n",
    "    s[OUT_CLS_FIELD] = fo.Classification(label=label, confidence=float(cov))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f01ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea67d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Evaluate as binary (positive = \"anormal\")\n",
    "results2 = mapped_view.evaluate_classifications(\n",
    "    OUT_CLS_FIELD,\n",
    "    gt_field=\"detections\",\n",
    "    eval_key=\"sam2_padim_eval\",\n",
    "    method=\"binary\",\n",
    "    classes=[\"normal\", \"anormal\"],\n",
    ")\n",
    "\n",
    "results2.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset2.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d61fe",
   "metadata": {},
   "source": [
    "## Generating Tiled Image Datasets with the FiftyOne Tile Plugin\n",
    "\n",
    "This section demonstrates how to use the **`fiftyone-tile` plugin** by [@mmoollllee](https://github.com/mmoollllee/fiftyone-tile) to split large images into smaller, fixed-size tiles.  \n",
    "This approach is particularly helpful for training, visual inspection, and patch-level anomaly detection.\n",
    "\n",
    "### 1. Install and verify the plugin\n",
    "We first download the plugin directly from GitHub and confirm it has been correctly loaded into FiftyOne.\n",
    "\n",
    "```python\n",
    "!fiftyone plugins download https://github.com/mmoollllee/fiftyone-tile/\n",
    "\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "# List all available operators\n",
    "operators = foo.list_operators()\n",
    "for op in operators:\n",
    "    print(op.uri)\n",
    "\n",
    "# Verify that the tile operator is available\n",
    "exists = foo.operator_exists(\"@mmoollllee/tile/make_tiles\")\n",
    "print(f\"Operator exists: {exists}\")\n",
    "\n",
    "# Check for any plugin loading errors\n",
    "from fiftyone.operators.registry import OperatorRegistry\n",
    "registry = OperatorRegistry()\n",
    "errors = registry.list_errors()\n",
    "if errors:\n",
    "    print(\"Plugin errors:\")\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "```\n",
    "\n",
    "Note: If the operator does not appear, restart your kernel or run fiftyone plugins list to confirm installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c398aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/mmoollllee/fiftyone-tile/\n",
    "!fiftyone plugins requirements @mmoollllee/tile --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed206d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "# List all available operators\n",
    "try:\n",
    "    operators = foo.list_operators()\n",
    "    print(f\"Found {len(operators)} operators\")\n",
    "    for op in operators:\n",
    "        print(op.uri)\n",
    "except Exception as e:\n",
    "    print(f\"Error listing operators: {e}\")\n",
    "\n",
    "# Check specifically for your operator\n",
    "try:\n",
    "    exists = foo.operator_exists(\"@mmoollllee/tile/make_tiles\")\n",
    "    print(f\"Operator exists: {exists}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error checking operator: {e}\")\n",
    "\n",
    "# Check for any plugin loading errors\n",
    "from fiftyone.operators.registry import OperatorRegistry\n",
    "registry = OperatorRegistry()\n",
    "errors = registry.list_errors()\n",
    "if errors:\n",
    "    print(\"Plugin errors:\")\n",
    "    for error in errors:\n",
    "        print(error)\n",
    "else:\n",
    "    print(\"No plugin loading errors found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9ff436",
   "metadata": {},
   "source": [
    "### 2. Generate tiles from your dataset\n",
    "\n",
    "Once the plugin is verified, we can call the @mmoollllee/tile/make_tiles operator to create a tiled dataset from our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a606f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.operators as foo\n",
    "\n",
    "# Get the operator using its URI\n",
    "make_tiles = foo.get_operator(\"@mmoollllee/tile/make_tiles\")\n",
    "\n",
    "# Execute the operator using the __call__ method\n",
    "make_tiles(\n",
    "    dataset2,\n",
    "    output_dir=\"tiled_images\",\n",
    "    destination=\"tiled_dataset\",\n",
    "    labels_field=None,  # Don't transfer any labels\n",
    "    save_empty=True,    # Keep all tiles\n",
    "    resize=1200,\n",
    "    tile_size=256,\n",
    "    padding=20,\n",
    "    log_level=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9b36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tiled dataset (the destination you specified)\n",
    "tiled_dataset = fo.load_dataset(\"tiled_dataset\")\n",
    "\n",
    "# Launch the FiftyOne App to visualize the tiles\n",
    "session = fo.launch_app(tiled_dataset, port = 5152, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4af8f",
   "metadata": {},
   "source": [
    "# Let's test anomaly detection on a tiled dataset\n",
    " \n",
    "We will use part of a tiled dataset of coffee leaves call [JMuBEN](https://www.kaggle.com/datasets/noamaanabdulazeem/jmuben-coffee-dataset). Just Healthy and Miner files as Nomarl and Anormal data, and we will use Anomalb and PaDiM model for detecting abnormal samples, and evaluate the classification task using Fiftyone. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedb09f4",
   "metadata": {},
   "source": [
    "## Download the tiled dataset from my Google Drive Folder. \n",
    "### üì¶ Importing the RoCoLe Dataset from Google Drive\n",
    "\n",
    "We‚Äôll begin by importing partial **JMuBEN** dataset directly from Google Drive.  \n",
    "This dataset contains images of coffee leaves under **healthy** and **diseased** conditions, captured in natural environments with varying lighting, backgrounds, and scales.\n",
    "\n",
    "Using `gdown`, we‚Äôll download the compressed dataset (`rocole_original.zip`) and extract it locally.  \n",
    "This will make the image data available for exploration and analysis with FiftyOne in the next steps.\n",
    "\n",
    "\n",
    "- [Kaggle dataset](https://www.kaggle.com/datasets/noamaanabdulazeem/jmuben-coffee-dataset)\n",
    "- [Original Dataset](https://data.mendeley.com/datasets/tgv3zb82nd/1)\n",
    "- [Google Drive link](https://drive.google.com/file/d/12qZHphtkr4yJa7dWo0ob6Nl-53G_nIir/view?usp=sharing) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe0d78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "# Download the coffee dataset from Google Drive\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=12qZHphtkr4yJa7dWo0ob6Nl-53G_nIir\"  # original\n",
    "gdown.download(url, output=\"partial_jmuben.zip\", quiet=False)\n",
    "!unzip partial_jmuben.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1212d6c0",
   "metadata": {},
   "source": [
    "## üìÇ Loading the JMuBEN Dataset into FiftyOne\n",
    "\n",
    "Once the dataset is extracted, we‚Äôll load it into FiftyOne to enable interactive exploration and visualization.\n",
    "\n",
    "The dataset follows the **COCO format**, which stores both **bounding box detections** and **segmentation masks** for each leaf image.  \n",
    "We‚Äôll use `fo.Dataset.from_dir()` to create a persistent FiftyOne dataset named `coffee_rocole_coco`.\n",
    "\n",
    "Key parameters:\n",
    "- `dataset_dir`: the local path to the dataset folder.  \n",
    "- `dataset_type`: specifies the annotation format (`ImageClassificationDirectoryTree`).  \n",
    "\n",
    "\n",
    "Once loaded, the dataset will be saved persistently for future sessions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e9147d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"partial_jmuben\"  # change if you like\n",
    "dataset_dir  = \"coffee_leaves\" # I have selected just two Healthy and Miner Folders from Kaggle page\n",
    "\n",
    "# Create (or overwrite) the dataset from a standard directory tree\n",
    "dataset3 = fo.Dataset.from_dir(\n",
    "    dataset_dir=dataset_dir,\n",
    "    dataset_type=fot.ImageClassificationDirectoryTree,\n",
    "    name=dataset_name,\n",
    "    label_field=\"ground_truth\",   # the field where labels will go\n",
    ")\n",
    "\n",
    "dataset3.persistent = True  # keep it around between sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf5e98",
   "metadata": {},
   "source": [
    "### ```create_index()```\n",
    "\n",
    "This method creates a databesa index on specific fields to enable efficient sorting, merging and querying operations. We will apply that to the ```ground_truth.labels``` and later on to the predictions to the anomaly detection model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dd110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3.create_index(\"ground_truth.label\")\n",
    "dataset3.reload()\n",
    "\n",
    "print(dataset3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303b181",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset3, port=5153, auto=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c8643c",
   "metadata": {},
   "source": [
    "### Explore your dataset with the Dashboard Plugin\n",
    "\n",
    "Find the Dashboard Panel in FiftyOne and explore the distribution of your dataset and number of labels per category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f510dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- env hygiene (optional but helpful) ---\n",
    "import os, gc, torch\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "torch.cuda.empty_cache(); gc.collect()\n",
    "\n",
    "# --- Anomalib: datamodule + model + trainer ---\n",
    "from anomalib.data import Folder\n",
    "from anomalib.engine import Engine\n",
    "from anomalib.models.image.padim.lightning_model import Padim  # direct import avoids WinCLIP deps\n",
    "\n",
    "# Datamodule applies a 256x256 resize internally\n",
    "datamodule = Folder(\n",
    "    name=\"coffee_leaves\",\n",
    "    root=\"coffee_leaves\",\n",
    "    normal_dir=\"Healthy\",\n",
    "    abnormal_dir=\"Miner\",\n",
    "    train_batch_size=4,             # adjust for your GPU\n",
    "    eval_batch_size=4,\n",
    "    num_workers=4,\n",
    ")\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "# Lightweight PaDiM (fits easier)\n",
    "model = Padim(\n",
    "    backbone=\"resnet18\",\n",
    "    layers=[\"layer2\", \"layer3\"],    # or [\"layer3\"] if memory is tight\n",
    "    n_features=50,                  # 32‚Äì100; lower uses less memory\n",
    "    pre_trained=True,\n",
    ")\n",
    "\n",
    "engine = Engine(\n",
    "    accelerator=\"gpu\", #\"cpu\" is also an option Padin is CPU trainable\n",
    "    devices=1,\n",
    ")\n",
    "\n",
    "engine.fit(model=model, datamodule=datamodule)\n",
    "# (optional) run validation/test afterwards\n",
    "# datamodule.setup(\"test\"); engine.test(model=model, datamodule=datamodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef9ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_inference(engine=engine, sample_collection=dataset3, ckpt_path= \"results/Padim/coffee_leaves/latest/weights/lightning/model.ckpt\", key=\"padim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce12680",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363d1a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset3.create_index(\"pred_anomaly_padim.label\")\n",
    "dataset3.reload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59cbb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_FIELD   = \"ground_truth\"\n",
    "PRED_FIELD = \"pred_anomaly_padim\"\n",
    "EVAL_KEY   = \"padim_eval\"\n",
    "\n",
    "# 1) Normalize prediction labels using map_labels\n",
    "label_map = {\"Healthy\": \"normal\", \"Miner\": \"anormal\"}\n",
    "mapped_view = dataset3.map_labels(PRED_FIELD, label_map)\n",
    "\n",
    "# 2) Evaluate as binary (positive = \"anormal\")\n",
    "results = mapped_view.evaluate_classifications(\n",
    "    PRED_FIELD,\n",
    "    gt_field=GT_FIELD,\n",
    "    eval_key=EVAL_KEY,\n",
    "    method=\"binary\",\n",
    "    classes=[\"normal\", \"anormal\"],\n",
    ")\n",
    "\n",
    "results.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8876a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paularamos/Documents/GitHub/awesome-fiftyone/coffe_workshop_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are running the oldest supported major version of MongoDB. Please refer to https://deprecation.voxel51.com for deprecation notices. You can suppress this exception by setting your `database_validation` config parameter to `False`. See https://docs.voxel51.com/user_guide/config.html#configuring-a-mongodb-connection for more information\n"
     ]
    }
   ],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "dataset =fo.load_dataset(\"coffee_rocole\")\n",
    "dataset2 = fo.load_dataset(\"coffee_rocole_original_patches\")\n",
    "tiled_dataset = fo.load_dataset(\"tiled_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538929aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/var/folders/6y/g2mslh_s7fz7qtj9vrxntqtm0000gn/T/tmp77b26skg' already exists; export will be merged with existing files\n",
      "Exporting samples...\n",
      " 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9840/9840 [3.3s elapsed, 0s remaining, 3.1K docs/s]        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading media files:   0%|          | 0/1 [00:00<?, ?it/s]It seems you are trying to upload a large folder at once. This might take some time and then fail if the folder is too large. For such cases, it is recommended to upload in smaller batches or to use `HfApi().upload_large_folder(...)`/`hf upload-large-folder` instead. For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/upload#upload-a-large-folder.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"timestamp\":\"2025-10-23T14:26:28.927128Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.928456Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.933986Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.934067Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.936016Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.936939Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.946988Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.948148Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.949646Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.949655Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.953362Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.956056Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.958234Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.958303Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.968051Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.968140Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.971695Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.971765Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.972928Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.978130Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.980247Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.980313Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.987601Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.987687Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.993706Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.993793Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:28.993800Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.000295Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.002500Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.002587Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.007124Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.007197Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.013360Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.016365Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.016641Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:29.154322Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:31.996824Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:33.701909Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:43.423228Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n",
      "{\"timestamp\":\"2025-10-23T14:26:48.294111Z\",\"level\":\"WARN\",\"fields\":{\"message\":\"Status Code: 429. Retrying...\",\"request_id\":\"\"},\"filename\":\"/Users/runner/work/xet-core/xet-core/cas_client/src/http_client.rs\",\"line_number\":236}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading media files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [03:38<00:00, 218.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"t\":{\"$date\":\"2025-10-23T14:20:50.682Z\"},\"s\":\"I\",  \"c\":\"CONTROL\",  \"id\":20697,   \"ctx\":\"-\",\"msg\":\"Renamed existing log file\",\"attr\":{\"oldLogPath\":\"/Users/paularamos/.fiftyone/var/lib/mongo/log/mongo.log\",\"newLogPath\":\"/Users/paularamos/.fiftyone/var/lib/mongo/log/mongo.log.2025-10-23T14-20-50\"}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subprocess ['/Users/paularamos/Documents/GitHub/awesome-fiftyone/coffe_workshop_env/lib/python3.10/site-packages/fiftyone/db/bin/mongod', '--dbpath', '/Users/paularamos/.fiftyone/var/lib/mongo', '--logpath', '/Users/paularamos/.fiftyone/var/lib/mongo/log/mongo.log', '--port', '0', '--nounixsocket'] exited with error -6:\n"
     ]
    }
   ],
   "source": [
    "from fiftyone.utils.huggingface import push_to_hub\n",
    "\n",
    "push_to_hub(tiled_dataset, \"tiled_dataset\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffe_workshop_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
