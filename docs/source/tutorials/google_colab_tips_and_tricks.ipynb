{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2d148e24-9b24-4f0e-ac2a-592f4477b8b2",
      "metadata": {
        "id": "2d148e24-9b24-4f0e-ac2a-592f4477b8b2"
      },
      "source": [
        "# Tips and Tricks for Using FiftyOne inside Google Colab\n",
        "\n",
        "Working with FiftyOne in Google Colab presents unique challenges: temporary instances that lose your data, installation overhead on every restart, and the need to manage datasets across sessions. This notebook shares battle-tested techniques to make your FiftyOne workflow in Colab faster, more reliable, and reproducible.\n",
        "\n",
        "Whether you're curating datasets, running computer vision experiments, or building ML pipelines, these tips will help with your work in the ephemeral Colab environment.\n",
        "\n",
        "## What you will learn\n",
        "\n",
        "- Install FiftyOne using modern tools like `uv`\n",
        "- Persist your datasets, models, and configurations across sessions with Google Drive\n",
        "- Share datasets with collaborators\n",
        "- Leverage GPU acceleration for faster inference\n",
        "- Control the FiftyOne App for a better development experience\n",
        "\n",
        "### So, what's the takeaway?\n",
        "\n",
        "These patterns apply with and beyond FiftyOne. The techniques for managing data persistence, sharing resources, and optimizing compute translate to other ML workflows in Colab.\n",
        "\n",
        "\n",
        "## Setup\n",
        "To follow this tutorial will need a valid Google user account with enough space on Google Drive (about 1 GB).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1c73a8-6f51-460f-90e6-a07802830f3c",
      "metadata": {
        "id": "9b1c73a8-6f51-460f-90e6-a07802830f3c"
      },
      "source": [
        "## Tip 1: Clean and fast FiftyOne installation with `uv` and `%%capture`\n",
        "\n",
        "Using `uv` for package installation and `%%capture` to suppress output are great practices for creating clean and efficient Colab notebooks.\n",
        "\n",
        "In Google Colab, the virtual machine instance you are using is temporary. When you close your browser tab or the notebook becomes idle for too long, the instance is recycled and all installed packages are lost. This is why having a quick way to re-install libraries like FiftyOne is very valuable, and `uv` helps significantly with this.\n",
        "\n",
        "### Why use `uv`?\n",
        "\n",
        "`uv` is a modern, fast Python package installer and resolver. It is designed to be significantly quicker than traditional tools like `conda`. Using `uv` can drastically reduce the time it takes to set up your environment, especially when dealing with many dependencies. This is useful on Colab notebooks, as they need to be re-configured each time that we close the browser window that runs them.\n",
        "\n",
        "### Why use `%%capture`?\n",
        "\n",
        "The `%%capture` magic command is used to suppress the standard output and standard error streams of a cell. When installing packages, the output can often be very verbose. Using `%%capture` keeps your notebook clean and focused on the results of your code, rather than the installation process details.\n",
        "\n",
        "### Explicitly specifying package versions\n",
        "\n",
        "It's always a good practice to make the version of the library you are using explicit in your notebook. This is crucial for reproducibility. Different versions of libraries like FiftyOne can have API changes or different dependencies. By specifying the version, you ensure that anyone running your notebook in the future will use the exact same environment you did, saving time and effort in reproducing your work.\n",
        "\n",
        "Let's look at the code cells in this section:\n",
        "\n",
        "This cell uses `uv` to install the `fiftyone` library with a specific version (`==1.8.0`). The `!` at the beginning indicates a shell command, and `%%capture` suppresses the installation output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e3c0e5a-2c8c-4fcc-ac6f-0046e7f8b919",
      "metadata": {
        "id": "2e3c0e5a-2c8c-4fcc-ac6f-0046e7f8b919"
      },
      "outputs": [],
      "source": [
        "# We use %%capture to avoid polluting the notebook with the install trace\n",
        "%%capture\n",
        "!uv pip install fiftyone==1.8.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8b4447-b82b-4265-a864-46c593259b32",
      "metadata": {
        "id": "1d8b4447-b82b-4265-a864-46c593259b32"
      },
      "source": [
        "This cell imports the FiftyOne library, aliasing it as `fo` for easier use in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764d3e51-c36b-4886-9878-5776d5483a91",
      "metadata": {
        "id": "764d3e51-c36b-4886-9878-5776d5483a91"
      },
      "outputs": [],
      "source": [
        "import fiftyone as fo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d04968cd-7058-4775-8167-270f9bd36474",
      "metadata": {
        "id": "d04968cd-7058-4775-8167-270f9bd36474"
      },
      "source": [
        "This cell prints the installed version of FiftyOne to confirm that the correct version was installed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31ed28a7-743a-4467-b508-3a1378f8c7de",
      "metadata": {
        "id": "31ed28a7-743a-4467-b508-3a1378f8c7de"
      },
      "outputs": [],
      "source": [
        "print(f\"FiftyOne version installed: {fo.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5f0962b-7b0d-4da4-8ac9-8b89e13d9419",
      "metadata": {
        "id": "f5f0962b-7b0d-4da4-8ac9-8b89e13d9419"
      },
      "source": [
        "## Tip 2: use Google Drive for persistent storage of data and models across sessions\n",
        "\n",
        "This is an important workflow for any serious work in Colab. It ensures that your data, models, and datasets are not lost when the virtual machine instance is recycled.\n",
        "\n",
        "Colab instances are typically recycled after a period of inactivity (usually around 90 minutes) or when you close your browser tab. Instances also have a maximum lifetime (currently 12 hours), after which they are also recycled. This means any data and work not saved to persistent storage will be lost.\n",
        "\n",
        "We will use our Google Drive account as persistent storage so that we can save our work. It's important to know that if we share the notebook with collaborators or the outside world, they will only be able to access the data in our Google Drive to which we have granted them open access, we will explore this soon."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d67c519c-07f9-46c7-9523-c3e034907921",
      "metadata": {
        "id": "d67c519c-07f9-46c7-9523-c3e034907921"
      },
      "outputs": [],
      "source": [
        "# You will be asked to authorize access to Drive after running this.\n",
        "# Note that this connects to your own Google Drive account.\n",
        "# It doesn't connect to folders from others unless they have been shared with you.\n",
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3139389c-0b82-4fcf-b676-e13d4206584c",
      "metadata": {
        "id": "3139389c-0b82-4fcf-b676-e13d4206584c"
      },
      "source": [
        "Export and import your persisted data using your Google Drive path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "766a5a92-f047-4952-b88d-e0dfc833bb9b",
      "metadata": {
        "id": "766a5a92-f047-4952-b88d-e0dfc833bb9b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "# Notice that this is a Google Drive path\n",
        "save_path = Path('/gdrive/MyDrive/fiftyone_dataset_curation')\n",
        "os.makedirs(save_path, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b833215-ed16-43b6-ae3a-b857502787e9",
      "metadata": {
        "id": "7b833215-ed16-43b6-ae3a-b857502787e9"
      },
      "source": [
        "We specify the location of our MongoDB database for FiftyOne in the folder that we have chosen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a91cd8b3-3a11-479c-9c90-b1ff889028df",
      "metadata": {
        "id": "a91cd8b3-3a11-479c-9c90-b1ff889028df"
      },
      "outputs": [],
      "source": [
        "# path to the MongoDB database\n",
        "database_path = save_path / \"mongodb\"\n",
        "os.makedirs(database_path, exist_ok=True)\n",
        "fo.config.database_dir = str(database_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "893d5673-cdb0-40e1-ad14-ed4e287b3558",
      "metadata": {
        "id": "893d5673-cdb0-40e1-ad14-ed4e287b3558"
      },
      "source": [
        "Let's demo this with a sample from the COCO-2017 dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81f215d8-6b21-4d37-af31-698f12a3d02a",
      "metadata": {
        "id": "81f215d8-6b21-4d37-af31-698f12a3d02a"
      },
      "outputs": [],
      "source": [
        "import fiftyone.zoo as foz\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"coco-2017\",\n",
        "    split=\"validation\",\n",
        "    max_samples=200,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3e944cd-9f79-4da3-b430-c3d325785a97",
      "metadata": {
        "id": "b3e944cd-9f79-4da3-b430-c3d325785a97"
      },
      "source": [
        "Export the dataset to a Google Drive path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1a7ab45-b467-4e92-a9b0-9b0d3d5ac244",
      "metadata": {
        "id": "b1a7ab45-b467-4e92-a9b0-9b0d3d5ac244"
      },
      "outputs": [],
      "source": [
        "# Export the dataset to a folder\n",
        "export_dir = save_path / \"coco-2017\"\n",
        "dataset.export(\n",
        "    export_dir=str(export_dir),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "     # set to True to export the original images alongside annotations\n",
        "    export_media=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e9b109e-1e9a-4c2f-bcfb-489956461c28",
      "metadata": {
        "id": "3e9b109e-1e9a-4c2f-bcfb-489956461c28"
      },
      "source": [
        "We can later reload the data using the same path."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c76f6de1-cd7d-4467-ad5f-d22567df4d2d",
      "metadata": {
        "id": "c76f6de1-cd7d-4467-ad5f-d22567df4d2d"
      },
      "outputs": [],
      "source": [
        "reloaded_dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=export_dir,\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FzVR7Jfdq5_q",
      "metadata": {
        "id": "FzVR7Jfdq5_q"
      },
      "source": [
        "\n",
        "## Tip 3: use `gdown` to download shared zipped folders\n",
        "\n",
        "When a dataset is shared as a Google Drive folder, using a tool like `gdown` is more convenient than manually downloading and re-uploading the data to your Colab environment. `gdown` is a command-line utility that allows you to download files and folders directly from a public Google Drive link.\n",
        "\n",
        "This approach is particularly useful because it automates the download process and can be included directly in your notebook, making your workflow fully reproducible.\n",
        "\n",
        "First, let's install `gdown` using `uv`, keeping the notebook clean with `%%capture`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rlf8x3z9q-AE",
      "metadata": {
        "id": "rlf8x3z9q-AE"
      },
      "outputs": [],
      "source": [
        "# We use %%capture to avoid polluting the notebook with the install trace\n",
        "%%capture\n",
        "!uv pip install gdown==5.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wOR534Y-rCB7",
      "metadata": {
        "id": "wOR534Y-rCB7"
      },
      "source": [
        "Next, we'll define the URL of the shared folder and specify a local path where we want to save the data. We will download the same COCO-2017 sample dataset from the link provided earlier.\n",
        "\n",
        "I have created a shared link with public access here: https://drive.google.com/drive/folders/1G6JKGm0sy5d5ViEpDktXMxIqq5cl2Nrc?usp=drive_link\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BrgOkWKrrFT4",
      "metadata": {
        "id": "BrgOkWKrrFT4"
      },
      "outputs": [],
      "source": [
        "# This assumes 'save_path' is defined from a previous cell.\n",
        "# Let's ensure it exists.\n",
        "save_path = Path('/gdrive/MyDrive/fiftyone_dataset_curation')\n",
        "download_output_path = save_path / \"gdown_downloads\"\n",
        "os.makedirs(download_output_path, exist_ok=True)\n",
        "\n",
        "# The public Google Drive folder URL for the COCO-2017 samples\n",
        "folder_url = \"https://drive.google.com/drive/folders/1G6JKGm0sy5d5ViEpDktXMxIqq5cl2Nrc?usp=drive_link\"\n",
        "\n",
        "# Use gdown to download the entire folder.\n",
        "# The --folder flag specifies that we are downloading a folder.\n",
        "# The -O flag sets the output directory.\n",
        "!gdown --folder \"{folder_url}\" -O \"{download_output_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K0K8M0GLrIWs",
      "metadata": {
        "id": "K0K8M0GLrIWs"
      },
      "source": [
        "Notice that we have hit a limit:\n",
        "\n",
        "```bash\n",
        "The gdrive folder with url: https://drive.google.com/drive/folders/1fe\n",
        "\tUZBqLJm_2OoxLKTxV53SGPZvUYPa49?hl=en has more than 50 files, gdrive\n",
        "\tcan't download more than this limit.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3Gx_092yrNNj",
      "metadata": {
        "id": "3Gx_092yrNNj"
      },
      "source": [
        "The file has to be zipped first to bypass this and we must use the shared link to this version."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iPYRZhhtrRKE",
      "metadata": {
        "id": "iPYRZhhtrRKE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Define the path to the folder you want to zip\n",
        "folder_to_zip = '/gdrive/MyDrive/fiftyone_dataset_curation/coco-2017'\n",
        "\n",
        "# Define the name and path for the output zip file\n",
        "output_zip_file = '/gdrive/MyDrive/fiftyone_dataset_curation/coco-2017.zip'\n",
        "\n",
        "# Create a ZipFile object in write mode\n",
        "with zipfile.ZipFile(output_zip_file, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "    # Walk through all the files and subdirectories in the folder\n",
        "    for root, dirs, files in os.walk(folder_to_zip):\n",
        "        for file in files:\n",
        "            # Create the full path to the file\n",
        "            file_path = os.path.join(root, file)\n",
        "            # Add the file to the zip archive, preserving the directory structure\n",
        "            zipf.write(file_path, os.path.relpath(file_path, folder_to_zip))\n",
        "\n",
        "print(f\"Folder '{folder_to_zip}' successfully zipped to '{output_zip_file}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "x2Xcy5acrWP0",
      "metadata": {
        "id": "x2Xcy5acrWP0"
      },
      "source": [
        "We need to create the shared link again:\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/create_sharing_link.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Kc8E2qkFvplZ",
      "metadata": {
        "id": "Kc8E2qkFvplZ"
      },
      "source": [
        "You will get a \"Link Copied\" message, then you will need to click on `Manage Access`. Be sure to give `Everyone` the `Viewer` permission if you want this file to be shared with others."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mXQNQPWtvbZ2",
      "metadata": {
        "id": "mXQNQPWtvbZ2"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/manage_access.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j7llYZxrwRZ5",
      "metadata": {
        "id": "j7llYZxrwRZ5"
      },
      "source": [
        "Now try running the download again! You will need to unzip the file to get the loading to work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nDAl_7-bwmxC",
      "metadata": {
        "id": "nDAl_7-bwmxC"
      },
      "outputs": [],
      "source": [
        "# The public Google Drive folder URL for the COCO-2017 samples\n",
        "file_url = \"https://drive.google.com/file/d/1UyL0clgoIPSYDWHlPjLP0IPZoFRQQJDE/view?usp=drive_link\"\n",
        "\n",
        "# Use gdown to download the entire file.\n",
        "!gdown \"{file_url}\" -O \"{download_output_path}\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KiKWDee60eem",
      "metadata": {
        "id": "KiKWDee60eem"
      },
      "source": [
        "Or in pure Python:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Tj9vDEr0hID",
      "metadata": {
        "id": "8Tj9vDEr0hID"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "import fiftyone as fo\n",
        "\n",
        "# --- 1. Setup Paths ---\n",
        "# This assumes 'save_path' is defined from a previous cell.\n",
        "# Let's ensure it exists.\n",
        "save_path = Path('/gdrive/MyDrive/fiftyone_dataset_curation')\n",
        "download_dir = save_path / \"gdown_downloads\"\n",
        "os.makedirs(download_dir, exist_ok=True)\n",
        "\n",
        "# Define the full path for the downloaded zip file\n",
        "zip_file_path = download_dir / \"coco_samples_dataset.zip\"\n",
        "\n",
        "# Define the directory where the contents will be unzipped\n",
        "unzip_dir = download_dir / \"coco_samples_unzipped\"\n",
        "os.makedirs(unzip_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# --- 2. Download the ZIP file from Google Drive ---\n",
        "# The public Google Drive file URL for the zipped COCO-2017 samples\n",
        "file_url = \"https://drive.google.com/file/d/1UyL0clgoIPSYDWHlPjLP0IPZoFRQQJDE/view?usp=drive_link\"\n",
        "\n",
        "print(f\"Downloading dataset from Google Drive to '{zip_file_path}'...\")\n",
        "# Use gdown.download to fetch the file\n",
        "gdown.download(url=file_url, output=str(zip_file_path), quiet=False)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "\n",
        "# --- 3. Unzip the File using the `zipfile` library ---\n",
        "print(f\"Unzipping '{zip_file_path}' to '{unzip_dir}'...\")\n",
        "\n",
        "# Use a 'with' statement to safely open and extract the zip archive\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(unzip_dir)\n",
        "    print(\"Unzipping successful.\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"Error: The downloaded file is not a valid zip file or is corrupted.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred during unzipping: {e}\")\n",
        "\n",
        "\n",
        "# --- 4. Load the dataset into FiftyOne ---\n",
        "# The directory containing the unzipped FiftyOne dataset\n",
        "# Note: The actual dataset might be in a subdirectory within `unzip_dir`.\n",
        "# For this specific zip file, the data is in the root of the unzipped folder.\n",
        "dataset_dir = unzip_dir\n",
        "\n",
        "print(f\"Loading FiftyOne dataset from '{dataset_dir}'...\")\n",
        "\n",
        "# Check if the directory exists and appears to be a FiftyOne dataset\n",
        "if (Path(dataset_dir) / \"samples.json\").exists():\n",
        "    # Use Dataset.from_dir to load the dataset from its source directory\n",
        "    dataset = fo.Dataset.from_dir(\n",
        "        dataset_dir=str(dataset_dir),\n",
        "        dataset_type=fo.types.FiftyOneDataset,\n",
        "        name=\"coco-samples-from-drive\" # Give the dataset a unique name\n",
        "    )\n",
        "\n",
        "    print(\"\\nDataset loaded successfully!\")\n",
        "    print(dataset)\n",
        "\n",
        "    # You can now work with the dataset, for example, launch the App\n",
        "    # session = fo.launch_app(dataset)\n",
        "    # print(session.url)\n",
        "else:\n",
        "    print(f\"Error: Could not find a valid FiftyOne dataset at '{dataset_dir}'.\")\n",
        "    print(\"Please check the contents of the unzipped folder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PdZbVrwlx9Z_",
      "metadata": {
        "id": "PdZbVrwlx9Z_"
      },
      "source": [
        "**But then you might get a message like this:**\n",
        "\n",
        "```bash\n",
        "Cannot retrieve the folder information from the link. You may need to\n",
        "\tchange the permission to 'Anyone with the link', or have had many\n",
        "\taccesses. Check FAQ in https://github.com/wkentaro/gdown?tab=readme-\n",
        "\tov-file#faq.\n",
        "```\n",
        "**\"...or have had many accesses\"** what to do then?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5d2780-b2dd-41e9-91c9-5a50785f8abf",
      "metadata": {
        "id": "4b5d2780-b2dd-41e9-91c9-5a50785f8abf"
      },
      "source": [
        "## Tip 4: share datasets instantly by using `\"Add Shortcut to Drive...\"`\n",
        "\n",
        "Adding a shortcut from a shared Google Drive folder to your own account allows you fast access to the data **without having to download it, or compress and decompress it**. It's a much more immediate option than using `gdown`.\n",
        "\n",
        "Follow these steps to add a shortcut to a shared Google Drive folder in your own Drive for quick access.\n",
        "\n",
        "You can try the following steps using this public access link to samples from the COCO-2017 validation set that I created: https://drive.google.com/drive/folders/1G6JKGm0sy5d5ViEpDktXMxIqq5cl2Nrc?usp=sharing\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1. Open Google Drive\n",
        "\n",
        "- Go to [drive.google.com](https://drive.google.com) and sign in with your Google account if you are not already signed in.\n",
        "\n",
        "2. (Optional) Locate the Shared Folder\n",
        "\n",
        "- If someone shared the folder with you through an invitation (instead of a link), look under the **\"Shared with me\"** section on the left sidebar.\n",
        "\n",
        "3. Add the Shortcut\n",
        "\n",
        "- Right-click (or two-finger click on a touchpad) on the folder name in Google Drive.\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/add_shortcut.png?raw=true)\n",
        "\n",
        "- Select **Organize > Add shortcut to Drive** from the menu.\n",
        "\n",
        "4. Choose Shortcut Location\n",
        "\n",
        "- A dialog box will appear. Select **My Drive** or any other folder in your Drive where you want the shortcut to appear.\n",
        "- Click **Add shortcut** to confirm.\n",
        "\n",
        "5. Access Your Shortcut\n",
        "\n",
        "- Go to the location you selected (e.g., \"My Drive\").\n",
        "- You will see the shortcut there, marked with a small arrow icon.\n",
        "- You will be able to refer to this folder using the path specified:\n",
        "\n",
        "```python\n",
        "from pathlib import Path\n",
        "import os\n",
        "path = Path(\"/gdrive/MyDrive/<folder_name>\")\n",
        "# This should print the filenames inside the folder\n",
        "os.listdir(path)\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Shortcuts are just pointers to the original folder-they do not make a copy or use your storage quota. Any changes you make to files within the shortcut are reflected in the original folder. They are an instant way to share data and come in handy when\n",
        "\n",
        "### Tip: Save your FiftyOne setup on Drive by modifying the `fo.config` file\n",
        "\n",
        "We use the Google Drive folder for your downloads, models, and MongoDB database by modifying `fo.config`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d8df589-9b93-4a11-a83d-3652c4168c92",
      "metadata": {
        "id": "1d8df589-9b93-4a11-a83d-3652c4168c92"
      },
      "outputs": [],
      "source": [
        "# Check the state of fo.config before doing any modification\n",
        "print(fo.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439241b7-6df7-4972-8efc-1cdca736c96a",
      "metadata": {
        "id": "439241b7-6df7-4972-8efc-1cdca736c96a"
      },
      "outputs": [],
      "source": [
        "# Where we will download the data when using the FiftyOne dataset zoo\n",
        "# https://docs.voxel51.com/dataset_zoo/index.html\n",
        "dataset_zoo_path = save_path / \"fo_dataset_zoo\"\n",
        "os.makedirs(dataset_zoo_path, exist_ok=True)\n",
        "fo.config.dataset_zoo_dir = str(dataset_zoo_path)\n",
        "\n",
        "# path to the MongoDB database\n",
        "database_path = save_path / \"mongodb\"\n",
        "os.makedirs(database_path, exist_ok=True)\n",
        "fo.config.database_dir = str(database_path)\n",
        "\n",
        "models_path = str(save_path / \"models\")\n",
        "os.makedirs(models_path, exist_ok=True)\n",
        "fo.config.model_zoo_dir = models_path\n",
        "\n",
        "model = foz.load_zoo_model(\"clip-vit-base32-torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "408018ff-a040-45d2-a72d-45c55536417f",
      "metadata": {
        "id": "408018ff-a040-45d2-a72d-45c55536417f"
      },
      "source": [
        "## Tip 5: add your HuggingFace token as a secret in Colab\n",
        "\n",
        "Adding a HuggingFace token as a secret to your Google Colab notebook allows you to share your work using models and data from HF without exposing your personal information.\n",
        "\n",
        "### Why You Need a Token\n",
        "\n",
        "*   **Access Restricted Models:** Many powerful models on HuggingFace, like Meta's DINOv3 family, are \"gated,\" meaning you must agree to their terms before you can download them. Your token verifies that you have the necessary permissions.\n",
        "*   **Avoid Rate Limiting:** Without a token, you are an anonymous user and subject to strict download limits. Using a token identifies you and grants you significantly higher download rates, preventing interruptions in your workflow.\n",
        "*   **Upload Your Own Work:** If you plan to share your fine-tuned models or datasets with the community, your token acts as your credential, allowing you to upload and manage content on the HuggingFace Hub.\n",
        "\n",
        "---\n",
        "\n",
        "### Step 1: Create a HuggingFace Account\n",
        "\n",
        "1.  Navigate to the [HuggingFace website](https://huggingface.co/).\n",
        "2.  Click the \"**Sign Up**\" button located in the top-right corner.\n",
        "3.  You can register using your email or by linking your Google or GitHub account.\n",
        "4.  Complete the process by verifying your email address.\n",
        "\n",
        "### Step 2: Generate Your Access Token\n",
        "\n",
        "1.  Once logged in, click on your profile picture at the top-right and select \"**Settings**\" from the dropdown menu.\n",
        "2.  In the left sidebar, click on \"**Access Tokens**.\"\n",
        "3.  Click the \"**New token**\" button.\n",
        "4.  Give your token a descriptive name (e.g., \"Google Colab Projects\").\n",
        "5.  Assign it the \"**Write**\" role to enable both downloading and uploading.\n",
        "6.  Click \"**Generate a token**.\"\n",
        "7.  Immediately copy the generated token and store it in a secure location, as you will not be able to see it again.\n",
        "\n",
        "### Step 3: Use Your Token in Google Colab\n",
        "\n",
        "The most secure way to use your token in Google Colab is with the built-in Secrets manager. This prevents your token from being exposed in your code.\n",
        "\n",
        "1.  **Install Required Libraries:**\n",
        "    In a Colab cell, run the following command to install the necessary HuggingFace libraries:\n",
        "    ```python\n",
        "    !pip install -q huggingface_hub transformers\n",
        "    ```\n",
        "\n",
        "2.  **Store the Token in Colab Secrets:**\n",
        "    *   In your Colab notebook, click the **key icon** (ðŸ”‘) in the left sidebar.\n",
        "    *   Click \"**Add a new secret**.\"\n",
        "    *   For the **Name**, enter `HUGGINGFACE_TOKEN`.\n",
        "    *   In the **Value** field, paste your copied access token.\n",
        "    *   Ensure the \"**Notebook access**\" toggle is enabled.\n",
        "\n",
        "3.  **Access the Token in Your Notebook:**\n",
        "    Use the following code to securely load your token from Colab Secrets into your environment.\n",
        "    ```python\n",
        "    import os\n",
        "    from google.colab import userdata\n",
        "\n",
        "    # Load the token from Colab secrets\n",
        "    os.environ[\"HUGGINGFACE_TOKEN\"] = userdata.get('HUGGINGFACE_TOKEN')\n",
        "\n",
        "    # You can now use the HuggingFace API\n",
        "    from huggingface_hub import HfApi\n",
        "    api = HfApi()\n",
        "\n",
        "    # Example: List your models to verify the token is working\n",
        "    # Replace \"your-username\" with your actual HuggingFace username\n",
        "    my_models = api.list_models(author=\"your-username\")\n",
        "    print(\"Successfully connected and fetched models.\")\n",
        "    ```\n",
        "\n",
        "---\n",
        "\n",
        "### Security Best Practices\n",
        "\n",
        "*   **Never expose your token** in plain text in your notebook cells or share it publicly.\n",
        "*   **Use the Secrets manager** as the primary method for handling tokens in Colab.\n",
        "*   If you believe your token has been compromised, go to your HuggingFace settings and revoke it immediately.\n",
        "\n",
        "\n",
        "### Getting a dataset from HuggingFace's Hub\n",
        "\n",
        "Having the HF token set up, interaction with the platform is much smoother."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7c10d32-d1d8-4f80-873b-e8557b458d92",
      "metadata": {
        "id": "e7c10d32-d1d8-4f80-873b-e8557b458d92"
      },
      "outputs": [],
      "source": [
        "from fiftyone.utils.huggingface import load_from_hub\n",
        "\n",
        "other_datasets_path = save_path / \"other_datasets\"\n",
        "os.makedirs(other_datasets_path, exist_ok=True)\n",
        "fo.config.default_dataset_dir = str(other_datasets_path)\n",
        "\n",
        "curated_mnist_dataset = load_from_hub(\"Voxel51/curated-mnist\",\n",
        "                                       max_samples=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87550f24-6997-40fd-b3f5-62d3a1f80879",
      "metadata": {
        "id": "87550f24-6997-40fd-b3f5-62d3a1f80879"
      },
      "source": [
        "## Tip 6: control the launch of the FiftyOne app in a separate browser tab\n",
        "\n",
        "Launching the FiftyOne app in a separate browser tab provides a full window view. This is often more convenient and provides a better user experience than launching the app directly within a notebook cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90a2c07-e815-46f9-8d19-4f7f457cc037",
      "metadata": {
        "id": "a90a2c07-e815-46f9-8d19-4f7f457cc037"
      },
      "outputs": [],
      "source": [
        "# Passing auto=False prevents the app from launch on its own notebook cell\n",
        "session = fo.launch_app(dataset, auto=False)\n",
        "# print session.url gives us a nice URL that we can click on ;)\n",
        "print(f\"Just click here to get to the app (whenever you want, no auto launch) {session.url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kXY9SmeFzAuT",
      "metadata": {
        "id": "kXY9SmeFzAuT"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/full_window_view.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ff89o5Wjr8eP",
      "metadata": {
        "id": "Ff89o5Wjr8eP"
      },
      "source": [
        "`session.open_tab()` is another option, however it fires the app automatically, which can feel disruptive / distracting. I prefer to use `print(session.url)` to control when I access the FiftyOne app.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4sFS6zoDr1zt",
      "metadata": {
        "id": "4sFS6zoDr1zt"
      },
      "outputs": [],
      "source": [
        "# session.open_tab() is another option, we just don't get to see the URL here\n",
        "#session.open_tab()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b207eb-1215-4752-b883-9bb6d8928014",
      "metadata": {
        "id": "59b207eb-1215-4752-b883-9bb6d8928014"
      },
      "source": [
        "## Tip 7: keep a single instance of the app running\n",
        "\n",
        "Each time you call `fo.launch_app()`, FiftyOne launches a new server process in the background to power the app. While you can launch multiple apps, it's a good practice to manage a single instance to conserve resources and avoids confusion. If you see the FiftyOne app \"flickering\" it's often because we have two instances of it running at the same time.\n",
        "\n",
        "When you launch the app, store the returned `session` object. You can use this object to programmatically control the app. If you have a session running and want to get a handle on it again, you can call `fo.launch_app()` again, and FiftyOne will simply give you a handle to the existing session without creating a new one.\n",
        "\n",
        "Let's see this in action. First, we launch the app and get our session object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da14ab77-8d26-4740-b608-cc1840dd30f5",
      "metadata": {
        "id": "da14ab77-8d26-4740-b608-cc1840dd30f5"
      },
      "outputs": [],
      "source": [
        "# The dataset is already loaded from a previous cell\n",
        "# dataset = foz.load_zoo_dataset(\"coco-2017\", split=\"validation\", max_samples=200)\n",
        "\n",
        "# If it's flickering it's because we have already called fo.launch_app() inside the notebook\n",
        "# This will gracefully shut down the App server associated with the previous session\n",
        "session.close()\n",
        "\n",
        "# Launch the app and store the session.\n",
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678e7cd0-02ba-4b8c-b690-b964344cd35b",
      "metadata": {
        "id": "678e7cd0-02ba-4b8c-b690-b964344cd35b"
      },
      "source": [
        "Now you can perform other operations in your notebook. When you want to interact with the app again, for example, to load a new view, you can just use the `session` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b1a20a-6663-4702-861c-8b2014605934",
      "metadata": {
        "id": "97b1a20a-6663-4702-861c-8b2014605934"
      },
      "outputs": [],
      "source": [
        "from fiftyone import ViewField as F\n",
        "\n",
        "dog_view = dataset.filter_labels(\"ground_truth\", F(\"label\") == \"dog\")\n",
        "session.view = dog_view"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55f5f747-063a-4424-8f43-6d1a9386d494",
      "metadata": {
        "id": "55f5f747-063a-4424-8f43-6d1a9386d494"
      },
      "source": [
        "If you close the App's cell or tab, the underlying Python process is still running. You can simply open a new tab to the App's URL. If you need to stop the App server completely, you can close the session."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ac877e3-0c4a-4d7a-b9c1-4b15096fd5d2",
      "metadata": {
        "id": "0ac877e3-0c4a-4d7a-b9c1-4b15096fd5d2"
      },
      "source": [
        "## Tip 8: use the GPU to run inference faster\n",
        "\n",
        "To significantly speed up inference and other computations in FiftyOne, you can leverage the power of a GPU. Google Colab provides free access to GPUs, which can we can enable by going to:\n",
        "\n",
        "```\n",
        "Runtime -> Change Runtime Type -> T4 GPU\n",
        "```\n",
        "\n",
        "This gives us access to an NVIDIA T4 GPU with about 16 GB of VRAM. Inference will run much faster with it enabled, this affects significantly how quickly we will be able to obtain predictions or embeddings out of our models. We can try this with a speed test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "017ac311-b0e5-4f32-8df9-51a441315e12",
      "metadata": {
        "id": "017ac311-b0e5-4f32-8df9-51a441315e12"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Check if the GPU is available\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cVZq4srmqfsf",
      "metadata": {
        "id": "cVZq4srmqfsf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import fiftyone.brain as fob\n",
        "\n",
        "# Helper: measure time for computing embeddings into a field\n",
        "def time_compute_embeddings_and_project(model, label=\"gpu\"):\n",
        "    start = time.time()\n",
        "    # Use Brain to both compute embeddings and reduce dimensionality\n",
        "    res = fob.compute_visualization(\n",
        "        dataset,\n",
        "        model=model,\n",
        "        embeddings=\"clip_embeddings_\" + label,\n",
        "        brain_key=\"vis_\" + label,\n",
        "        method='pca',\n",
        "        batch_size=16,\n",
        "    )\n",
        "    end = time.time()\n",
        "    return end - start\n",
        "\n",
        "# Load CLIP (OpenAI ViT-B/32) from the model zoo\n",
        "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\")\n",
        "print(\"Model has embeddings:\", getattr(clip_model, \"has_embeddings\", None))\n",
        "\n",
        "# Benchmark on GPU (if available)\n",
        "gpu_time = None\n",
        "if torch.cuda.is_available():\n",
        "    # Many FiftyOne zoo models run on GPU automatically if available;\n",
        "    # timing reflects GPU execution.\n",
        "    gpu_time = time_compute_embeddings_and_project(clip_model, label=\"gpu\")\n",
        "    print(f\"GPU time (s): {gpu_time:.2f}\")\n",
        "\n",
        "# Force CPU run by moving model to CPU (and/or disabling CUDA)\n",
        "# Depending on environment, you may need to ensure the model runs on CPU.\n",
        "# Re-load a fresh model instance to avoid device cross-talk\n",
        "clip_model_cpu = foz.load_zoo_model(\"clip-vit-base32-torch\")\n",
        "# If the wrapped model exposes .cuda()/.cpu(), it will be set appropriately by the integration.\n",
        "# Here we simply assume CPU because no CUDA ops are used when CUDA is unavailable.\n",
        "# If your environment auto-selects GPU, set CUDA_VISIBLE_DEVICES=\"\" before launching Python\n",
        "cpu_time = time_compute_embeddings_and_project(clip_model_cpu, label=\"cpu\")\n",
        "print(f\"CPU time (s): {cpu_time:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67041a7d-b6da-4903-885f-7589de648341",
      "metadata": {
        "id": "67041a7d-b6da-4903-885f-7589de648341"
      },
      "source": [
        "## Tip 9: use the extra RAM if you have it\n",
        "\n",
        "When working with large datasets, especially with high-resolution images or videos, you may run into memory limitations on the standard Colab runtime (which typically provides around 12GB of RAM). Colab Pro offers a \"High-RAM\" runtime that provides significantly more memory (around 25GB or more), which can be essential for memory-intensive tasks.\n",
        "\n",
        "To enable a high-RAM runtime, go to:\n",
        "```\n",
        "Runtime -> Change Runtime Type -> Runtime shape -> High-RAM\n",
        "```\n",
        "This option will give your notebook access to a machine with a larger memory capacity, allowing you to load and process more data without the session crashing. This is very useful when you need to load a large number of samples into FiftyOne, compute embeddings for your entire dataset, or perform other memory-heavy operations.\n",
        "\n",
        "FiftyOne is snappy and quick even on low resources.\n",
        "You can try loading a bigger version of the COCO-2017 dataset and compare the feel of the app when you have the extra RAM available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4221d6f-2391-4960-93ed-303c73489849",
      "metadata": {
        "id": "e4221d6f-2391-4960-93ed-303c73489849"
      },
      "outputs": [],
      "source": [
        "import psutil\n",
        "\n",
        "# Check available RAM\n",
        "total_ram_bytes = psutil.virtual_memory().total\n",
        "# Convert 16GB to bytes\n",
        "sixteen_gb_bytes = 16 * 1024 * 1024 * 1024\n",
        "\n",
        "if total_ram_bytes > sixteen_gb_bytes:\n",
        "    try:\n",
        "        big_dataset = foz.load_zoo_dataset(\n",
        "            \"coco-2017\",\n",
        "            split=\"validation\",\n",
        "            max_samples=5000,  # Loading 5000 samples requires more memory\n",
        "        )\n",
        "        print(f\"Successfully loaded a larger dataset with {len(big_dataset)} samples.\")\n",
        "        # You can now launch the app with this larger dataset\n",
        "        # session = fo.launch_app(big_dataset)\n",
        "        # print(session.url)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        print(\"This may be due to insufficient RAM. Try switching to a High-RAM runtime.\")\n",
        "else:\n",
        "    print(\"Skipping loading the larger dataset: Insufficient RAM (less than 16GB) available.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I5G_ssxD2yRl",
      "metadata": {
        "id": "I5G_ssxD2yRl"
      },
      "source": [
        "## Tip 10: ignore `Invalid Notebook` messages on GitHub using the `githubtocolab` URL trick\n",
        "\n",
        "Many Jupyter and Colab notebooks produce artifacts that don't allow them to render properly on GitHub previews.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/invalid_notebook.png?raw=true)\n",
        "\n",
        "Here we have an example: https://github.com/andandandand/practical-computer-vision/blob/main/notebooks/Food_Dataset_Curation_with_Fiftyone.ipynb\n",
        "\n",
        "If we change the URL from `github.com` to `githubtocolab.com`, the same renders immediately. Try it out!\n",
        "\n",
        "https://githubtocolab.com/andandandand/practical-computer-vision/blob/main/notebooks/Food_Dataset_Curation_with_Fiftyone.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R8QKGjHf3kB_",
      "metadata": {
        "id": "R8QKGjHf3kB_"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/notebooks/working_colab.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TsjcntVQi84d",
      "metadata": {
        "id": "TsjcntVQi84d"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook teaches techniques for working with FiftyOne in Google Colab's temporary environment.\n",
        "\n",
        "**Installation & Setup**\n",
        "- Use `uv` for fast package installation\n",
        "- Apply `%%capture` to suppress verbose output\n",
        "- Pin explicit versions for reproducibility\n",
        "\n",
        "**Data Persistence**\n",
        "- Mount Google Drive to preserve datasets across sessions\n",
        "- Configure FiftyOne's MongoDB to Drive storage paths\n",
        "- Export/import datasets using `fo.types.FiftyOneDataset`\n",
        "\n",
        "**Sharing & Collaboration**\n",
        "- Download shared folders with `gdown` (works best for smaller datasets)\n",
        "- Set proper permissions for collaborative access\n",
        "\n",
        "**App Control**\n",
        "- Launch with `auto=False` and print URLs for manual access\n",
        "- Manage single sessions to prevent flickering\n",
        "- Close/reopen without data loss\n",
        "\n",
        "**Performance**\n",
        "- Enable T4 GPU runtime for faster inference\n",
        "- Use High-RAM runtime (25GB+) for large datasets\n",
        "- Benchmark CPU vs GPU execution times\n",
        "\n",
        "**Workflow Tips**\n",
        "- Use `githubtocolab.com` to render notebooks that fail on GitHub\n",
        "- Remember: 90-minute idle timeout, 12-hour maximum session\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}