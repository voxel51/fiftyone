{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LeNet-5 Model Evaluation on Test Data\n",
    "\n",
    "After training our LeNet-5 model, the next crucial step is to evaluate its performance on the unseen test set. This gives us an unbiased measure of how well our model generalizes to new data. We will apply the model, store its predictions in FiftyOne, and then use FiftyOne's powerful evaluation tools to analyze the results in detail.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Applying a PyTorch model to a FiftyOne dataset\n",
    "*   Storing predictions, confidence, and logits\n",
    "*   Evaluating classification performance\n",
    "*   Analyzing prediction confidence distributions\n",
    "*   Computing sample hardness and mistakenness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We need to reload our datasets and redefine our model architecture and helper classes to apply the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Redefine the model architecture so we can load the weights\n",
    "class ModernLeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModernLeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(Fun.relu(self.conv1(x)))\n",
    "        x = self.pool(Fun.relu(self.conv2(x)))\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = Fun.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Redefine the custom dataset class\n",
    "class CustomTorchImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fiftyone_dataset, image_transforms=None, label_map=None, gt_field=\"ground_truth\"):\n",
    "        self.fiftyone_dataset = fiftyone_dataset\n",
    "        self.image_paths = self.fiftyone_dataset.values(\"filepath\")\n",
    "        self.str_labels = self.fiftyone_dataset.values(f\"{gt_field}.label\")\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_map = label_map if label_map is not None else {str(i): i for i in range(10)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        if self.image_transforms: image = self.image_transforms(image)\n",
    "        label_str = self.str_labels[idx]\n",
    "        label_idx = self.label_map.get(label_str, -1)\n",
    "        return image, torch.tensor(label_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the Best LeNet Model\n",
    "\n",
    "We load the saved model weights that achieved the best validation performance during training. This ensures we are evaluating the most generalizable version of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_save_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
    "\n",
    "loaded_model = ModernLeNet5().to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Model to the Test Set\n",
    "\n",
    "We'll now run inference on the entire test set. We'll collect the predictions, confidence scores, and raw logits for each sample and store them back into our FiftyOne dataset. Storing predictions as structured `fo.Classification` objects allows for rich, interactive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "train_dataset = fo.load_dataset(\"mnist-training-set\")\n",
    "\n",
    "# Recreate transforms using stats from the training set\n",
    "mean_intensity, std_intensity = 0.1307, 0.3081 # Pre-computed for simplicity\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
    "])\n",
    "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
    "label_map = {label: i for i, label in enumerate(dataset_classes)}\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "torch_test_set = CustomTorchImageDataset(test_dataset, image_transforms=image_transforms, label_map=label_map)\n",
    "test_loader = torch.utils.data.DataLoader(torch_test_set, batch_size=64, num_workers=os.cpu_count())\n",
    "\n",
    "# Run inference\n",
    "predictions, all_logits = [], []\n",
    "with torch.inference_mode():\n",
    "    for images, _ in tqdm(test_loader, desc=\"Applying model to test set\"):\n",
    "        images = images.to(device)\n",
    "        logits = loaded_model(images)\n",
    "        all_logits.append(logits.cpu().numpy())\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "# Store predictions back in FiftyOne\n",
    "with fo.ProgressBar(total=len(test_dataset), desc=\"Storing predictions\") as pb:\n",
    "    for i, sample in enumerate(test_dataset):\n",
    "        pred_idx = predictions[i]\n",
    "        sample_logits = all_logits[i]\n",
    "        conf = float(Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()[pred_idx])\n",
        "        sample[\"lenet_classification\"] = fo.Classification(\n",
    "            label=dataset_classes[pred_idx],\n",
    "            confidence=conf,\n",
    "            logits=sample_logits.tolist()\n",
    "        )\n",
    "        sample.save()\n",
    "        pb.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating LeNet's Performance\n",
    "\n",
    "With predictions stored, we can use `evaluate_classifications()` again to get a performance report and confusion matrix for our custom LeNet model. We expect a significant improvement over CLIP's zero-shot performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(test_dataset)\n",
    "lenet_evaluation_results = test_dataset.evaluate_classifications(\n",
    "    \"lenet_classification\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"lenet_eval\")\n",
    "\n",
    "session.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenet_evaluation_results.print_report(digits=3)\n",
    "lenet_evaluation_results.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy should be >99%, a huge leap from CLIP's ~88%. The confusion matrix is also much cleaner, with most values concentrated on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardness and Mistakenness\n",
    "\n",
    "Beyond accuracy, we can analyze the model's logits to understand sample-level difficulties.\n",
    "\n",
    "- **Hardness**: Measures the model's prediction uncertainty. High hardness indicates samples the model found difficult, which are often edge cases.\n",
    "- **Mistakenness**: Identifies samples where the model was confident but wrong. High mistakenness can often point to labeling errors in the dataset.\n",
    "\n",
    "We use `fiftyone.brain` to compute these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fob.compute_hardness(test_dataset, label_field='lenet_classification')\n",
    "\n",
    "fob.compute_mistakenness(test_dataset, \n",
    "                         pred_field=\"lenet_classification\",\n",
    "                         label_field=\"ground_truth\")\n",
    "\n",
    "session.refresh()\n",
    "print(\"Hardness and mistakenness computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the samples with the highest mistakenness scores. These are the prime candidates for being mislabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mistakenness_quantiles = test_dataset.quantiles(\"mistakenness\", [0.99])\n",
    "\n",
    "suspicious_test_samples_view = test_dataset.match(\n",
    "                             F(\"mistakenness\") > mistakenness_quantiles[-1]\n",
    "                             ).sort_by(\"mistakenness\", reverse=True)\n",
    "\n",
    "session.view = suspicious_test_samples_view\n",
    "print(f\"Displaying {len(suspicious_test_samples_view)} most mistaken samples in the App: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We've thoroughly evaluated our LeNet model on the test set. But what did the model actually *learn*? \n",
    "\n",
    "In the next notebook, we'll dive into the model's internal representations by extracting embeddings from its hidden layers on the *training data*. This will help us understand the features it learned and analyze the training set for quality issues.\n",
    "\n",
    "Proceed to `6_lenet_feature_analysis.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}