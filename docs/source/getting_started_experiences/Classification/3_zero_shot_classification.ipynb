{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Zero-Shot Classification with CLIP\n",
    "\n",
    "Now that we understand how CLIP creates meaningful embeddings, we can leverage them for **zero-shot classification**. This means classifying images into new categories without any task-specific model retraining. CLIP maps images and text to the same embedding space, allowing us to classify an image by finding the text description with the most similar embedding, something that we explored briefly [in the previous notebook](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/2_clip_embeddings.ipynb). \n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Zero-shot classification principles\n",
    "*   Text prompts for classification\n",
    "*   [Applying a model](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset) to a FiftyOne [dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset)\n",
    "*   Evaluating classification results including accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import our libraries and load the test dataset, which should now contain the CLIP embeddings from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "\n",
      "Welcome to\n",
      "\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
      "â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
      "â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•     â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•\n",
      "â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
      "â•šâ•â•     â•šâ•â•â•šâ•â•        â•šâ•â•      â•šâ•â•    â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• v1.7.0\n",
      "\n",
      "If you're finding FiftyOne helpful, here's how you can get involved:\n",
      "\n",
      "|\n",
      "|  â­â­â­ Give the project a star on GitHub â­â­â­\n",
      "|  https://github.com/voxel51/fiftyone\n",
      "|\n",
      "|  ðŸš€ðŸš€ðŸš€ Join the FiftyOne Discord community ðŸš€ðŸš€ðŸš€\n",
      "|  https://community.voxel51.com/\n",
      "|\n",
      "\n",
      "http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Ensure the test dataset exists\n",
    "if \"mnist-test-set\" in fo.list_datasets():\n",
    "    test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "else:\n",
    "    print(\"Test dataset not found. Please run '1_explore_mnist.ipynb' and '2_clip_embeddings.ipynb' first.\")\n",
    "\n",
    "session = fo.launch_app(test_dataset, auto=False)\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Zero-Shot Classification\n",
    "\n",
    "First, we get the distinct class labels from our dataset. For MNIST, these are the digits 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
    "dataset_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Effect of the Text Prompt\n",
    "\n",
    "With CLIP, the `text_prompt` significantly affects accuracy. It provides context for the class labels. We'll use a simple prompt, but feel free to experiment with others like \"A grayscale image of the number\" or \"An MNIST digit\" to see how performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|    2.6Gb/2.6Gb [36.2s elapsed, 0s remaining, 123.6Mb/s]      \n",
      "Downloading CLIP tokenizer...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|   10.4Mb/10.4Mb [156.6ms elapsed, 0s remaining, 66.1Mb/s]     \n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of \",\n",
    "    classes=dataset_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the model to our dataset. The `apply_model()` method will iterate through the samples, generate a prediction for each one, and store it in a new field. We set `store_logits=True` to save the model's raw output scores, which are useful for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [48.9s elapsed, 0s remaining, 199.1 samples/s]      \n",
      "CLIP predictions added to the dataset.\n"
     ]
    }
   ],
   "source": [
    "test_dataset.apply_model(\n",
    "    model=clip_model,\n",
    "    label_field=\"clip_zero_shot_classification\",\n",
    "    store_logits=True,\n",
    "    batch_size=256,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "session.refresh()\n",
    "print(\"CLIP predictions added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating CLIP's Performance\n",
    "\n",
    "Now that we have predictions, we can evaluate them against the ground truth labels. FiftyOne's `evaluate_classifications()` method provides a comprehensive report and a confusion matrix.\n",
    "\n",
    "You can also view these results interactively in the FiftyOne App's [Model Evaluation Panel](https://docs.voxel51.com/user_guide/app.html#model-evaluation-panel-sub-new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_evaluation_results = test_dataset.evaluate_classifications(\n",
    "    \"clip_zero_shot_classification\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"clip_zero_shot_eval\")\n",
    "\n",
    "session.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a classification report. We can see that the overall performance is well below the 88% accuracy reported on the original CLIP paper (as we are using a smaller variant of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0 - zero      0.256     0.999     0.407       980\n",
      "     1 - one      0.144     0.033     0.053      1135\n",
      "     2 - two      1.000     0.016     0.031      1032\n",
      "   3 - three      0.899     0.387     0.541      1010\n",
      "    4 - four      0.515     0.178     0.265       982\n",
      "    5 - five      0.600     0.161     0.254       892\n",
      "     6 - six      0.076     0.254     0.117       958\n",
      "   7 - seven      0.612     0.634     0.623      1028\n",
      "   8 - eight      0.179     0.116     0.141       974\n",
      "    9 - nine      0.000     0.000     0.000      1009\n",
      "\n",
      "    accuracy                          0.275     10000\n",
      "   macro avg      0.428     0.278     0.243     10000\n",
      "weighted avg      0.427     0.275     0.241     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clip_evaluation_results.print_report(classes=dataset_classes, digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows which classes are most often confused. A perfect model would have values only on the main diagonal. Here, we see significant confusion between certain digits (like 4s and 9s, or 3s and 8s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please install anywidget to use the FigureWidget class",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclip_evaluation_results\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fiftyone/.venv/lib/python3.10/site-packages/fiftyone/utils/eval/base.py:877\u001b[0m, in \u001b[0;36mBaseClassificationResults.plot_confusion_matrix\u001b[0;34m(self, classes, include_other, include_missing, other_label, backend, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Plots a confusion matrix for the evaluation results.\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \n\u001b[1;32m    826\u001b[0m \u001b[38;5;124;03mIf you are working in a notebook environment with the default plotly\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m    -   a matplotlib figure, otherwise\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    869\u001b[0m confusion_matrix, labels, ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_confusion_matrix(\n\u001b[1;32m    870\u001b[0m     classes\u001b[38;5;241m=\u001b[39mclasses,\n\u001b[1;32m    871\u001b[0m     include_other\u001b[38;5;241m=\u001b[39minclude_other,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    874\u001b[0m     tabulate_ids\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    875\u001b[0m )\n\u001b[0;32m--> 877\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgt_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fiftyone/.venv/lib/python3.10/site-packages/fiftyone/core/plots/base.py:83\u001b[0m, in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(confusion_matrix, labels, ids, samples, eval_key, gt_field, pred_field, backend, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplotly\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m plot_confusion_matrix \u001b[38;5;28;01mas\u001b[39;00m _plot_confusion_matrix\n\u001b[1;32m     73\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m     75\u001b[0m             ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m     )\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_plot_confusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fiftyone/.venv/lib/python3.10/site-packages/fiftyone/core/plots/plotly.py:111\u001b[0m, in \u001b[0;36mplot_confusion_matrix\u001b[0;34m(confusion_matrix, labels, ids, samples, eval_key, gt_field, pred_field, colorscale, log_colorscale, title, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _plot_confusion_matrix_static(\n\u001b[1;32m    102\u001b[0m         confusion_matrix,\n\u001b[1;32m    103\u001b[0m         labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    109\u001b[0m     )\n\u001b[0;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_plot_confusion_matrix_interactive\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m    \u001b[49m\u001b[43msamples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgt_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolorscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtitle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/fiftyone/.venv/lib/python3.10/site-packages/fiftyone/core/plots/plotly.py:239\u001b[0m, in \u001b[0;36m_plot_confusion_matrix_interactive\u001b[0;34m(confusion_matrix, labels, ids, samples, eval_key, gt_field, pred_field, colorscale, title, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m     selection_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselect\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    237\u001b[0m     init_fcn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 239\u001b[0m plot \u001b[38;5;241m=\u001b[39m \u001b[43mInteractiveHeatmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mylabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mylabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mzlim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mzlim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgt_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgt_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpred_field\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpred_field\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolorscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolorscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlink_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_view\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_fields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_fields\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mselection_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mselection_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_fcn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_fcn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m plot\u001b[38;5;241m.\u001b[39mupdate_layout(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_DEFAULT_LAYOUT)\n\u001b[1;32m    256\u001b[0m plot\u001b[38;5;241m.\u001b[39mupdate_layout(title\u001b[38;5;241m=\u001b[39mtitle, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/fiftyone/.venv/lib/python3.10/site-packages/fiftyone/core/plots/plotly.py:2029\u001b[0m, in \u001b[0;36mInteractiveHeatmap.__init__\u001b[0;34m(self, Z, ids, xlabels, ylabels, zlim, values_title, gt_field, pred_field, colorscale, grid_opacity, bg_opacity, **kwargs)\u001b[0m\n\u001b[1;32m   2025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr_zlim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2027\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cells_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m-> 2029\u001b[0m widget \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_widget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2030\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_cells_map()\n\u001b[1;32m   2032\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(widget, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/fiftyone/.venv/lib/python3.10/site-packages/fiftyone/core/plots/plotly.py:2062\u001b[0m, in \u001b[0;36mInteractiveHeatmap._make_widget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_make_widget\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m-> 2062\u001b[0m     widget \u001b[38;5;241m=\u001b[39m \u001b[43mgo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFigureWidget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_figure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2063\u001b[0m     gridw, selectedw, bgw \u001b[38;5;241m=\u001b[39m widget\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m   2065\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gridw \u001b[38;5;241m=\u001b[39m gridw\n",
      "File \u001b[0;32m~/fiftyone/.venv/lib/python3.10/site-packages/plotly/missing_anywidget.py:13\u001b[0m, in \u001b[0;36mFigureWidget.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease install anywidget to use the FigureWidget class\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: Please install anywidget to use the FigureWidget class"
     ]
    }
   ],
   "source": [
    "clip_evaluation_results.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up\n",
    "\n",
    "Before moving on, let's clear the CLIP model from memory to free up GPU resources for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clip_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"CLIP model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have successfully performed zero-shot classification and evaluated the results. This provides a strong baseline for comparison.\n",
    "\n",
    "Next, we will build and train a traditional supervised learning model, LeNet-5, to see how a task-specific model compares.\n",
    "\n",
    "Proceed to `4_lenet_training.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
