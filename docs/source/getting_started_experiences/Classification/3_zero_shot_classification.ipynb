{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Zero-Shot Classification with CLIP\n",
    "\n",
    "Now that we understand how CLIP creates meaningful embeddings, we can leverage them for **zero-shot classification**. This means classifying images into new categories without any task-specific model retraining. CLIP maps images and text to the same embedding space, allowing us to classify an image by finding the text description with the most similar embedding, something that we explored briefly [in the previous notebook](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/2_clip_embeddings.ipynb). \n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Zero-shot classification principles\n",
    "*   Text prompts for classification\n",
    "*   [Applying a model](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset) to a FiftyOne [dataset](https://docs.voxel51.com/api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset)\n",
    "*   Evaluating classification results including accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import our libraries and load the test dataset, which should now contain the CLIP embeddings from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at 0.0.0.0.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Ensure the test dataset exists\n",
    "if \"mnist-test-set\" in fo.list_datasets():\n",
    "    test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "else:\n",
    "    print(\"Test dataset not found. Please run '1_explore_mnist.ipynb' and '2_clip_embeddings.ipynb' first.\")\n",
    "\n",
    "session = fo.launch_app(test_dataset, auto=False)\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Zero-Shot Classification\n",
    "\n",
    "First, we get the distinct class labels from our dataset. For MNIST, these are the digits 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0 - zero',\n",
       " '1 - one',\n",
       " '2 - two',\n",
       " '3 - three',\n",
       " '4 - four',\n",
       " '5 - five',\n",
       " '6 - six',\n",
       " '7 - seven',\n",
       " '8 - eight',\n",
       " '9 - nine']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
    "dataset_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Effect of the Text Prompt\n",
    "\n",
    "With CLIP, the `text_prompt` significantly affects accuracy. It provides context for the class labels. We'll use a simple prompt, but feel free to experiment with others like \"A grayscale image of the number\" or \"An MNIST digit\" to see how performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of \",\n",
    "    classes=dataset_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the model to our dataset. The `apply_model()` method will iterate through the samples, generate a prediction for each one, and store it in a new field. We set `store_logits=True` to save the model's raw output scores, which are useful for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [25.6s elapsed, 0s remaining, 403.3 samples/s]      \n",
      "CLIP predictions added to the dataset.\n"
     ]
    }
   ],
   "source": [
    "test_dataset.apply_model(\n",
    "    model=clip_model,\n",
    "    label_field=\"clip_zero_shot_classification\",\n",
    "    store_logits=True,\n",
    "    batch_size=32,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "session.refresh()\n",
    "print(\"CLIP predictions added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating CLIP's Performance\n",
    "\n",
    "Now that we have predictions, we can evaluate them against the ground truth labels. FiftyOne's `evaluate_classifications()` method provides a comprehensive report and a confusion matrix.\n",
    "\n",
    "You can also view these results interactively in the FiftyOne App's [Model Evaluation Panel](https://docs.voxel51.com/user_guide/app.html#model-evaluation-panel-sub-new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_evaluation_results = test_dataset.evaluate_classifications(\n",
    "    \"clip_zero_shot_classification\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"clip_zero_shot_eval\")\n",
    "\n",
    "session.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a classification report. We can see that the overall performance is well below the 88% accuracy reported on the original CLIP paper (as we are using a smaller variant of the model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0 - zero      0.256     0.999     0.407       980\n",
      "     1 - one      0.144     0.033     0.053      1135\n",
      "     2 - two      1.000     0.016     0.031      1032\n",
      "   3 - three      0.899     0.387     0.541      1010\n",
      "    4 - four      0.515     0.178     0.265       982\n",
      "    5 - five      0.600     0.161     0.254       892\n",
      "     6 - six      0.076     0.254     0.117       958\n",
      "   7 - seven      0.612     0.634     0.623      1028\n",
      "   8 - eight      0.179     0.116     0.141       974\n",
      "    9 - nine      0.000     0.000     0.000      1009\n",
      "\n",
      "    accuracy                          0.275     10000\n",
      "   macro avg      0.428     0.278     0.243     10000\n",
      "weighted avg      0.427     0.275     0.241     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clip_evaluation_results.print_report(classes=dataset_classes, digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows which classes are most often confused. A perfect model would have values only on the main diagonal. Here, we see significant confusion between certain digits (like 4s and 9s, or 3s and 8s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40b1acb036d46029ec0a03903f72d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'markers',\n",
       "              'opacity': 0.1,\n",
       "              'type': 'scatter',\n",
       "              'uid': '64401fee-01d8-442f-ab6a-ecdf851642ad',\n",
       "              'x': {'bdata': ('AAECAwQFBgcICQABAgMEBQYHCAkAAQ' ... 'kAAQIDBAUGBwgJAAECAwQFBgcICQ=='),\n",
       "                    'dtype': 'i1'},\n",
       "              'y': {'bdata': ('AAAAAAAAAAAAAAEBAQEBAQEBAQECAg' ... 'cICAgICAgICAgICQkJCQkJCQkJCQ=='),\n",
       "                    'dtype': 'i1'}},\n",
       "             {'colorscale': [[0.0, 'rgb(255,245,235)'], [0.125,\n",
       "                             'rgb(254,230,206)'], [0.25, 'rgb(253,208,162)'],\n",
       "                             [0.375, 'rgb(253,174,107)'], [0.5, 'rgb(253,141,60)'],\n",
       "                             [0.625, 'rgb(241,105,19)'], [0.75, 'rgb(217,72,1)'],\n",
       "                             [0.875, 'rgb(166,54,3)'], [1.0, 'rgb(127,39,4)']],\n",
       "              'hoverinfo': 'skip',\n",
       "              'showscale': False,\n",
       "              'type': 'heatmap',\n",
       "              'uid': 'e89fb7cd-703e-427d-a04b-931963c47aa4',\n",
       "              'z': {'bdata': ('5QAQAAAAAAAGAAQAigEmAEIBAACUAD' ... 'AA0wMAAAAAAAAAAAAAAAAAAAEAAAA='),\n",
       "                    'dtype': 'i2',\n",
       "                    'shape': '10, 10'},\n",
       "              'zmax': np.int64(1050),\n",
       "              'zmin': 0},\n",
       "             {'colorbar': {'len': 1, 'lenmode': 'fraction'},\n",
       "              'colorscale': [[0.0, 'rgb(255,245,235)'], [0.125,\n",
       "                             'rgb(254,230,206)'], [0.25, 'rgb(253,208,162)'],\n",
       "                             [0.375, 'rgb(253,174,107)'], [0.5, 'rgb(253,141,60)'],\n",
       "                             [0.625, 'rgb(241,105,19)'], [0.75, 'rgb(217,72,1)'],\n",
       "                             [0.875, 'rgb(166,54,3)'], [1.0, 'rgb(127,39,4)']],\n",
       "              'hovertemplate': ('<b>count: %{z}</b><br>ground_t' ... 'ification: %{x}<extra></extra>'),\n",
       "              'opacity': 0.25,\n",
       "              'type': 'heatmap',\n",
       "              'uid': '4c939ad7-97f0-4694-8933-4c4feb8298f1',\n",
       "              'z': {'bdata': ('5QAQAAAAAAAGAAQAigEmAEIBAACUAD' ... 'AA0wMAAAAAAAAAAAAAAAAAAAEAAAA='),\n",
       "                    'dtype': 'i2',\n",
       "                    'shape': '10, 10'},\n",
       "              'zmax': np.int64(1050),\n",
       "              'zmin': 0}],\n",
       "    'layout': {'clickmode': 'event',\n",
       "               'margin': {'b': 0, 'l': 0, 'r': 0, 't': 30},\n",
       "               'template': '...',\n",
       "               'title': {},\n",
       "               'xaxis': {'constrain': 'domain',\n",
       "                         'range': [-0.5, 9.5],\n",
       "                         'tickmode': 'array',\n",
       "                         'ticktext': [0 - zero, 1 - one, 2 - two, 3 - three, 4 -\n",
       "                                      four, 5 - five, 6 - six, 7 - seven, 8 -\n",
       "                                      eight, 9 - nine],\n",
       "                         'tickvals': {'bdata': 'AAECAwQFBgcICQ==', 'dtype': 'i1'}},\n",
       "               'yaxis': {'constrain': 'domain',\n",
       "                         'range': [-0.5, 9.5],\n",
       "                         'scaleanchor': 'x',\n",
       "                         'scaleratio': 1,\n",
       "                         'tickmode': 'array',\n",
       "                         'ticktext': array(['9 - nine', '8 - eight', '7 - seven', '6 - six', '5 - five', '4 - four',\n",
       "                                            '3 - three', '2 - two', '1 - one', '0 - zero'], dtype=object),\n",
       "                         'tickvals': {'bdata': 'AAECAwQFBgcICQ==', 'dtype': 'i1'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_evaluation_results.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up\n",
    "\n",
    "Before moving on, let's clear the CLIP model from memory to free up GPU resources for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clip_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"CLIP model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have successfully performed zero-shot classification and evaluated the results. This provides a strong baseline for comparison.\n",
    "\n",
    "Next, we will build and train a traditional supervised learning model, LeNet-5, to see how a task-specific model compares.\n",
    "\n",
    "Proceed to `4_lenet_training.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
