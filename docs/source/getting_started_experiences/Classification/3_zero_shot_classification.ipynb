{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Zero-Shot Classification with CLIP\n",
    "\n",
    "Now that we understand how CLIP creates meaningful embeddings, we can leverage them for **zero-shot classification**â€”classifying images into categories without any task-specific training. CLIP maps images and text to the same embedding space, allowing us to classify an image by finding the text description with the most similar embedding.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Zero-shot classification principles\n",
    "*   Text prompts for classification\n",
    "*   Applying a model to a FiftyOne dataset\n",
    "*   Evaluating classification results including accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's import our libraries and load the test dataset, which should now contain the CLIP embeddings from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import gc\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Ensure the test dataset exists\n",
    "if \"mnist-test-set\" in fo.list_datasets():\n",
    "    test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "else:\n",
    "    print(\"Test dataset not found. Please run '1_explore_mnist.ipynb' and '2_clip_embeddings.ipynb' first.\")\n",
    "\n",
    "session = fo.launch_app(test_dataset, auto=False)\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performing Zero-Shot Classification\n",
    "\n",
    "First, we get the distinct class labels from our dataset. For MNIST, these are the digits 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
    "dataset_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Effect of the Text Prompt\n",
    "\n",
    "With CLIP, the `text_prompt` significantly affects accuracy. It provides context for the class labels. We'll use a simple prompt, but feel free to experiment with others like \"A grayscale image of the number\" or \"An MNIST digit\" to see how performance changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of \",\n",
    "    classes=dataset_classes,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply the model to our dataset. The `apply_model()` method will iterate through the samples, generate a prediction for each one, and store it in a new field. We set `store_logits=True` to save the model's raw output scores, which are useful for later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.apply_model(\n",
    "    model=clip_model,\n",
    "    label_field=\"clip_zero_shot_classification\",\n",
    "    store_logits=True,\n",
    "    batch_size=256,\n",
    "    num_workers=os.cpu_count()\n",
    ")\n",
    "\n",
    "session.refresh()\n",
    "print(\"CLIP predictions added to the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating CLIP's Performance\n",
    "\n",
    "Now that we have predictions, we can evaluate them against the ground truth labels. FiftyOne's `evaluate_classifications()` method provides a comprehensive report and a confusion matrix.\n",
    "\n",
    "You can also view these results interactively in the FiftyOne App's [Model Evaluation Panel](https://docs.voxel51.com/user_guide/app.html#model-evaluation-panel-sub-new)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_evaluation_results = test_dataset.evaluate_classifications(\n",
    "    \"clip_zero_shot_classification\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"clip_zero_shot_eval\")\n",
    "\n",
    "session.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print a classification report. We can see the overall accuracy is around 88%, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_evaluation_results.print_report(classes=dataset_classes, digits=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows which classes are most often confused. A perfect model would have values only on the main diagonal. Here, we see significant confusion between certain digits (like 4s and 9s, or 3s and 8s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_evaluation_results.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up\n",
    "\n",
    "Before moving on, let's clear the CLIP model from memory to free up GPU resources for the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del clip_model\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"CLIP model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have successfully performed zero-shot classification and evaluated the results. This provides a strong baseline for comparison.\n",
    "\n",
    "Next, we will build and train a traditional supervised learning model, LeNet-5, to see how a task-specific model compares.\n",
    "\n",
    "Proceed to `4_lenet_training.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}