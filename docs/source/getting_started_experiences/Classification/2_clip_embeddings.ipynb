{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image Embeddings with CLIP\n",
    "\n",
    "In this notebook, we'll generate image embeddings for the MNIST test set using a pre-trained CLIP model. Embeddings are powerful vector representations that capture the semantic content of images. We will then use dimensionality reduction techniques (PCA and UMAP) to visualize these high-dimensional vectors in 2D, allowing us to visually explore the structure of our dataset.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Loading pre-trained models from the FiftyOne Model Zoo\n",
    "*   Computing image embeddings with CLIP\n",
    "*   Assigning embeddings to dataset samples\n",
    "*   Dimensionality reduction: PCA and UMAP\n",
    "*   Visualizing embedding plots in FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's add our imports and load the test dataset we created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to FiftyOne on port 5151 at 0.0.0.0.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "# Ensure the test dataset exists\n",
    "if \"mnist-test-set\" in fo.list_datasets():\n",
    "    test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "else:\n",
    "    print(\"Test dataset not found. Please run '1_explore_mnist.ipynb' first.\")\n",
    "\n",
    "session = fo.launch_app(test_dataset, auto=False)\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Image Embeddings with CLIP\n",
    "\n",
    "Image embeddings are high-dimensional vectors that translate visual concepts into a format that machine learning models can compare. Similar images will have similar embedding vectors.\n",
    "\n",
    "We will use a pre-trained CLIP model from the FiftyOne Model Zoo to generate these embeddings. FiftyOne makes this process simple with the `compute_embeddings()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading model from 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'...\n",
      " 100% |██████|    2.6Gb/2.6Gb [29.9s elapsed, 0s remaining, 100.9Mb/s]     \n",
      "Downloading CLIP tokenizer...\n",
      " 100% |█████|   10.4Mb/10.4Mb [207.0ms elapsed, 0s remaining, 50.0Mb/s]      \n",
      "The model is loaded on cuda\n",
      "The CLIP model has 151,277,313 parameters.\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\",\n",
    "                                device=device)\n",
    "print(f\"The model is loaded on {clip_model._device}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in clip_model._model.parameters())\n",
    "print(f\"The CLIP model has {total_params:,} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [25.3s elapsed, 0s remaining, 474.4 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# This will take about 3 min on a Google Colab instance with GPU enabled\n",
    "clip_embeddings = test_dataset.compute_embeddings(model=clip_model,\n",
    "                                        batch_size=512,\n",
    "                                        num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a NumPy array where each row is a 512-dimensional vector representing an image. Now, we'll attach each embedding to its corresponding sample in the FiftyOne dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> (10000, 512)\n"
     ]
    }
   ],
   "source": [
    "# Check the format and shape of our embeddings n-dimensional array.\n",
    "# We should have 10000 embeddings, each with 512 dimensions.\n",
    "print(type(clip_embeddings), clip_embeddings.shape)\n",
    "\n",
    "test_dataset.add_sample_field(\"clip_embeddings\", fo.VectorField)\n",
    "test_dataset.set_values(\"clip_embeddings\", clip_embeddings)\n",
    "# We save the values to the dataset \n",
    "test_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Similarity Index\n",
    "\n",
    "A **similarity index** allows for efficient searching of similar samples based on their embeddings. Instead of a slow, brute-force search, the index organizes embeddings for fast retrieval. We'll build one on our new CLIP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity index computed successfully!\n"
     ]
    }
   ],
   "source": [
    "similarity_index = fob.compute_similarity(\n",
    "    test_dataset,\n",
    "    model=\"clip-vit-base32-torch\",\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    brain_key=\"clip_cosine_similarity_index\",\n",
    "    backend=\"sklearn\",\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "print(\"Similarity index computed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily find the most similar images to any given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying for images similar to sample: 686921ec5484ada2e34aea0c with label 7 - seven\n",
      "Found 10 most similar samples. Check the App: http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "query_sample = test_dataset.first()\n",
    "print(f\"Querying for images similar to sample: {query_sample.id} with label {query_sample.ground_truth.label}\")\n",
    "\n",
    "similar_view = test_dataset.sort_by_similarity(\n",
    "    query_sample.id,\n",
    "    brain_key=\"clip_cosine_similarity_index\",\n",
    "    k=10\n",
    ")\n",
    "\n",
    "session.view = similar_view\n",
    "session.refresh()\n",
    "print(f\"Found {len(similar_view)} most similar samples. Check the App: {session.url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save individual views with descriptive names and query them in the app\n",
    "test_dataset.save_view(f\"Images similar to {query_sample.id}\", similar_view)\n",
    "session.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/assets/similar_to_query_id.webp?raw=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [CLIP model](https://docs.voxel51.com/model_zoo/models.html#clip-vit-base32-torch) has an important feature: it supports text prompts, meaning that we can search for images given a text query. We will go deeper into this in the next notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "text_query = \"the digit five\"\n",
    "\n",
    "similar_to_5_view = test_dataset.sort_by_similarity(text_query, k=5)\n",
    "session.view = similar_to_5_view\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/assets/text_query_five.webp?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a 2D Projection of the Embedding Space\n",
    "\n",
    "Our 512-dimensional embeddings are impossible to visualize directly. We use dimensionality reduction techniques like **PCA** and **UMAP** to project them into 2D space. This is a lossy compression, but it helps us visually identify clusters, outliers, and patterns in the data.\n",
    "\n",
    "- **PCA (Principal Component Analysis)**: A linear method that preserves global variance.\n",
    "- **UMAP (Uniform Manifold Approximation and Projection)**: A non-linear method that excels at preserving local neighborhood structures and revealing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Sat Jul  5 14:06:58 2025 Construct fuzzy simplicial set\n",
      "Sat Jul  5 14:06:58 2025 Finding Nearest Neighbors\n",
      "Sat Jul  5 14:06:58 2025 Building RP forest with 10 trees\n",
      "Sat Jul  5 14:07:02 2025 NN descent for 13 iterations\n",
      "\t 1  /  13\n",
      "\t 2  /  13\n",
      "\t 3  /  13\n",
      "\t 4  /  13\n",
      "\tStopping threshold met -- exiting after 4 iterations\n",
      "Sat Jul  5 14:07:14 2025 Finished Nearest Neighbor Search\n",
      "Sat Jul  5 14:07:16 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe9cb50483e4bbe8100378e185ea9a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Sat Jul  5 14:08:16 2025 Finished embedding\n",
      "PCA and UMAP visualizations computed.\n"
     ]
    }
   ],
   "source": [
    "pca_visualization = fob.compute_visualization(test_dataset,\n",
    "                                              method=\"pca\",\n",
    "                                              embeddings=\"clip_embeddings\",\n",
    "                                              num_dims=2,\n",
    "                                              brain_key=\"pca_visualization_clip_embeds\")\n",
    "\n",
    "umap_visualization = fob.compute_visualization(test_dataset,\n",
    "                                              method=\"umap\",\n",
    "                                              embeddings=\"clip_embeddings\",\n",
    "                                              num_dims=2,\n",
    "                                              brain_key=\"umap_visualization_clip_embeds\")\n",
    "\n",
    "# Refresh the session to show the new visualizations\n",
    "session.refresh()\n",
    "print(\"PCA and UMAP visualizations computed.\")\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings in the FiftyOne App\n",
    "\n",
    "To see your new 2D projections:\n",
    "\n",
    "1. In the App, click the **`+`** icon next to \"Samples\".\n",
    "2. Select **Embeddings**.\n",
    "3. Choose `pca_visualization_clip_embeds` or `umap_visualization_clip_embeds` from the dropdown.\n",
    "\n",
    "You can now interact with the plot. Try coloring the points by `ground_truth.label` to see if CLIP's embeddings naturally separate the digits into clusters.\n",
    "\n",
    "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/image_embeddings_zero_cluster.gif?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Compute the T-SNE visualization of the embeddings with:\n",
    "\n",
    "```python\n",
    "tsne_visualization = fob.compute_visualization(test_dataset,\n",
    "                                              method=\"tsne\",\n",
    "                                              embeddings=\"clip_embeddings\",\n",
    "                                              num_dims=2,\n",
    "                                              brain_key=\"tsne_visualization_clip_embeds\")\n",
    "```\n",
    "and compare the results in the FiftyOne app. Refer to the [documentation](https://docs.voxel51.com/api/fiftyone.brain.html#fiftyone.brain.compute_visualization) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With our dataset enriched with CLIP embeddings and visualizations, we're ready to use these features for a classification task.\n",
    "\n",
    "Proceed to `3_zero_shot_classification.ipynb` to perform zero-shot classification with CLIP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
