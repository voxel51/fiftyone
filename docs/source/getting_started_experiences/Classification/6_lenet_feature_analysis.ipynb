{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Analysis of LeNet-5 Learned Features\n",
    "\n",
    "We've seen that our LeNet model performs very well, but *what* has it learned? In this notebook, we'll peek inside the model by extracting embeddings from its intermediate layers. Unlike CLIP's general-purpose embeddings, these are feature representations specifically optimized for MNIST digit classification.\n",
    "\n",
    "We will analyze these embeddings on the *training data* to understand how the model organizes the concepts it has learned and to identify interesting samples within our training set.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Extracting embeddings from intermediate PyTorch model layers\n",
    "*   Storing custom model embeddings in FiftyOne\n",
    "*   Visualizing custom embeddings with PCA and UMAP\n",
    "*   Analyzing uniqueness and representativeness of training samples\n",
    "\n",
    "![](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/assets/pca_lenet_embeddings.webp?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we'll set up our environment, reloading the necessary datasets, model definitions, and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Redefine the model architecture\n",
    "class ModernLeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModernLeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(Fun.relu(self.conv1(x)))\n",
    "        x = self.pool(Fun.relu(self.conv2(x)))\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = Fun.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Redefine the custom dataset class\n",
    "class CustomTorchImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fiftyone_dataset, image_transforms=None, label_map=None, gt_field=\"ground_truth\"):\n",
    "        self.fiftyone_dataset = fiftyone_dataset\n",
    "        self.image_paths = self.fiftyone_dataset.values(\"filepath\")\n",
    "        self.str_labels = self.fiftyone_dataset.values(f\"{gt_field}.label\")\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_map = label_map if label_map is not None else {str(i): i for i in range(10)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        if self.image_transforms: image = self.image_transforms(image)\n",
    "        label_str = self.str_labels[idx]\n",
    "        label_idx = self.label_map.get(label_str, -1)\n",
    "        return image, torch.tensor(label_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings from the LeNet-5 Model\n",
    "\n",
    "We'll use a **PyTorch hook** to capture the output of an intermediate layer during a forward pass. A hook is a function that can be registered to a module and will be executed during the forward or backward pass. We'll grab the 84-dimensional output of the `fc1` layer, which serves as a rich feature embedding just before the final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lenet_embeddings(model, dataloader, device, layer_name='fc1'):\n",
    "    \"\"\"Extracts embeddings from a specified layer of the LeNet model using PyTorch hooks.\"\"\"\n",
    "    embeddings_dict = {}\n",
    "\n",
    "    def hook_fn(module, input, output):\n",
    "        if len(output.shape) > 2:\n",
    "            embeddings_dict['embeddings'] = output.view(output.size(0), -1).cpu().detach()\n",
    "        else:\n",
    "            embeddings_dict['embeddings'] = output.cpu().detach()\n",
    "\n",
    "    target_layer = getattr(model, layer_name)\n",
    "    hook_handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "    model.eval()\n",
    "    all_embeddings = []\n",
    "    with torch.inference_mode():\n",
    "        for images, _ in tqdm(dataloader, desc=f\"Extracting {layer_name} embeddings\"):\n",
    "            _ = model(images.to(device))\n",
    "            all_embeddings.append(embeddings_dict['embeddings'].numpy())\n",
    "    \n",
    "    hook_handle.remove()\n",
    "    return np.concatenate(all_embeddings, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll apply this function to our training dataset. We need a `DataLoader` for the training set **without shuffling** to ensure the extracted embeddings correctly map back to the samples in our FiftyOne dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting fc1 embeddings:  77%|███████▋  | 617/797 [00:07<00:01, 115.83it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting fc1 embeddings: 100%|██████████| 797/797 [00:09<00:00, 84.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet embeddings stored in the training dataset.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = fo.load_dataset(\"mnist-training-set\")\n",
    "model_save_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
    "loaded_model = ModernLeNet5().to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "# Recreate transforms\n",
    "mean_intensity, std_intensity = 0.1307, 0.3081 # Pre-computed\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
    "])\n",
    "dataset_classes = sorted(train_dataset.distinct(\"ground_truth.label\"))\n",
    "label_map = {label: i for i, label in enumerate(dataset_classes)}\n",
    "\n",
    "# Create a non-shuffled DataLoader for the training set\n",
    "torch_train_set = CustomTorchImageDataset(train_dataset, image_transforms=image_transforms, label_map=label_map)\n",
    "train_inference_loader = torch.utils.data.DataLoader(torch_train_set, batch_size=64, shuffle=False, num_workers=os.cpu_count())\n",
    "\n",
    "# Extract embeddings\n",
    "lenet_embeddings = extract_lenet_embeddings(loaded_model, train_inference_loader, device, 'fc1')\n",
    "\n",
    "# Store embeddings in the FiftyOne dataset\n",
    "train_dataset.set_values(\"lenet_embeddings\", lenet_embeddings)\n",
    "train_dataset.save()\n",
    "print(\"LeNet embeddings stored in the training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Learned Embeddings\n",
    "\n",
    "Just as we did with CLIP, we can compute and visualize 2D projections of our new LeNet embeddings. This will show us how our custom-trained model has learned to separate the different digit classes in its own feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP( verbose=True)\n",
      "Sun Jul  6 16:11:59 2025 Construct fuzzy simplicial set\n",
      "Sun Jul  6 16:11:59 2025 Finding Nearest Neighbors\n",
      "Sun Jul  6 16:11:59 2025 Building RP forest with 16 trees\n",
      "Sun Jul  6 16:11:59 2025 NN descent for 16 iterations\n",
      "\t 1  /  16\n",
      "\t 2  /  16\n",
      "\t 3  /  16\n",
      "\tStopping threshold met -- exiting after 3 iterations\n",
      "Sun Jul  6 16:12:10 2025 Finished Nearest Neighbor Search\n",
      "Sun Jul  6 16:12:11 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98a9b214a7d141519617b2f9d2c0abf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/200 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  200 epochs\n",
      "\tcompleted  20  /  200 epochs\n",
      "\tcompleted  40  /  200 epochs\n",
      "\tcompleted  60  /  200 epochs\n",
      "\tcompleted  80  /  200 epochs\n",
      "\tcompleted  100  /  200 epochs\n",
      "\tcompleted  120  /  200 epochs\n",
      "\tcompleted  140  /  200 epochs\n",
      "\tcompleted  160  /  200 epochs\n",
      "\tcompleted  180  /  200 epochs\n",
      "Sun Jul  6 16:14:58 2025 Finished embedding\n",
      "Generating visualization...\n",
      "Session launched. Run `session.show()` to open the App in a cell output.\n",
      "UMAP and PCA visualizations for LeNet embeddings are ready in the App.\n",
      "http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "fob.compute_visualization(\n",
    "    train_dataset,\n",
    "    embeddings=\"lenet_embeddings\",\n",
    "    num_dims=2,\n",
    "    method=\"umap\",\n",
    "    brain_key=\"umap_lenet_embeddings\"\n",
    ")\n",
    "\n",
    "fob.compute_visualization(\n",
    "    train_dataset,\n",
    "    embeddings=\"lenet_embeddings\",\n",
    "    num_dims=2,\n",
    "    method=\"pca\",\n",
    "    brain_key=\"pca_lenet_embeddings\"\n",
    ")\n",
    "\n",
    "session = fo.launch_app(train_dataset, auto=False)\n",
    "print(\"UMAP and PCA visualizations for LeNet embeddings are ready in the App.\")\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the **Embeddings** panel in the App and select `umap_lenet_embeddings`. Color the points by `ground_truth.label`. You should see remarkably clean and well-separated clusters for each digit, demonstrating that our model has learned very effective features for this task.\n",
    "\n",
    "#### UMAP 2D Projection of the LeNet Embedding Space\n",
    "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/umap_lenet_embeddings.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uniqueness and Representativeness\n",
    "\n",
    "Using these new embeddings, we can compute **uniqueness** and **representativeness** scores for our training samples. \n",
    "\n",
    "- **Uniqueness**: Identifies outliers or edge cases that are far from any cluster center.\n",
    "- **Representativeness**: Identifies samples that are archetypes of their class, sitting right at the center of their clusters.\n",
    "\n",
    "These metrics are invaluable for data cleaning and for finding the most informative samples to use for tasks like data augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing uniqueness...\n",
      "Uniqueness computation complete\n",
      "Computing representativeness...\n",
      "Computing clusters for 51000 embeddings; this may take awhile...\n",
      "Representativeness computation complete\n",
      "Uniqueness and representativeness scores computed.\n"
     ]
    }
   ],
   "source": [
    "fob.compute_uniqueness(train_dataset, embeddings='lenet_embeddings')\n",
    "fob.compute_representativeness(train_dataset, embeddings='lenet_embeddings')\n",
    "\n",
    "session.refresh()\n",
    "print(\"Uniqueness and representativeness scores computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find the most unique samples in our training set. These are often the most interesting, sometimes revealing oddities or labeling errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 51 most unique training samples in the App: http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "uniqueness_quantiles = train_dataset.quantiles(\"uniqueness\", [0.999])\n",
    "\n",
    "most_unique_samples_view = train_dataset.match(\n",
    "                             F(\"uniqueness\") > uniqueness_quantiles[-1]\n",
    "                             ).sort_by(\"uniqueness\", reverse=True)\n",
    "\n",
    "session.view = most_unique_samples_view\n",
    "print(f\"Displaying {len(most_unique_samples_view)} most unique training samples in the App: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/assets/51_most_unique_samples.webp?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create views of highly unique and highly representative samples grouped by `ground_truth.label`, use descending sort by `uniqueness` and `representativeness` to produce them. \n",
    "2. Create a list of images that you would like to **remove** from the training set due to their quality. Use the `id` or `filepath` primitives to select them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have now deeply analyzed our training data and our model's learned features. We have identified unique, representative, and potentially problematic samples.\n",
    "\n",
    "The final step is to use this knowledge to improve our model. We will perform targeted data augmentation on the samples our model struggles with and then fine-tune it.\n",
    "\n",
    "Proceed to `7_retraining_with_augmentation.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
