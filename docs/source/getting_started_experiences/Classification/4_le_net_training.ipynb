{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Supervised Classification: LeNet-5 with PyTorch\n",
    "\n",
    "While zero-shot classification with CLIP is powerful, building and training a model from scratch is a fundamental skill. In this notebook, we'll implement **LeNet-5**, one of the earliest and most influential Convolutional Neural Networks (CNNs). We will train it on the MNIST training dataset using PyTorch.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   LeNet-5 architecture\n",
    "*   Defining a custom `nn.Module` in PyTorch\n",
    "*   Splitting FiftyOne data for training and validation\n",
    "*   Creating custom PyTorch Datasets from FiftyOne views\n",
    "*   Data normalization through mean/standard deviation\n",
    "*   Using PyTorch DataLoaders\n",
    "*   Implementing a training loop and model checkpointing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "As always, we start with imports and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.utils.random as four\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "To ensure our training experiments are reproducible, we'll define a function to set random seeds for all relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=51):\n",
    "    \"\"\"Sets seeds for reproducibility.\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    cv2.setRNGSeed(seed)\n",
    "    try:\n",
    "        A.seed_everything(seed)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    try:\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "    except RuntimeError:\n",
    "        print(\"Warning: Some operations may not be deterministic\")\n",
    "\n",
    "def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n",
    "    \"\"\"Creates a DataLoader with deterministic behavior.\"\"\"\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(51)\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        generator=generator if shuffle else None,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "set_seeds(51)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the LeNet-5 Architecture\n",
    "\n",
    "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/lenet5-architecture.png)\n",
    "\n",
    "We'll define a modernized version of LeNet-5 using `ReLU` activations and `MaxPooling`, which generally perform better than the original `tanh` and `AveragePooling`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModernLeNet5(nn.Module):\n",
    "    \"\"\"\n",
    "    Modernized version of LeNet-5 with ReLU activations and max pooling.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModernLeNet5, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(Fun.relu(self.conv1(x)))\n",
    "        x = self.pool(Fun.relu(self.conv2(x)))\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = Fun.relu(self.fc1(x))\n",
    "        x = self.dropout(x)  # Add dropout for regularization\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Data\n",
    "\n",
    "We'll now load the `train` split of MNIST and divide it into a training set (85%) and a validation set (15%). The validation set is crucial for monitoring overfitting and for saving the best version of our model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_dataset = foz.load_zoo_dataset(\"mnist\",\n",
    "                                         split='train',\n",
    "                                         dataset_name=\"mnist-train-val\",\n",
    "                                         persistent=True)\n",
    "\n",
    "# Ensure tags from previous runs are cleared\n",
    "train_val_dataset.untag_samples([\"train\", \"validation\"])\n",
    "\n",
    "set_seeds(51)\n",
    "four.random_split(train_val_dataset,\n",
    "                  {\"train\": 0.85, \"validation\": 0.15},\n",
    "                  seed=51)\n",
    "\n",
    "train_dataset = train_val_dataset.match_tags(\"train\").clone(name=\"mnist-training-set\", persistent=True)\n",
    "val_dataset = train_val_dataset.match_tags(\"validation\").clone(name=\"mnist-validation-set\", persistent=True)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Custom PyTorch Dataset\n",
    "\n",
    "To use our FiftyOne datasets with PyTorch, we create a custom `Dataset` class. This acts as a bridge, allowing PyTorch's `DataLoader` to efficiently load images and labels while we still benefit from FiftyOne's powerful data management features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTorchImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fiftyone_dataset,\n",
    "                 image_transforms=None,\n",
    "                 label_map=None,\n",
    "                 gt_field=\"ground_truth\"):\n",
    "        self.fiftyone_dataset = fiftyone_dataset\n",
    "        self.image_paths = self.fiftyone_dataset.values(\"filepath\")\n",
    "        self.str_labels = self.fiftyone_dataset.values(f\"{gt_field}.label\")\n",
    "        self.image_transforms = image_transforms\n",
    "\n",
    "        if label_map is None:\n",
    "            self.label_map = {str(i): i for i in range(10)}  # \"0\"->0, \"1\"->1, etc.\n",
    "        else:\n",
    "            self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')\n",
    "\n",
    "        if self.image_transforms:\n",
    "            image = self.image_transforms(image)\n",
    "\n",
    "        label_str = self.str_labels[idx]\n",
    "        label_idx = self.label_map.get(label_str, -1)\n",
    "        return image, torch.tensor(label_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Normalization\n",
    "\n",
    "Normalizing input data to have a mean of 0 and a standard deviation of 1 is a critical preprocessing step. It helps stabilize training and allows the model to converge faster. We'll compute these statistics on our training set and apply the same normalization to all splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_stats_fiftyone(fiftyone_view):\n",
    "    filepaths = fiftyone_view.values(\"filepath\")\n",
    "    all_pixels = []\n",
    "    for filepath in tqdm(filepaths, desc=\"Computing Stats\"):\n",
    "        image = Image.open(filepath).convert('L')\n",
    "        pixels = np.array(image, dtype=np.float32) / 255.0\n",
    "        all_pixels.append(pixels.flatten())\n",
    "    all_pixels = np.concatenate(all_pixels)\n",
    "    return np.mean(all_pixels), np.std(all_pixels)\n",
    "\n",
    "mean_intensity, std_intensity = compute_stats_fiftyone(train_dataset)\n",
    "print(f\"Mean: {mean_intensity:.4f}, Std: {std_intensity:.4f}\")\n",
    "\n",
    "# Define transforms with normalization\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
    "])\n",
    "\n",
    "# Create label map and torch datasets\n",
    "dataset_classes = sorted(train_val_dataset.distinct(\"ground_truth.label\"))\n",
    "label_map = {label: i for i, label in enumerate(dataset_classes)}\n",
    "\n",
    "torch_train_set = CustomTorchImageDataset(train_dataset, label_map=label_map, image_transforms=image_transforms)\n",
    "torch_val_set = CustomTorchImageDataset(val_dataset, label_map=label_map, image_transforms=image_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch DataLoaders\n",
    "\n",
    "`DataLoaders` wrap our datasets and handle batching, shuffling, and parallel data loading, which are essential for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = os.cpu_count()\n",
    "\n",
    "train_loader = create_deterministic_training_dataloader(\n",
    "    torch_train_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    torch_val_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False, # No need to shuffle validation data\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "print(\"DataLoaders created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We'll now set up the training loop. This involves defining a loss function, an optimizer, and functions to handle one epoch of training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = ModernLeNet5().to(device)\n",
    "optimizer = Adam(model.parameters(), lr=0.003)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "def train_epoch(model, train_loader, optimizer, ce_loss):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    for images, labels in tqdm(train_loader, desc=\"Training\"): \n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(images)\n",
    "        loss = ce_loss(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        batch_losses.append(loss.item())\n",
    "    return np.mean(batch_losses)\n",
    "\n",
    "def val_epoch(model, val_loader, ce_loss):\n",
    "    model.eval()\n",
    "    batch_losses = []\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in tqdm(val_loader, desc=\"Validation\"): \n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = model(images)\n",
    "            loss = ce_loss(logits, labels)\n",
    "            batch_losses.append(loss.item())\n",
    "    return np.mean(batch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop with Checkpointing\n",
    "\n",
    "We'll train for several epochs. Crucially, we will monitor the validation loss after each epoch and save the model's weights only when the validation loss improves. This practice, known as **checkpointing**, ensures we keep the model that generalizes best, protecting us from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(51)\n",
    "num_epochs = 10\n",
    "train_losses, val_losses = [], []\n",
    "best_val_loss = float('inf')\n",
    "model_save_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(model, train_loader, optimizer, ce_loss)\n",
    "    val_loss = val_epoch(model, val_loader, ce_loss)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print('âœ“ Found and saved better model weights.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Training Progress\n",
    "\n",
    "Plotting the training and validation losses helps us diagnose the training process. In an ideal scenario, both losses decrease, and the validation loss remains close to the training loss. If the validation loss starts to increase while the training loss continues to decrease, the model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "best_epoch = np.argmin(val_losses) + 1\n",
    "plt.axvline(x=best_epoch-1, color='r', linestyle='--', label=f'Best Model @ Epoch {best_epoch}')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We have successfully trained a LeNet-5 model and saved its best-performing weights.\n",
    "\n",
    "Now, let's evaluate this model on the unseen test set to see how well it performs.\n",
    "\n",
    "Proceed to `5_lenet_evaluation.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}