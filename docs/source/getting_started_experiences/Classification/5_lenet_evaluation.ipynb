{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LeNet-5 Model Evaluation on Test Data\n",
    "\n",
    "After training our LeNet-5 model, the next crucial step is to evaluate its performance on the unseen test set. This gives us an unbiased measure of how well our model generalizes to new data. We will apply the model, store its predictions in FiftyOne, and then use FiftyOne's powerful evaluation tools to analyze the results in detail.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Applying a PyTorch model to a FiftyOne dataset\n",
    "*   Storing predictions, confidence, and logits\n",
    "*   Evaluating classification performance\n",
    "*   Analyzing prediction confidence distributions\n",
    "*   Computing sample hardness and mistakenness\n",
    "\n",
    "![](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/assets/mistakenness_test_set_decreasing.png?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "We need to reload our datasets and redefine our model architecture and helper classes to apply the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "from fiftyone import ViewField as F\n",
    "\n",
    "# Redefine the model architecture so we can load the weights\n",
    "class ModernLeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModernLeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(Fun.relu(self.conv1(x)))\n",
    "        x = self.pool(Fun.relu(self.conv2(x)))\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = Fun.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Redefine the custom dataset class\n",
    "class CustomTorchImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, fiftyone_dataset, image_transforms=None, label_map=None, gt_field=\"ground_truth\"):\n",
    "        self.fiftyone_dataset = fiftyone_dataset\n",
    "        self.image_paths = self.fiftyone_dataset.values(\"filepath\")\n",
    "        self.str_labels = self.fiftyone_dataset.values(f\"{gt_field}.label\")\n",
    "        self.image_transforms = image_transforms\n",
    "        self.label_map = label_map if label_map is not None else {str(i): i for i in range(10)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('L')\n",
    "        if self.image_transforms: image = self.image_transforms(image)\n",
    "        label_str = self.str_labels[idx]\n",
    "        label_idx = self.label_map.get(label_str, -1)\n",
    "        return image, torch.tensor(label_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reload the Best LeNet Model\n",
    "\n",
    "We load the saved model weights that achieved the best validation performance during training. This ensures we are evaluating the most generalizable version of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from /root/workspace/fiftyone/docs/source/getting_started_experiences/Classification/best_lenet.pth\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_save_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
    "\n",
    "loaded_model = ModernLeNet5().to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "loaded_model.eval()\n",
    "\n",
    "print(f\"Model loaded successfully from {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the Model to the Test Set\n",
    "\n",
    "We'll now run inference on the entire test set. We'll collect the predictions, confidence scores, and raw logits for each sample and store them back into our FiftyOne dataset. Storing predictions as structured `fo.Classification` objects allows for rich and interactive analysis, such as retrieving the samples where the labeling confidence is within querying thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/157 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [00:04<00:00, 33.71it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0% |/------------|    23/10000 [112.9ms elapsed, 49.0s remaining, 203.7 samples/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [23.0s elapsed, 0s remaining, 438.1 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\teta.core.utils:utils.py:_draw()-  100% |█████████████| 10000/10000 [23.0s elapsed, 0s remaining, 438.1 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "train_dataset = fo.load_dataset(\"mnist-training-set\")\n",
    "\n",
    "# Recreate transforms using stats from the training set\n",
    "mean_intensity, std_intensity = 0.1307, 0.3081 # Pre-computed for simplicity\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
    "])\n",
    "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
    "label_map = {label: i for i, label in enumerate(dataset_classes)}\n",
    "\n",
    "# Create DataLoader for the test set\n",
    "torch_test_set = CustomTorchImageDataset(test_dataset, image_transforms=image_transforms, label_map=label_map)\n",
    "test_loader = torch.utils.data.DataLoader(torch_test_set, batch_size=64, num_workers=os.cpu_count())\n",
    "\n",
    "# Run inference\n",
    "predictions, all_logits = [], []\n",
    "with torch.inference_mode():\n",
    "    for images, _ in tqdm(test_loader):\n",
    "        images = images.to(device)\n",
    "        logits = loaded_model(images)\n",
    "        all_logits.append(logits.cpu().numpy())\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "all_logits = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "# Store predictions back in FiftyOne\n",
    "with fo.ProgressBar(total=len(test_dataset)) as pb:\n",
    "    for i, sample in enumerate(test_dataset):\n",
    "        pred_idx = predictions[i]\n",
    "        sample_logits = all_logits[i]\n",
    "        conf = float(Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()[pred_idx])\n",
    "        sample[\"lenet_classification\"] = fo.Classification(\n",
    "            label=dataset_classes[pred_idx],\n",
    "            confidence=conf,\n",
    "            logits=sample_logits.tolist()\n",
    "        )\n",
    "        sample.save()\n",
    "        pb.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating LeNet's Performance\n",
    "\n",
    "With predictions stored, we can use `evaluate_classifications()` again to get a performance report and confusion matrix for our custom LeNet model. We expect a significant improvement over CLIP's zero-shot performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\tfiftyone.core.session.session:session.py:launch_app()- Session launched. Run `session.show()` to open the App in a cell output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "session = fo.launch_app(test_dataset, auto=False)\n",
    "lenet_evaluation_results = test_dataset.evaluate_classifications(\n",
    "    \"lenet_classification\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    eval_key=\"lenet_eval\")\n",
    "\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0 - zero      0.996     0.993     0.994       980\n",
      "     1 - one      0.998     0.990     0.994      1135\n",
      "     2 - two      0.988     0.993     0.991      1032\n",
      "   3 - three      0.990     0.988     0.989      1010\n",
      "    4 - four      0.983     0.993     0.988       982\n",
      "    5 - five      0.978     0.990     0.984       892\n",
      "     6 - six      0.997     0.987     0.992       958\n",
      "   7 - seven      0.980     0.991     0.985      1028\n",
      "   8 - eight      0.988     0.989     0.988       974\n",
      "    9 - nine      0.991     0.975     0.983      1009\n",
      "\n",
      "    accuracy                          0.989     10000\n",
      "   macro avg      0.989     0.989     0.989     10000\n",
      "weighted avg      0.989     0.989     0.989     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lenet_evaluation_results.print_report(digits=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy should be >99%, a huge leap from CLIP's ~28%. The confusion matrix is also much cleaner, with most values concentrated on the diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52bb4e368da64f429b8200bf9fab9c20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FigureWidget({\n",
       "    'data': [{'mode': 'markers',\n",
       "              'opacity': 0.1,\n",
       "              'type': 'scatter',\n",
       "              'uid': 'd5774b03-5e0a-4391-a4c4-0db9b1518576',\n",
       "              'x': {'bdata': ('AAECAwQFBgcICQABAgMEBQYHCAkAAQ' ... 'kAAQIDBAUGBwgJAAECAwQFBgcICQ=='),\n",
       "                    'dtype': 'i1'},\n",
       "              'y': {'bdata': ('AAAAAAAAAAAAAAEBAQEBAQEBAQECAg' ... 'cICAgICAgICAgICQkJCQkJCQkJCQ=='),\n",
       "                    'dtype': 'i1'}},\n",
       "             {'colorscale': [[0.0, 'rgb(255,245,235)'], [0.125,\n",
       "                             'rgb(254,230,206)'], [0.25, 'rgb(253,208,162)'],\n",
       "                             [0.375, 'rgb(253,174,107)'], [0.5, 'rgb(253,141,60)'],\n",
       "                             [0.625, 'rgb(241,105,19)'], [0.75, 'rgb(217,72,1)'],\n",
       "                             [0.875, 'rgb(166,54,3)'], [1.0, 'rgb(127,39,4)']],\n",
       "              'hoverinfo': 'skip',\n",
       "              'showscale': False,\n",
       "              'type': 'heatmap',\n",
       "              'uid': '22ffdccc-e23f-49cb-92b5-5b7e4a1bee73',\n",
       "              'z': {'bdata': ('AAAAAAAAAAAKAAcAAAAHAAEA2AMAAA' ... 'AAzQMAAAEAAAAAAAEAAAACAAMAAAA='),\n",
       "                    'dtype': 'i2',\n",
       "                    'shape': '10, 10'},\n",
       "              'zmax': np.int64(1124),\n",
       "              'zmin': 0},\n",
       "             {'colorbar': {'len': 1, 'lenmode': 'fraction'},\n",
       "              'colorscale': [[0.0, 'rgb(255,245,235)'], [0.125,\n",
       "                             'rgb(254,230,206)'], [0.25, 'rgb(253,208,162)'],\n",
       "                             [0.375, 'rgb(253,174,107)'], [0.5, 'rgb(253,141,60)'],\n",
       "                             [0.625, 'rgb(241,105,19)'], [0.75, 'rgb(217,72,1)'],\n",
       "                             [0.875, 'rgb(166,54,3)'], [1.0, 'rgb(127,39,4)']],\n",
       "              'hovertemplate': ('<b>count: %{z}</b><br>ground_t' ... 'ification: %{x}<extra></extra>'),\n",
       "              'opacity': 0.25,\n",
       "              'type': 'heatmap',\n",
       "              'uid': '6e513392-8834-4ccb-a949-9a4cd8f5c41c',\n",
       "              'z': {'bdata': ('AAAAAAAAAAAKAAcAAAAHAAEA2AMAAA' ... 'AAzQMAAAEAAAAAAAEAAAACAAMAAAA='),\n",
       "                    'dtype': 'i2',\n",
       "                    'shape': '10, 10'},\n",
       "              'zmax': np.int64(1124),\n",
       "              'zmin': 0}],\n",
       "    'layout': {'clickmode': 'event',\n",
       "               'margin': {'b': 0, 'l': 0, 'r': 0, 't': 30},\n",
       "               'template': '...',\n",
       "               'title': {},\n",
       "               'xaxis': {'constrain': 'domain',\n",
       "                         'range': [-0.5, 9.5],\n",
       "                         'tickmode': 'array',\n",
       "                         'ticktext': [0 - zero, 1 - one, 2 - two, 3 - three, 4 -\n",
       "                                      four, 5 - five, 6 - six, 7 - seven, 8 -\n",
       "                                      eight, 9 - nine],\n",
       "                         'tickvals': {'bdata': 'AAECAwQFBgcICQ==', 'dtype': 'i1'}},\n",
       "               'yaxis': {'constrain': 'domain',\n",
       "                         'range': [-0.5, 9.5],\n",
       "                         'scaleanchor': 'x',\n",
       "                         'scaleratio': 1,\n",
       "                         'tickmode': 'array',\n",
       "                         'ticktext': array(['9 - nine', '8 - eight', '7 - seven', '6 - six', '5 - five', '4 - four',\n",
       "                                            '3 - three', '2 - two', '1 - one', '0 - zero'], dtype=object),\n",
       "                         'tickvals': {'bdata': 'AAECAwQFBgcICQ==', 'dtype': 'i1'}}}\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lenet_evaluation_results.plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Predictions Side by Side within the FiftyOne App with the Model Evaluation Panel\n",
    "\n",
    "We can go to the Model Evaluation Panel to gain better insights about how the fine-tuned LeNet performs vs CLIP's zero-shot predictions. We go to `+` -> `Model Evaluation` (next to `Samples`) to launch the panel. \n",
    "\n",
    "![](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/assets/launch_model_eval_panel.webp?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardness and Mistakenness\n",
    "\n",
    "Beyond accuracy, we can analyze the model's logits to understand sample-level difficulties.\n",
    "\n",
    "- **Hardness**: Measures the model's prediction uncertainty. High hardness indicates samples the model found difficult, which are often edge cases.\n",
    "- **Mistakenness**: Identifies samples where the model was confident but wrong. High mistakenness can often point to labeling errors in the dataset.\n",
    "\n",
    "We use `fiftyone.brain` to compute these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hardness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\tfiftyone.brain.internal.core.hardness:hardness.py:compute_hardness()- Computing hardness...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1% |-------------|    84/10000 [205.5ms elapsed, 24.3s remaining, 408.8 samples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [14.7s elapsed, 0s remaining, 723.3 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\teta.core.utils:utils.py:_draw()-  100% |█████████████| 10000/10000 [14.7s elapsed, 0s remaining, 723.3 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardness computation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\tfiftyone.brain.internal.core.hardness:hardness.py:compute_hardness()- Hardness computation complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing mistakenness...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\tfiftyone.brain.internal.core.mistakenness:mistakenness.py:compute_mistakenness()- Computing mistakenness...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |█████████████| 10000/10000 [12.6s elapsed, 0s remaining, 762.6 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\teta.core.utils:utils.py:_draw()-  100% |█████████████| 10000/10000 [12.6s elapsed, 0s remaining, 762.6 samples/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mistakenness computation complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO\tfiftyone.brain.internal.core.mistakenness:mistakenness.py:compute_mistakenness()- Mistakenness computation complete\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hardness and mistakenness computed.\n"
     ]
    }
   ],
   "source": [
    "fob.compute_hardness(test_dataset, label_field='lenet_classification')\n",
    "\n",
    "fob.compute_mistakenness(test_dataset, \n",
    "                         pred_field=\"lenet_classification\",\n",
    "                         label_field=\"ground_truth\")\n",
    "\n",
    "session.refresh()\n",
    "print(\"Hardness and mistakenness computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the samples with the highest mistakenness scores. These are the prime candidates for being mislabeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 100 most mistaken samples in the App: http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "mistakenness_quantiles = test_dataset.quantiles(\"mistakenness\", [0.99])\n",
    "\n",
    "suspicious_test_samples_view = test_dataset.match(\n",
    "                             F(\"mistakenness\") > mistakenness_quantiles[-1]\n",
    "                             ).sort_by(\"mistakenness\", reverse=True)\n",
    "\n",
    "session.view = suspicious_test_samples_view\n",
    "print(f\"Displaying {len(suspicious_test_samples_view)} most mistaken samples in the App: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1. Create and save views of samples with high `hardness` and filter them further by `ground_truth.label` and `lenet_classification_label`\n",
    "2. Repeat the process in 1. for `mistakenness`.  Which digits do you find most quirky? What impact do these have on the evaluation of the model?  \n",
    "\n",
    "![](https://github.com/andandandand/fiftyone/blob/develop/docs/source/getting_started_experiences/Classification/assets/weird_test_digit.webp?raw=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "We've thoroughly evaluated our LeNet model on the test set. But what did the model actually *learn*? \n",
    "\n",
    "In the next notebook, we'll dive into the model's internal representations by extracting embeddings from its hidden layers on the *training data*. This will help us understand the features it learned and analyze the training set for quality issues.\n",
    "\n",
    "Proceed to `6_lenet_feature_analysis.ipynb`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
