{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Augmentation and Retraining\n",
    "\n",
    "In this final notebook, we'll use the insights gained from our analysis to improve our model. We will identify challenging samples from the training set—those that were misclassified, are highly unique, or had low prediction confidence. We will then apply targeted **data augmentation** to these samples and fine-tune our LeNet model on this enriched dataset.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Identifying problematic samples for augmentation\n",
    "*   Defining effective data augmentation strategies for MNIST\n",
    "*   Creating a combined dataset of original and augmented data\n",
    "*   Fine-tuning a pre-trained model\n",
    "*   Comparing performance before and after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's begin by setting up our environment, including all necessary imports and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import albumentations as A\n",
    "\n",
    "# Redefine model and dataset classes\n",
    "class ModernLeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModernLeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(Fun.relu(self.conv1(x)))\n",
    "        x = self.pool(Fun.relu(self.conv2(x)))\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = Fun.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n",
    "    generator = torch.Generator().manual_seed(51)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, generator=generator if shuffle else None, **kwargs)\n",
    "\n",
    "def set_seeds(seed=51):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Samples for Augmentation\n",
    "\n",
    "First, we need to get our model's predictions on the training set to identify which samples it misclassified. We'll need to run inference on the training set, similar to how we did for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = fo.load_dataset(\"mnist-training-set\")\n",
    "test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "model_save_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
    "loaded_model = ModernLeNet5().to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# Recreate transforms and dataloaders\n",
    "mean_intensity, std_intensity = 0.1307, 0.3081 # Pre-computed\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
    "])\n",
    "dataset_classes = sorted(train_dataset.distinct(\"ground_truth.label\"))\n",
    "label_map = {label: i for i, label in enumerate(dataset_classes)}\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class CustomTorchImageDataset(Dataset):\n",
    "    def __init__(self, fo_dset, xforms, l_map):\n",
    "        self.fo_dset, self.xforms, self.l_map = fo_dset, xforms, l_map\n",
    "        self.img_paths = self.fo_dset.values(\"filepath\")\n",
    "        self.labels = self.fo_dset.values(\"ground_truth.label\")\n",
    "    def __len__(self): return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert('L')\n",
    "        return self.xforms(img), self.l_map[self.labels[idx]]\n",
    "\n",
    "torch_train_set = CustomTorchImageDataset(train_dataset, image_transforms, label_map)\n",
    "train_inference_loader = torch.utils.data.DataLoader(torch_train_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Add predictions to training set if they don't exist\n",
    "if \"lenet_train_classification\" not in train_dataset.get_field_schema():\n",
    "    train_preds, train_logits = [], []\n",
    "    with torch.inference_mode():\n",
    "        for imgs, _ in tqdm(train_inference_loader, desc=\"Getting train preds\"):\n",
    "            logits = loaded_model(imgs.to(device))\n",
    "            train_logits.append(logits.cpu().numpy())\n",
    "            train_preds.extend(torch.max(logits, 1)[1].cpu().numpy())\n",
    "    train_logits = np.concatenate(train_logits)\n",
    "    \n",
    "    classifications = []\n",
    "    for i in range(len(train_dataset)):\n",
    "        pred_idx = train_preds[i]\n",
    "        logits = train_logits[i]\n",
    "        conf = float(Fun.softmax(torch.tensor(logits), dim=0)[pred_idx])\n",
    "        classifications.append(fo.Classification(label=dataset_classes[pred_idx], confidence=conf, logits=logits.tolist()))\n",
    "    train_dataset.set_values(\"lenet_train_classification\", classifications)\n",
    "    train_dataset.save()\n",
    "\n",
    "# Create view of misclassified training samples\n",
    "mislabeled_train_images_view = train_dataset.match(F(\"lenet_train_classification.label\") != F(\"ground_truth.label\"))\n",
    "print(f\"Found {len(mislabeled_train_images_view)} misclassified training samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Augmentations\n",
    "\n",
    "Effective augmentation for MNIST involves creating realistic variations that a model might encounter. We'll use small geometric transformations (rotation, translation, scaling) and elastic deformations to simulate natural handwriting styles. We will use the `albumentations` library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(51)\n",
    "mnist_augmentations = A.Compose([\n",
    "    A.Affine(\n",
    "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "        scale=(0.9, 1.1),\n",
    "        rotate=(-10, 10),\n",
    "        p=0.8\n",
    "    ),\n",
    "    A.ElasticTransform(\n",
    "        alpha=20, sigma=5, border_mode=cv2.BORDER_CONSTANT, p=0.6\n",
    "    ),\n",
    "    A.GridDistortion(num_steps=3, distort_limit=0.1, p=0.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Augmented Dataset\n",
    "\n",
    "We'll define a new PyTorch `Dataset` class that takes our misclassified samples and applies these augmentations on the fly. For each misclassified sample, it will generate multiple augmented versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedMNISTDataset(Dataset):\n",
    "    def __init__(self, fiftyone_view, label_map, base_transforms, augmentations, augment_factor=5):\n",
    "        self.image_paths = fiftyone_view.values(\"filepath\")\n",
    "        self.str_labels = fiftyone_view.values(\"ground_truth.label\")\n",
    "        self.label_map = label_map\n",
    "        self.base_transforms = base_transforms\n",
    "        self.augmentations = augmentations\n",
    "        self.augment_factor = augment_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) * self.augment_factor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_idx = idx // self.augment_factor\n",
    "        image = Image.open(self.image_paths[base_idx]).convert('L')\n",
    "        image_np = np.array(image, dtype=np.uint8)\n",
    "        augmented = self.augmentations(image=image_np)['image']\n",
    "        image = Image.fromarray(augmented, mode='L')\n",
    "        if self.base_transforms: image = self.base_transforms(image)\n",
    "        label_idx = self.label_map.get(self.str_labels[base_idx], -1)\n",
    "        return image, torch.tensor(label_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_augmented_dataset = AugmentedMNISTDataset(\n",
    "    mislabeled_train_images_view,\n",
    "    label_map=label_map,\n",
    "    base_transforms=image_transforms,\n",
    "    augmentations=mnist_augmentations,\n",
    "    augment_factor=10\n",
    ")\n",
    "\n",
    "# Combine original training set with the new augmented samples\n",
    "combined_dataset = ConcatDataset([torch_train_set, torch_augmented_dataset])\n",
    "print(f\"Original training set size: {len(torch_train_set)}\")\n",
    "print(f\"Augmented samples added: {len(torch_augmented_dataset)}\")\n",
    "print(f\"Combined dataset size: {len(combined_dataset)}\")\n",
    "\n",
    "# Create a new DataLoader for fine-tuning\n",
    "combined_train_loader = create_deterministic_training_dataloader(\n",
    "    combined_dataset, batch_size=64, shuffle=True, num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "We'll now fine-tune our model. We start with the best weights from our initial training and train for a few more epochs on the combined dataset. We use a **lower learning rate** for fine-tuning to make small, careful adjustments to the already-learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(51)\n",
    "# Load the best model to start fine-tuning\n",
    "retrain_model = ModernLeNet5().to(device)\n",
    "retrain_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# Use a smaller learning rate for fine-tuning\n",
    "retrain_optimizer = Adam(retrain_model.parameters(), lr=0.0001)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reload the validation loader\n",
    "val_dataset = fo.load_dataset(\"mnist-validation-set\")\n",
    "torch_val_set = CustomTorchImageDataset(val_dataset, image_transforms, label_map)\n",
    "val_loader = torch.utils.data.DataLoader(torch_val_set, batch_size=64)\n",
    "\n",
    "# Training loop\n",
    "retrain_epochs = 15\n",
    "best_retrain_val_loss = float('inf')\n",
    "retrain_model_save_path = Path(os.getcwd()) / 'retrained_lenet.pth'\n",
    "\n",
    "for epoch in range(retrain_epochs):\n",
    "    retrain_model.train()\n",
    "    # Simplified training and validation epoch functions for brevity\n",
    "    for images, labels in tqdm(combined_train_loader, desc=f\"Retraining Epoch {epoch+1}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        retrain_optimizer.zero_grad()\n",
    "        logits = retrain_model(images)\n",
    "        loss = ce_loss(logits, labels)\n",
    "        loss.backward()\n",
    "        retrain_optimizer.step()\n",
    "\n",
    "    retrain_model.eval()\n",
    "    val_losses = []\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = retrain_model(images)\n",
    "            val_losses.append(ce_loss(logits, labels).item())\n",
    "    \n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_retrain_val_loss:\n",
    "        best_retrain_val_loss = avg_val_loss\n",
    "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
    "        print(\"✓ Saved improved retrained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Finally, let's evaluate our newly fine-tuned model on the test set and compare its performance to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the best retrained model\n",
    "final_model = ModernLeNet5().to(device)\n",
    "final_model.load_state_dict(torch.load(retrain_model_save_path, map_location=device))\n",
    "final_model.eval()\n",
    "\n",
    "# Create test loader\n",
    "torch_test_set = CustomTorchImageDataset(test_dataset, image_transforms, label_map)\n",
    "test_loader = torch.utils.data.DataLoader(torch_test_set, batch_size=64)\n",
    "\n",
    "# Run inference with the retrained model\n",
    "retrained_predictions, retrained_logits = [], []\n",
    "with torch.inference_mode():\n",
    "    for images, _ in tqdm(test_loader, desc=\"Evaluating retrained model\"):\n",
    "        logits = final_model(images.to(device))\n",
    "        retrained_logits.append(logits.cpu().numpy())\n",
    "        retrained_predictions.extend(torch.max(logits.data, 1)[1].cpu().numpy())\n",
    "\n",
    "retrained_logits = np.concatenate(retrained_logits, axis=0)\n",
    "\n",
    "# Store retrained predictions in FiftyOne\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    pred_idx = retrained_predictions[i]\n",
    "    logits = retrained_logits[i]\n",
    "    conf = float(Fun.softmax(torch.tensor(logits), dim=0)[pred_idx])\n",
    "    sample[\"retrained_lenet_classification\"] = fo.Classification(\n",
    "        label=dataset_classes[pred_idx], confidence=conf, logits=logits.tolist()\n",
    "    )\n",
    "    sample.save()\n",
    "\n",
    "print(\"Retrained predictions stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate original and retrained models\n",
    "original_eval = test_dataset.evaluate_classifications(\"lenet_classification\", eval_key=\"original_eval\")\n",
    "retrained_eval = test_dataset.evaluate_classifications(\"retrained_lenet_classification\", eval_key=\"retrained_eval\")\n",
    "\n",
    "print(\"\\n--- Original Model Performance ---\")\n",
    "original_eval.print_report()\n",
    "\n",
    "print(\"\\n--- Retrained Model Performance ---\")\n",
    "retrained_eval.print_report()\n",
    "\n",
    "# Compare performance\n",
    "orig_accuracy = original_eval.metrics()['accuracy']\n",
    "retrain_accuracy = retrained_eval.metrics()['accuracy']\n",
    "print(f\"\\nAccuracy Improvement: {retrain_accuracy - orig_accuracy:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Changes\n",
    "\n",
    "Let's see exactly which samples were fixed by retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "originally_wrong = test_dataset.match(F(\"lenet_classification.label\") != F(\"ground_truth.label\"))\n",
    "now_correct = originally_wrong.match(F(\"retrained_lenet_classification.label\") == F(\"ground_truth.label\"))\n",
    "\n",
    "now_wrong = test_dataset.match(\n",
    "    (F(\"lenet_classification.label\") == F(\"ground_truth.label\")) & \n",
    "    (F(\"retrained_lenet_classification.label\") != F(\"ground_truth.label\"))\n",
    ")\n",
    "\n",
    "print(f\"Samples fixed by retraining: {len(now_correct)}\")\n",
    "print(f\"Samples broken by retraining: {len(now_wrong)}\")\n",
    "print(f\"Net improvement in correct predictions: {len(now_correct) - len(now_wrong)}\")\n",
    "\n",
    "session = fo.launch_app(test_dataset)\n",
    "session.view = now_correct\n",
    "print(f\"\\nView the fixed samples in the App: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have completed the entire workflow from data exploration to model training, analysis, and targeted improvement. You've seen how a generalist model like CLIP provides a strong baseline, and how a specialized, supervised model can achieve superior performance. Most importantly, you've learned how to use model predictions and embeddings to analyze your dataset, find problematic samples, and use that information to make your model even better.\n",
    "\n",
    "Please see `summary.md` for a full recap and suggested next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}