{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JO75fXaFtdvE"
      },
      "source": [
        "# Getting Started with Image Classification and Dataset Curation using FiftyOne and PyTorch\n",
        "\n",
        "## Zero-shot Classification with CLIP and Supervised Learning with Convolutional Neural Networks\n",
        "\n",
        "\n",
        "### Who Is this Tutorial for\n",
        "This tutorial is designed for computer vision practitioners and data scientists who want to master image classification workflows using FiftyOne. Whether you're new to computer vision or experienced with other tools, you'll learn how to leverage FiftyOne's powerful capabilities for dataset curation, model evaluation, and visual analysis.\n",
        "\n",
        "This tutorial is appropriate for any level of computer vision knowledge. By the end of this tutorial, you'll be able to quickly identify mislabeled samples, compare classification models, create meaningful embeddings, and seamlessly move between FiftyOne and PyTorch workflows.\n",
        "\n",
        "### Assumed Knowledge\n",
        "We assume familiarity with basic Python programming and fundamental machine learning concepts. Knowledge of PyTorch is helpful but not required,  we'll explain the key concepts as we go. This tutorial is recommended for beginners to intermediate practitioners in computer vision.\n",
        "\n",
        "### Time to Complete\n",
        "90-120 minutes\n",
        "\n",
        "### Required Packages\n",
        "FiftyOne, PyTorch, and several other packages are required. You can install them with:\n",
        "\n",
        "```bash\n",
        "pip install fiftyone torch torchvision numpy\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISDyu809KcxZ"
      },
      "source": [
        "\n",
        "## Content Overview\n",
        "\n",
        "### 1. MNIST Dataset Exploration with FiftyOne\n",
        "\n",
        "Understand the MNIST dataset structure. Load the test split into FiftyOne. Compute and visualize metadata. Explore data distributions using aggregations and the FiftyOne App.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Loading datasets from FiftyOne Dataset Zoo\n",
        "*   Computing image metadata\n",
        "*   Using FiftyOne aggregations for data statistics\n",
        "*   Visualizing dataset distributions\n",
        "\n",
        "### 2. Image Embeddings with CLIP\n",
        "\n",
        "Generate image embeddings for the test dataset using a pre-trained CLIP model. Visualize these high dimensional vectors in 2D using PCA and UMAP to understand image similarity.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Loading pre-trained models from FiftyOne Model Zoo\n",
        "*   Computing image embeddings with CLIP\n",
        "*   Assigning embeddings to dataset samples\n",
        "*   Dimensionality reduction: PCA and UMAP\n",
        "*   Visualizing embedding plots in FiftyOne\n",
        "\n",
        "### 3. Dataset Analysis using CLIP Embeddings\n",
        "\n",
        "Continue analysis of the CLIP embeddings on the test dataset. Explore dataset clustering concepts. Compute and examine sample uniqueness and representativeness based on CLIP embeddings.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Introduction to clustering with embeddings\n",
        "*   Creating a similarity index\n",
        "*   Identifying outliers and representative samples\n",
        "\n",
        "### 4. Zero-Shot Classification with CLIP\n",
        "\n",
        "Perform image classification on the test dataset using CLIP without task specific training. Evaluate CLIP's performance using FiftyOne's tools.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Zero-shot classification principles\n",
        "*   Text prompts for classification\n",
        "*   Applying a model to a FiftyOne dataset\n",
        "*   Evaluating classification results including accuracy and confusion matrix\n",
        "\n",
        "### 5. Supervised Classification: LeNet-5 with PyTorch\n",
        "\n",
        "Build and train a LeNet-5 convolutional neural network from scratch using PyTorch. Prepare the MNIST training data. Implement the training loop and validation procedures.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   LeNet-5 architecture\n",
        "*   Defining a subclass of PyTorch's `nn.Module`\n",
        "*   Splitting FiftyOne data for training and validation\n",
        "*   Creating custom PyTorch Datasets from FiftyOne views\n",
        "*   Data normalization through mean and standard deviation computation\n",
        "*   PyTorch DataLoaders\n",
        "*   Defining loss functions and optimizers\n",
        "*   Training loops and model checkpointing\n",
        "\n",
        "### 6. LeNet-5 Model Evaluation on Test Data\n",
        "\n",
        "Apply the trained LeNet-5 model to the MNIST test set. Store predictions in FiftyOne. Evaluate its performance. Analyze prediction characteristics, including hardness and mistakenness.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Applying a PyTorch model to a FiftyOne dataset\n",
        "*   Storing predictions, confidence, and logits\n",
        "*   Evaluating classification performance\n",
        "*   Analyzing prediction confidence distributions\n",
        "*   Computing sample hardness and mistakenness\n",
        "\n",
        "### 7. Analysis of LeNet-5 Learned Features using Training Data\n",
        "\n",
        "Extract embeddings from the trained LeNet-5 model using the training data. Compute and visualize these embeddings. Analyze uniqueness and representativeness based on LeNet's learned features. Identify misclassifications within the training set.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Extracting embeddings from intermediate PyTorch model layers\n",
        "*   Storing custom model embeddings in FiftyOne\n",
        "*   Visualizing custom embeddings with PCA and UMAP\n",
        "*   Analyzing uniqueness and representativeness of training samples\n",
        "*   Evaluating model performance on training data\n",
        "*   Identifying false positives and false negatives in training data\n",
        "\n",
        "### 8. Data Augmentation Concepts for MNIST\n",
        "\n",
        "Understand effective data augmentation strategies for the MNIST dataset. Learn about geometric transformations and elastic deformations. Discuss augmentations to avoid. This section provides a conceptual discussion.\n",
        "\n",
        "**Key concepts covered:**\n",
        "*   Rationale for data augmentation\n",
        "*   Geometric and elastic transformations suitable for MNIST\n",
        "*   Best practices and pitfalls in augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQnUrahBi1Yp"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9NFRcctjBMc"
      },
      "outputs": [],
      "source": [
        "# Remove > /dev/null if you encounter errors during installation\n",
        "!pip install fiftyone==1.5.2 > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG3_ZBDEjNwq"
      },
      "source": [
        "### FiftyOne Plug-ins"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRlSg9rPlT2H"
      },
      "source": [
        "We'll also install FiftyOne plugins for model evaluation and data augmentation:\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JzHKYfmnjRYa",
        "outputId": "424b45ef-02f4-40e0-870a-9332642d4efb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading voxel51/fiftyone-plugins...\n",
            "\n",
            "Copying plugin '@voxel51/evaluation' to '/root/fiftyone/__plugins__/@voxel51/evaluation'\n"
          ]
        }
      ],
      "source": [
        "# Plug-in to evaluate the performance of our classification models\n",
        "!fiftyone plugins download \\\n",
        "    https://github.com/voxel51/fiftyone-plugins \\\n",
        "    --plugin-names @voxel51/evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r0OvvAflvmV",
        "outputId": "a766f10e-cca1-48b2-ed18-c09f3a6779ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading jacobmarks/fiftyone-albumentations-plugin...\n",
            "\n",
            "Copying plugin '@jacobmarks/albumentations_augmentation' to '/root/fiftyone/__plugins__/@jacobmarks/albumentations_augmentation'\n"
          ]
        }
      ],
      "source": [
        "# Plug-in for image augmentations\n",
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCOAC2gFi4ND"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Rj1VqYYi6J1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set environment variables for reproducibility BEFORE importing torch\n",
        "os.environ['PYTHONHASHSEED'] = '51'\n",
        "os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Fun\n",
        "import torchvision.transforms.v2 as transforms\n",
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "import fiftyone.brain as fob\n",
        "from torch.utils.data import Dataset, ConcatDataset\n",
        "from fiftyone import ViewField as F\n",
        "import fiftyone.utils.random as four\n",
        "from tqdm import tqdm\n",
        "from torch.optim import Adam\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import gc\n",
        "import albumentations as A\n",
        "import cv2\n",
        "import random\n",
        "from typing import Optional, Dict, Tuple, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKx_LKY_W87E"
      },
      "source": [
        "## The MNIST dataset\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/mnist_clean.png?raw=true)\n",
        "\n",
        "The Modified National Institute of Standards and Technology (MNIST) dataset stands as one of the most influential benchmarks in computer vision and machine learning history. Created by Yann LeCun and colleagues in 1998, MNIST transformed a collection of handwritten digits from American Census Bureau employees and high school students into a standardized machine learning challenge that has shaped decades of research.\n",
        "\n",
        "\n",
        "\n",
        "**Dataset Structure and Characteristics**\n",
        "\n",
        "The MNIST dataset contains 60,000 training images and 10,000 testing images of handwritten digits (0-9). These are grayscale images of size 28x28 pixels, with each pixel value ranging from 0 (black) to 255 (white). The images have been size-normalized and centered, making them ideal for learning fundamental computer vision concepts without the complexity of dealing with varying scales, rotations, or backgrounds found in natural images. You can inspect the samples on the test portion of the dataset through [try.fiftyone.ai](https://try.fiftyone.ai/datasets/mnist/samples).\n",
        "\n",
        "**Historical Significance and Impact**\n",
        "\n",
        "MNIST earned its status as the \"Hello World\" of computer vision for several reasons. First, it provided the research community with a common benchmark that was computationally tractable, even early personal computers could train models on MNIST in reasonable time. Second, its simplicity allowed researchers to focus on algorithmic innovations rather than data preprocessing challenges. Landmark achievements in deep learning, from early multilayer perceptrons to convolutional architectures like LeNet-5, were first demonstrated and validated on MNIST.\n",
        "\n",
        "The dataset served as a proving ground for fundamental concepts of modern computer vision: convolutional neural networks, regularization techniques, and optimization algorithms were all tested on these handwritten digits. Many techniques that seem obvious today, like data augmentation, dropout, and batch normalization, were first explored and validated using MNIST as a testbed.\n",
        "\n",
        "**Why MNIST Remains Relevant**\n",
        "\n",
        "While critics sometimes dismiss MNIST as \"too easy\" for modern standards, it continues to serve crucial educational and research purposes. For newcomers to computer vision, MNIST provides an ideal environment to understand core concepts without overwhelming complexity. The dataset is small enough to experiment with quickly, yet rich enough to demonstrate important phenomena like overfitting, the importance of data augmentation, and the impact of architectural choices.\n",
        "\n",
        "Moreover, MNIST's apparent simplicity can be deceptive. Achieving state-of-the-art performance >99.7% accuracy requires sophisticated techniques and careful attention to detail, making it an interesting benchmark for testing new methodologies.\n",
        "\n",
        "See the [discussion on Kaggle here](https://www.kaggle.com/competitions/digit-recognizer/discussion/61480) to get inspiration of how to best tackle this challenge.\n",
        "\n",
        "\n",
        "## MNIST in the Modern Era\n",
        "\n",
        "Today, MNIST serves as an excellent starting point for understanding how modern techniques like embeddings, zero-shot classification, and transfer learning work. While a model trained specifically on MNIST might achieve 99%+ accuracy, applying a general-purpose vision model like CLIP without any MNIST-specific training provides insights into how well these models generalize and what they've learned about visual patterns from their massive training datasets.\n",
        "\n",
        "This makes MNIST perfect for comparing traditional supervised learning approaches with modern pre-trained models, helping us understand the trade-offs between task-specific optimization and general-purpose visual understanding.\n",
        "\n",
        "\n",
        "## CLIP\n",
        "\n",
        "**CLIP (Contrastive Language-Image Pre-training)** is a vision-language model developed by OpenAI that learns to understand the relationship between images and text descriptions. Traditional computer vision models are trained on fixed sets of image categories, but CLIP was trained on 400 million image-text pairs from the internet, learning to match images with their captions. This training enables CLIP to perform \"zero-shot\" classification: the ability to classify images into categories it has not seen during training by comparing the image representation with text descriptions of potential classes. The model works by encoding both images and text into the same high-dimensional embedding space, where similar concepts cluster together, allowing it to determine which text description best matches a given image through similarity comparison.\n",
        "\n",
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/clip%20contrastive%20pre-training.png?raw=true)\n",
        "\n",
        "### CLIP vs MNIST\n",
        "\n",
        "In [OpenAI's 2021 CLIP paper](https://arxiv.org/abs/2103.00020), the comparison with MNIST revealed fascinating insights about zero-shot learning capabilities. While supervised models trained specifically on MNIST achieve near-perfect accuracy (>99.7%), OpenAI's best performing variant of CLIP (trained on about 400 million image-text pairs) achieved only 88% accuracy on these handwritten digits. This is interesting considering the model was not trained explicitly trained on any variant of this dataset. This comparison highlighted a shift in modern computer vision from specialized models that excel at narrow tasks to general models that perform reasonably well across diverse domains. The 11+ percentage point gap between supervised and zero-shot approaches on MNIST demonstrates both the power and limitations of general-purpose vision-language models, making MNIST an excellent case study for understanding the trade-offs between specialized optimization and general-purpose learning, particularly valuable for exploring modern approaches like few-shot learning, prompt engineering, and transfer learning strategies.\n",
        "\n",
        "Note that the variant of CLIP that we use in our experiments of this notebook, `\"clip-vit-base32-torch\"` (a Vit-B/32 model) is *not* the top performing variant of CLIP that is showcased on the original OpenAI paper. That would be \"ViT-L/14@336px\", a bigger vision transformer model. The Vit-B/32 base models remain interesting and widely used due to their lower number of parameters.\n",
        "\n",
        "\n",
        "**Known Issues and Research Opportunities**\n",
        "\n",
        "The original dataset contains several images where the ground truth labeling is ambiguous or questionable. The academic community has identified [several of these ambiguous samples](https://arxiv.org/abs/1912.05283). These labeling inconsistencies, while representing less than 0.1% of the dataset, provide opportunities to explore data quality assessment techniques.\n",
        "\n",
        "Finding and analyzing these edge cases teaches valuable lessons about real-world data challenges. In production systems, you'll inevitably encounter ambiguous samples, annotation errors, and edge cases. MNIST's imperfections make it an excellent sandbox for developing robust approaches to handle these issues. Let's see if we can find them using FiftyOne's powerful analysis capabilities!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws9PjUuSZhH1"
      },
      "source": [
        "## Zero-shot Classification vs \"Traditional\" Supervised Training\n",
        "\n",
        "**Zero-shot classification** leverages pre-trained models like CLIP that have learned rich visual representations from massive datasets, allowing them to classify images into \"interpolated\" categories they've not explicitly seen during training. These models understand the semantic relationship between images and text descriptions, enabling classification through natural language prompts like \"a photo of the digit 3\" without requiring any task-specific training data. In contrast, **traditional supervised training** requires labeled examples for each class you want to predict. You must provide the model with many of images of each digit along with their correct labels, then train the network to learn the mapping from pixel patterns to class labels through backpropagation. While supervised training often achieves higher accuracy on specific datasets and allows for domain-specific optimization, zero-shot approaches offer remarkable flexibility and can instantly work on new classification tasks without additional training time or computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qaJxdibqZeZ"
      },
      "source": [
        "![](https://github.com/andandandand/images-for-colab-notebooks/blob/main/clip%20zero-shot%20prediction.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bo7U6NjqTSu"
      },
      "source": [
        "## Where CLIP Fails: Specialized Domains\n",
        "\n",
        "**Understanding the Limitations of General-Purpose Vision Models**\n",
        "\n",
        "While CLIP excels at understanding natural images and common objects, it struggles with specialized domains that differ from its internet-scale training data. MNIST handwritten digits represent one such domain where CLIP's performance drops compared to task-specific models.\n",
        "\n",
        "**Domain-Specific Challenges:**\n",
        "\n",
        "**Limited Training Exposure**: CLIP trained on web-scraped image-text pairs, which contain fewer examples of handwritten digits compared to photographs of everyday objects. The model lacks deep exposure to the subtle variations in handwriting styles.\n",
        "\n",
        "**Scale and Context Mismatch**: CLIP expects high-resolution, colorful images with rich contextual information. MNIST's 28x28 grayscale digits provide minimal visual context that CLIP relies on for classification decisions.\n",
        "\n",
        "**Specialized Visual Features**: Handwritten digits require recognition of fine-grained stroke patterns, curves, and connections that differ from the broader visual concepts CLIP learned from natural images. A \"6\" versus \"9\" distinction depends on subtle orientation cues.\n",
        "\n",
        "**Text-Image Alignment**: CLIP's strength lies in matching images with descriptive captions. Simple digit classification lacks the rich semantic relationships between visual and textual information that CLIP exploits in other domains.\n",
        "\n",
        "**Performance Gap**: This explains why CLIP achieves only ~88% accuracy on MNIST while specialized CNNs reach >99%. The 11+ percentage point gap highlights the trade-off between general-purpose capabilities and domain-specific optimization.\n",
        "\n",
        "Other specialized domains where CLIP struggles include medical imaging, satellite imagery, microscopy, and industrial inspection: areas where domain expertise and task-specific training data prove more valuable than general visual understanding.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p3C20CqVqgRN"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/clip_limitations.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3J8_p_7ZsvW"
      },
      "source": [
        "### Loading the MNIST Dataset from FiftyOne's Dataset Zoo\n",
        "\n",
        "A FiftyOne dataset wraps together the annotations and image data into a unified, queryable structure that makes computer vision workflows seamless. Unlike traditional approaches where you might manage separate files for images and labels, FiftyOne treats each sample as a rich object containing the image itself, ground truth labels, metadata, and any predictions or embeddings you add later. This design enables powerful operations like filtering by class imbalance, visualizing prediction confidence, or finding samples with specific characteristics, all through a consistent API.\n",
        "\n",
        "Loading MNIST from [FiftyOne's Dataset Zoo](https://docs.voxel51.com/dataset_zoo/index.html) is straightforward:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzg-FFDoTVWr",
        "outputId": "f8017571-9112-483b-9fc2-c34974aaaac4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading split 'test' to '/root/fiftyone/mnist/test'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'test' to '/root/fiftyone/mnist/test'\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 308kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 2.94MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 8.73MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   2% |/------------|   230/10000 [100.7ms elapsed, 4.3s remaining, 2.3K samples/s] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [3.4s elapsed, 0s remaining, 2.8K samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [3.4s elapsed, 0s remaining, 2.8K samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info written to '/root/fiftyone/mnist/info.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/mnist/info.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'mnist' split 'test'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'mnist' split 'test'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [5.3s elapsed, 0s remaining, 1.9K samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [5.3s elapsed, 0s remaining, 1.9K samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'mnist-test' created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'mnist-test' created\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Name:        mnist-test\n",
              "Media type:  image\n",
              "Num samples: 10000\n",
              "Persistent:  False\n",
              "Tags:        []\n",
              "Sample fields:\n",
              "    id:               fiftyone.core.fields.ObjectIdField\n",
              "    filepath:         fiftyone.core.fields.StringField\n",
              "    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    created_at:       fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at: fiftyone.core.fields.DateTimeField\n",
              "    ground_truth:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We will load the test split from the dataset first\n",
        "test_dataset = foz.load_zoo_dataset(\"mnist\", split='test')\n",
        "test_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3UYLNkgyKNj"
      },
      "source": [
        "We launch the FiftyOne app to visualize the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD7xgYznyGZR",
        "outputId": "0f4ad1b1-6c86-4edb-b255-c6f7274aa98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.core.session.session:Session launched. Run `session.show()` to open the App in a cell output.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Welcome to\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
            "â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
            "â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•     â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•\n",
            "â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
            "â•šâ•â•     â•šâ•â•â•šâ•â•        â•šâ•â•      â•šâ•â•    â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• v1.5.2\n",
            "\n",
            "If you're finding FiftyOne helpful, here's how you can get involved:\n",
            "\n",
            "|\n",
            "|  â­â­â­ Give the project a star on GitHub â­â­â­\n",
            "|  https://github.com/voxel51/fiftyone\n",
            "|\n",
            "|  ðŸš€ðŸš€ðŸš€ Join the FiftyOne Discord community ðŸš€ðŸš€ðŸš€\n",
            "|  https://community.voxel51.com/\n",
            "|\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.core.session.session:\n",
            "Welcome to\n",
            "\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•—   â–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
            "â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•\n",
            "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—     â–ˆâ–ˆâ•‘    â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â–ˆâ–ˆâ•— â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
            "â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•     â–ˆâ–ˆâ•‘     â•šâ–ˆâ–ˆâ•”â•  â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘â•šâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•\n",
            "â–ˆâ–ˆâ•‘     â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘        â–ˆâ–ˆâ•‘      â–ˆâ–ˆâ•‘   â•šâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘ â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\n",
            "â•šâ•â•     â•šâ•â•â•šâ•â•        â•šâ•â•      â•šâ•â•    â•šâ•â•â•â•â•â• â•šâ•â•  â•šâ•â•â•â•â•šâ•â•â•â•â•â•â• v1.5.2\n",
            "\n",
            "If you're finding FiftyOne helpful, here's how you can get involved:\n",
            "\n",
            "|\n",
            "|  â­â­â­ Give the project a star on GitHub â­â­â­\n",
            "|  https://github.com/voxel51/fiftyone\n",
            "|\n",
            "|  ðŸš€ðŸš€ðŸš€ Join the FiftyOne Discord community ðŸš€ðŸš€ðŸš€\n",
            "|  https://community.voxel51.com/\n",
            "|\n",
            "\n"
          ]
        }
      ],
      "source": [
        "session = fo.launch_app(test_dataset, auto=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym7vp063WUm-"
      },
      "source": [
        "With `compute_metadata()` we add the size in bytes, the image file type, the width and height of the image, and the number of channels to our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKBLQnntVila",
        "outputId": "3162ebfd-26e8-430d-baef-5f20d2923c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing metadata...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.core.metadata:Computing metadata...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [3.8s elapsed, 0s remaining, 2.8K samples/s]        \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [3.8s elapsed, 0s remaining, 2.8K samples/s]        \n"
          ]
        }
      ],
      "source": [
        "test_dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTH-cijUZfAm"
      },
      "source": [
        "We can do [aggregations](https://docs.voxel51.com/user_guide/using_aggregations.html) on the dataset to explore the dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xr_v-8_yZ041"
      },
      "source": [
        "We can use the [bounds](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.bounds) aggregation to compute the [min, max] range of a numeric field of a dataset. And [mean()](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.bounds) and [std()](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.std) to compute the mean and standard deviation of it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yphCNxUxY-BW",
        "outputId": "90a105a7-b073-4f72-a7bf-134c0e295641"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(483, 1033)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset.bounds(\"metadata.size_bytes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXWLJSuYZV0-",
        "outputId": "ae116d0f-9045-420e-8b1b-8a3ad3bd577b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(768.6065, 84.01331833554713)"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset.mean(\"metadata.size_bytes\"), test_dataset.std(\"metadata.size_bytes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOmjhEc6X4Am"
      },
      "source": [
        "Try filtering by label and visualizing the metadata of the MNIST images through the FiftyOne app.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/filtering_by_label_mnist_w_metadata.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "Rm8fbYruVVZI",
        "outputId": "45b368f3-60bd-4f8b-f94d-078a30409325"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XF-aubVNe3JX"
      },
      "source": [
        "We can also visualize the distributions of `metadata.size_bytes` and `ground_truth.label`. Both of these are relevant when we decide the batch size on which to run our models and to evaluate the balance of classes in our dataset.\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/distribtution_size_bytes.png?raw=true)\n",
        "\n",
        "For this try clicking on the `+` symbol next to samples, select `Histograms` and then `metadata.size_bytes` and `ground_truth.label` from the dropdown menu. The `Split horizontally button` will allow you to see the panel alongside the image data.\n",
        "\n",
        "\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/ground_truh_distribution_mnist.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQQPA9HTXVsC"
      },
      "source": [
        "## Creating Image Embeddings with CLIP\n",
        "\n",
        "**Image embeddings** are high-dimensional vector representations that capture the semantic and aesthetic content of images in a vector format that machine learning models can understand and compare.\n",
        "\n",
        "Think of embeddings as a way to translate visual concepts into vectors where similar images will correspond to similar embedding vectors, while visually or semantically different images will have more distant vectors in a high-dimensional space.\n",
        "\n",
        "OpenAI's CLIP model creates particularly powerful embeddings because it was trained to understand the relationship between images and their matching text descriptions, enabling it to capture rich semantic meaning. In FiftyOne, creating embeddings with CLIP is straightforward:\n",
        "\n",
        "1. We obtain the model through [`foz.load_zoo_model(\"\"clip-vit-base32-torch\"\")`](https://docs.voxel51.com/model_zoo/models.html#clip-vit-base32-torch) and pass it to the GPU (if we have it available).\n",
        "2. We use the [`compute_embeddings()`](https://docs.voxel51.com/api/fiftyone.brain.internal.core.elasticsearch.html#fiftyone.brain.internal.core.elasticsearch.ElasticsearchSimilarityIndex.compute_embeddings) method:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST1OahV5kvgi",
        "outputId": "2fd5c88a-8792-45e6-9bb5-c9fbd4eccae4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading model from 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.core.models:Downloading model from 'https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt'...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|    2.6Gb/2.6Gb [6.5s elapsed, 0s remaining, 499.6Mb/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|    2.6Gb/2.6Gb [6.5s elapsed, 0s remaining, 499.6Mb/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading CLIP tokenizer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.clip.zoo:Downloading CLIP tokenizer...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|   10.4Mb/10.4Mb [26.8ms elapsed, 0s remaining, 386.0Mb/s]    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ|   10.4Mb/10.4Mb [26.8ms elapsed, 0s remaining, 386.0Mb/s]    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model is loaded on cuda\n"
          ]
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\",\n",
        "                                device=device)\n",
        "print(f\"The model is loaded on {clip_model._device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhHT2scrakDC",
        "outputId": "dab058cd-854e-4674-9b8e-d93ce8216b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The CLIP model has 151,277,313 parameters.\n"
          ]
        }
      ],
      "source": [
        "# Calculate the total number of parameters in the model\n",
        "total_params = sum(p.numel() for p in clip_model._model.parameters())\n",
        "\n",
        "print(f\"The CLIP model has {total_params:,} parameters.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1Z9t8CBoXKz"
      },
      "source": [
        "This will take about 3 min on a Google Colab instance with GPU enabled"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-va5BZtpFw6",
        "outputId": "e6aa3772-8cf3-460b-80ab-ee30b61fe703"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [32.1s elapsed, 0s remaining, 331.1 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [32.1s elapsed, 0s remaining, 331.1 samples/s]      \n"
          ]
        }
      ],
      "source": [
        "clip_embeddings = test_dataset.compute_embeddings(model=clip_model,\n",
        "                                        batch_size=512,\n",
        "                                        num_workers=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV6umlNpEmGe"
      },
      "source": [
        "The collection of embeddings is a NumPy array. Each embedding is a 512-dimensional vector. We can think about each embedding as a distinct point in a high-dimensional space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSyH8a7M0IrT",
        "outputId": "f86dd033-4b7b-447e-b658-0b9554f61183"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(numpy.ndarray, (10000, 512))"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Check the format and shape of our embeddings vector\n",
        "type(clip_embeddings), clip_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQQtu1jkE0I8"
      },
      "source": [
        "Here we attach each embedding to its corresponding image sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wKDE9MR80UJ7"
      },
      "outputs": [],
      "source": [
        "for index, sample in enumerate(test_dataset):\n",
        "    sample[\"clip_embeddings\"] = clip_embeddings[index]\n",
        "    sample.save()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8ZVmfIOFHtK"
      },
      "source": [
        "We see that the first sample now has the field `embeddings` attached to it, and we can query its shape.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDSCqkEss_K4",
        "outputId": "c4ed92f6-bd08-44e3-efd5-f88ac3a0b399"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(512,)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dataset.first().clip_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7i_BkCiFYXs"
      },
      "source": [
        "## Creating a Similarity Index\n",
        "\n",
        "A **similarity index** allows for efficient searching of similar samples within a dataset based on their embeddings. Instead of comparing a query sample's embedding to every other embedding in the dataset (which can be slow for large datasets), the index organizes the embeddings in a way that allows for fast retrieval of the most similar samples. This is particularly useful for tasks like:\n",
        "\n",
        "- Finding visually similar images.\n",
        "- Identifying near-duplicate samples.\n",
        "- Exploring clusters of similar data points.\n",
        "\n",
        "In this case, we're creating a similarity index based on the CLIP embeddings, which capture the semantic content of the images. This index will enable us to quickly find images that are semantically similar to a given query image.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsloYFP4eUIe",
        "outputId": "bce1de59-b26d-42fc-ac34-0a976ea8e775"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing similarity index based on CLIP embeddings...\n",
            "Similarity index computed successfully!\n"
          ]
        }
      ],
      "source": [
        "# Compute similarity index based on CLIP embeddings\n",
        "print(\"Computing similarity index based on CLIP embeddings...\")\n",
        "\n",
        "similarity_index = fob.compute_similarity(\n",
        "    test_dataset,\n",
        "    embeddings=\"clip_embeddings\",  # Field containing the CLIP embeddings\n",
        "    brain_key=\"clip_cosine_similarity_index\",  # Unique identifier for this similarity index\n",
        "    backend=\"sklearn\",  # Can also use \"pinecone\" for large datasets\n",
        "    metric=\"cosine\"  # Similarity metric (cosine is good for embeddings)\n",
        ")\n",
        "\n",
        "print(f\"Similarity index computed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "id": "2kXiNdZaewRJ",
        "outputId": "f22d2538-cc3f-4845-c3ad-af96ea19b6fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Similarity Index Details:\n",
            "- Total samples indexed: 10000\n",
            "- Embeddings field: clip_embeddings\n",
            "- Metric: cosine\n",
            "\n",
            "Querying for images similar to sample: 685150e9e6f3512ccfd1151c\n",
            "            with label 7 - seven\n",
            "Found 10 most similar samples\n",
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "# Display similarity index information\n",
        "print(f\"Similarity Index Details:\")\n",
        "print(f\"- Total samples indexed: {len(test_dataset)}\")\n",
        "print(f\"- Embeddings field: {similarity_index.config.embeddings_field}\")\n",
        "print(f\"- Metric: {similarity_index.config.metric}\")\n",
        "\n",
        "# Example 1: Find similar images to a specific digit\n",
        "query_sample = test_dataset.first()\n",
        "print(f\"\"\"\\nQuerying for images similar to sample: {query_sample.id}\n",
        "            with label {query_sample.ground_truth.label}\"\"\")\n",
        "\n",
        "# Use the sample ID (string) instead of the sample object\n",
        "similar_view = test_dataset.sort_by_similarity(\n",
        "    query_sample.id,  # Pass the sample ID, not the sample object\n",
        "    brain_key=\"clip_cosine_similarity_index\",\n",
        "    k=10,  # Return top 10 most similar\n",
        "    reverse=False  # Most similar first\n",
        ")\n",
        "\n",
        "print(f\"Found {len(similar_view)} most similar samples\")\n",
        "session.view = similar_view\n",
        "session.refresh()\n",
        "print(session.url)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjHRCURddjR8"
      },
      "source": [
        "### Creating a 2D Projection of the Embeddings for Visualization\n",
        "\n",
        "**Compressing High-Dimensional Representations for Visual Analysis**\n",
        "\n",
        "Projecting high-dimensional embeddings to 2D enables visual exploration of data structure and relationships. This process represents lossy compression. The 2D visualization cannot capture all information present in the original embedding space. The projection serves as an approximation for understanding data patterns, not for precise analysis.\n",
        "\n",
        "**Dimensionality Reduction Methods**: PCA and UMAP compress embeddings through different approaches. PCA finds linear combinations that preserve maximum variance, making it faster and deterministic, the same data produces identical results. UMAP uses sophisticated manifold learning to preserve local neighborhoods and reveal cluster structure, but operates through stochastic processes that can produce different results across runs.\n",
        "\n",
        "**Visualization Purpose**: These projections help identify clusters, outliers, and data distribution patterns. Points close together in the visualization often share similar characteristics, while distant points represent different concepts. The projection quality depends on how well the chosen method preserves the relationships present in the original high-dimensional space.\n",
        "\n",
        "**Interpretation Limits**: Remember that 2D projections cannot represent all relationships from the original embeddings. Some distances may appear closer or farther than they actually are in the full dimensional space.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XacGyC6OrtdR",
        "outputId": "d109d37d-7a7b-445b-b09d-f176fd4f4643"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating visualization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.visualization:Generating visualization...\n"
          ]
        }
      ],
      "source": [
        "pca_visualization = fob.compute_visualization(test_dataset,\n",
        "                                              method=\"pca\",\n",
        "                                              embeddings=\"clip_embeddings\",\n",
        "                                              num_dims=2,\n",
        "                                              brain_key=\"pca_visualization_clip_embeds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517,
          "referenced_widgets": [
            "174bb0a373a74f48a553d5c39b885009",
            "1dfa8c1055e24386a3842bbd29a4f000",
            "93b1e06982384dc59266823e5d0fa0db",
            "91ae5f5fa74c42cc86d680a87648d30e",
            "2743b9036442401d982cc40066b313e8",
            "b9a82587c2164fe195b2dcb6b94819d5",
            "4e633d835f4c43258d6b6040734b451f",
            "6b4efe8d06224db99536cd5679f9e3fa",
            "392a80de6b1f404291b4acd97139e4fa",
            "b1849c27cc774e11bcbedbbe874ae02a",
            "c036d26df71f4d5ca6ecdbc3ab4f9f6e"
          ]
        },
        "id": "4mq4hg9_ovbh",
        "outputId": "6c621e23-c38f-4714-e265-896fd2b24717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating visualization...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.visualization:Generating visualization...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "UMAP( verbose=True)\n",
            "Tue Jun 17 11:28:07 2025 Construct fuzzy simplicial set\n",
            "Tue Jun 17 11:28:07 2025 Finding Nearest Neighbors\n",
            "Tue Jun 17 11:28:07 2025 Building RP forest with 10 trees\n",
            "Tue Jun 17 11:28:12 2025 NN descent for 13 iterations\n",
            "\t 1  /  13\n",
            "\t 2  /  13\n",
            "\t 3  /  13\n",
            "\t 4  /  13\n",
            "\tStopping threshold met -- exiting after 4 iterations\n",
            "Tue Jun 17 11:28:25 2025 Finished Nearest Neighbor Search\n",
            "Tue Jun 17 11:28:27 2025 Construct embedding\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "174bb0a373a74f48a553d5c39b885009",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epochs completed:   0%|            0/500 [00:00]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\tcompleted  0  /  500 epochs\n",
            "\tcompleted  50  /  500 epochs\n",
            "\tcompleted  100  /  500 epochs\n",
            "\tcompleted  150  /  500 epochs\n",
            "\tcompleted  200  /  500 epochs\n",
            "\tcompleted  250  /  500 epochs\n",
            "\tcompleted  300  /  500 epochs\n",
            "\tcompleted  350  /  500 epochs\n",
            "\tcompleted  400  /  500 epochs\n",
            "\tcompleted  450  /  500 epochs\n",
            "Tue Jun 17 11:28:34 2025 Finished embedding\n"
          ]
        }
      ],
      "source": [
        "umap_visualization = fob.compute_visualization(test_dataset,\n",
        "                                              method=\"umap\",\n",
        "                                              embeddings=\"clip_embeddings\",\n",
        "                                              num_dims=2,\n",
        "                                              brain_key=\"umap_visualization_clip_embeds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtBJDv7_JwMY"
      },
      "source": [
        "## Visualizing Embeddings in the FiftyOne App\n",
        "\n",
        "To view the embedding visualizations (PCA and UMAP) in the FiftyOne App:\n",
        "\n",
        "1. **Open the Panel**: Click on the \"**+**\" icon next to \"Samples\" in the FiftyOne App's header.\n",
        "2. **Select Visualizations**: From the dropdown menu, select \"Embeddings\" under the \"Visualizations\" section.\n",
        "3. **Choose Embeddings**: In the Embeddings panel that appears, select the embedding field you want to visualize from the dropdown (e.g., `pca_visualization_clip_embeds` or `umap_visualization_clip_embeds`).\n",
        "\n",
        "You can then interact with the 2D plot, hovering over points to see the corresponding images and selecting regions to filter the samples in the grid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "htl4-xTnt5xY",
        "outputId": "5c5ae709-4a80-463e-fa11-08fa797d0c6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJTHNOjydh7d"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/image_embeddings_zero_cluster.gif?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzdQd-g3A248"
      },
      "source": [
        "Let's use the FiftyOne app to explore the  samples in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "EDJ5HWQzAnEZ",
        "outputId": "c8931f20-85ef-45b9-c2a2-f804c1f37148"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwoQi7T52naS"
      },
      "source": [
        "## Clustering through Embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJeaax5-8Rww"
      },
      "source": [
        "\n",
        "**Clustering** transforms the high-dimensional embedding space into groups of similar images, helping us discover hidden patterns and structure in our dataset. Rather than manually browsing through thousands of images, clustering algorithms like K-means, HDBSCAN, or Gaussian Mixture Models automatically identify samples that share visual or semantic characteristics. This is particularly powerful when combined with CLIP embeddings, as the clusters often correspond to semantic concepts like \"outdoor scenes,\" \"close-up portraits,\" or \"nighttime images.\" FiftyOne provides a dedicated clustering plugin that makes this analysis seamless. Install it with:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNWq6eg43Fn_",
        "outputId": "5842c300-d6d0-4613-c3d2-0ef0d2aa68ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading jacobmarks/clustering-plugin...\n",
            "\n",
            "Copying plugin '@jacobmarks/clustering' to '/root/fiftyone/__plugins__/@jacobmarks/clustering'\n"
          ]
        }
      ],
      "source": [
        "!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXlzLzWZ860S"
      },
      "source": [
        "Once installed, you can access clustering functionality directly from the FiftyOne App by pressing the backtick key (`) and typing `compute_clusters`. The plugin offers multiple clustering algorithms and customizable parameters. This workflow is valuable for understanding the groupings within your data produced by the embedding model. Learn more about clustering workflows and advanced techniques in the [FiftyOne clustering tutorial](https://docs.voxel51.com/tutorials/clustering.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "WNnlzTX93OnS",
        "outputId": "d4b70d05-c61b-4e7b-f0e9-653480be4f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JloCmwLUXZ78"
      },
      "source": [
        "## Zero-shot Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87uZ17UaeBcw"
      },
      "source": [
        "Now that we understand how CLIP creates meaningful embeddings, we can leverage these representations for **zero-shot classification** - the ability to classify images into categories without any task-specific training. CLIP's vision-language training enables it to understand the semantic relationship between images and text descriptions, allowing us to classify MNIST digits simply by providing natural language descriptions of each class.\n",
        "\n",
        "The key insight is that CLIP learns to map both images and text into the same embedding space, where semantically similar concepts cluster together. By computing the similarity between an image's embedding and text embeddings for each class description, we can determine which category best matches the input image. This approach is remarkably flexible, we can change our classification categories simply by modifying the text prompts, without retraining the models.\n",
        "\n",
        "For MNIST digit classification, we'll use descriptive text prompts combined with the written names of each digit.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8Jk1qpak2Q9",
        "outputId": "9d5f0051-4c8a-4a70-9bc3-50e29f089902"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['0 - zero',\n",
              " '1 - one',\n",
              " '2 - two',\n",
              " '3 - three',\n",
              " '4 - four',\n",
              " '5 - five',\n",
              " '6 - six',\n",
              " '7 - seven',\n",
              " '8 - eight',\n",
              " '9 - nine']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We obtain the distinct labels of the dataset\n",
        "dataset_classes = sorted(test_dataset.distinct(\"ground_truth.label\"))\n",
        "dataset_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLYZ287EUJQh"
      },
      "source": [
        "## The Effect of the Text Prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4oAWHCawr7b"
      },
      "source": [
        "With CLIP, the `text_prompt` parameter can have a significant effect on the accuracy of the model, as it affects the text embedding for the label.\n",
        "\n",
        "### Experiment with the Prompt\n",
        "\n",
        "Try modifying the `text_prompt` field with:\n",
        "\n",
        "* \"The handwritten digit \"\n",
        "* \"A grayscale image of the number \"\n",
        "* \"A pixel illustration of the digit \"\n",
        "* \"A low resolution image of the number\"\n",
        "* \"An MNIST digit \"\n",
        "\n",
        "And observe the effect on the accuracy, precision and recall on our classification report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6Q7NqbJdYCL"
      },
      "outputs": [],
      "source": [
        "clip_model = foz.load_zoo_model(\n",
        "    \"clip-vit-base32-torch\",\n",
        "    text_prompt=\"A photo of \",\n",
        "    classes=dataset_classes,\n",
        "    device=device\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBgS_90dWgbf",
        "outputId": "a008aae3-f18e-48a5-9558-a6b255861bdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The CLIP model preprocesses the input data\n",
        "clip_model.preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJvdRwml6Cff"
      },
      "source": [
        "Notice that the CLIP model was originally trained on much larger images and that\n",
        "these where RGB. In order to make the MNIST grayscale image data work with CLIP, we transform it scale it using the mean and standard deviations of the original training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rMuKMCQWP9u",
        "outputId": "92109106-3353-4c25-8021-5c106c386811"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Compose(\n",
              "    <fiftyone.utils.torch.ToPILImage object at 0x7f22a3243990>\n",
              "    Resize(size=[224, 224], interpolation=bilinear, max_size=None, antialias=True)\n",
              "    ToTensor()\n",
              "    Normalize(mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clip_model._transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mG-ECXjiQ7p"
      },
      "source": [
        "The `store_logits=True` parameter is important as it preserves the model's confidence scores for each prediction, enabling us to analyze prediction uncertainty and identify samples where the model was unsure. After applying the model, we can compute comprehensive evaluation metrics and visualize the results using FiftyOne's evaluation framework:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9F4bKQheb_-",
        "outputId": "2bd0123b-27ad-42fa-b3a0-f2b076622a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [36.4s elapsed, 0s remaining, 282.3 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [36.4s elapsed, 0s remaining, 282.3 samples/s]      \n"
          ]
        }
      ],
      "source": [
        "test_dataset.apply_model(\n",
        "    model=clip_model,\n",
        "    label_field=\"clip_zero_shot_classification\",\n",
        "    # We need to store the logits to compute the \"mistakenness\" value\n",
        "    store_logits=True,\n",
        "    # This is how many samples we will show to the model at once\n",
        "    batch_size=256,\n",
        "    progress_bar=True,\n",
        "     # Use more running threads if you have multiple cpus\n",
        "    num_workers=os.cpu_count()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "kACq6EVGf5RR",
        "outputId": "092c9018-a4c7-48a6-d408-90f1f51231d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plmWGrpTXfSQ"
      },
      "source": [
        "## Evaluating CLIP's Classification Performance\n",
        "\n",
        "With our zero-shot CLIP predictions generated, we need to evaluate how well the model performs compared to the ground truth labels. FiftyOne provides powerful evaluation capabilities that go beyond simple accuracy metrics, allowing us to understand where and why our model succeeds or fails.\n",
        "\n",
        "[FiftyOne's Model Evaluation Panel](https://docs.voxel51.com/user_guide/app.html#model-evaluation-panel-sub-new ) provides interactive confusion matrices, per-class metrics, and the ability to drill down into specific failure modes. This visual analysis helps identify systematic errors, class imbalances, and edge cases that pure numerical metrics might miss, enabling targeted improvements to your classification pipeline.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "y04n9H8WhAUf",
        "outputId": "af684f67-7512-4f60-c11d-b8e40496d729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "clip_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"clip_zero_shot_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"clip_zero_shot_eval\")\n",
        "\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaXb016KpLcd",
        "outputId": "bdef2944-b917-408e-d896-f275b7e0a8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    0 - zero      0.256     0.999     0.408       980\n",
            "     1 - one      0.145     0.033     0.053      1135\n",
            "     2 - two      1.000     0.016     0.031      1032\n",
            "   3 - three      0.901     0.386     0.541      1010\n",
            "    4 - four      0.513     0.177     0.263       982\n",
            "    5 - five      0.600     0.161     0.254       892\n",
            "     6 - six      0.076     0.255     0.117       958\n",
            "   7 - seven      0.613     0.635     0.624      1028\n",
            "   8 - eight      0.178     0.115     0.140       974\n",
            "    9 - nine      0.000     0.000     0.000      1009\n",
            "\n",
            "    accuracy                          0.275     10000\n",
            "   macro avg      0.428     0.278     0.243     10000\n",
            "weighted avg      0.427     0.275     0.241     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clip_evaluation_results.print_report(classes=dataset_classes, digits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "TioSsNHjS_sw",
        "outputId": "909108ca-d9e1-47ed-f35d-5ffa2f7ec0e8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fiftyone/core/plots/plotly.py:1591: UserWarning:\n",
            "\n",
            "Interactive plots are currently only supported in Jupyter notebooks. Support outside of notebooks and in Google Colab and Databricks will be included in an upcoming release. In the meantime, you can still use this plot, but note that (i) selecting data will not trigger callbacks, and (ii) you must manually call `plot.show()` to launch a new plot that reflects the current state of an attached session.\n",
            "\n",
            "See https://docs.voxel51.com/user_guide/plots.html#working-in-notebooks for more information.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"adfdb8b8-11ba-48bb-b8e5-97c2e39fe9f4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"adfdb8b8-11ba-48bb-b8e5-97c2e39fe9f4\")) {                    Plotly.newPlot(                        \"adfdb8b8-11ba-48bb-b8e5-97c2e39fe9f4\",                        [{\"mode\":\"markers\",\"opacity\":0.1,\"x\":[0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9],\"y\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"type\":\"scatter\",\"uid\":\"5f3f6e49-af9c-42c1-ad60-e1f4c4b8316e\"},{\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hoverinfo\":\"skip\",\"showscale\":false,\"z\":[[229,16,0,0,6,4,394,38,322,0],[147,47,0,1,6,3,647,10,112,1],[192,64,0,0,47,1,66,653,5,0],[603,3,0,0,0,3,244,0,105,0],[58,0,0,0,1,144,684,2,3,0],[162,0,0,0,174,1,592,30,23,0],[25,0,0,390,40,75,416,30,34,0],[378,89,16,42,55,0,141,303,8,0],[1049,37,0,0,10,9,15,0,15,0],[979,0,0,0,0,0,0,0,1,0]],\"zmax\":1049,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"c03dacac-6d99-47e3-b592-ec30ebd40ad5\"},{\"colorbar\":{\"len\":1,\"lenmode\":\"fraction\"},\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hovertemplate\":\"\\u003cb\\u003ecount: %{z}\\u003c\\u002fb\\u003e\\u003cbr\\u003eground_truth: %{y}\\u003cbr\\u003eclip_zero_shot_classification: %{x}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"opacity\":0.25,\"z\":[[229,16,0,0,6,4,394,38,322,0],[147,47,0,1,6,3,647,10,112,1],[192,64,0,0,47,1,66,653,5,0],[603,3,0,0,0,3,244,0,105,0],[58,0,0,0,1,144,684,2,3,0],[162,0,0,0,174,1,592,30,23,0],[25,0,0,390,40,75,416,30,34,0],[378,89,16,42,55,0,141,303,8,0],[1049,37,0,0,10,9,15,0,15,0],[979,0,0,0,0,0,0,0,1,0]],\"zmax\":1049,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"40025415-10e0-407c-9660-09d0b630633e\"}],                        {\"clickmode\":\"event\",\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(51,51,51)\"},\"error_y\":{\"color\":\"rgb(51,51,51)\"},\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"baxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"colorscale\":{\"sequential\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"sequentialminus\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]]},\"colorway\":[\"#F8766D\",\"#A3A500\",\"#00BF7D\",\"#00B0F6\",\"#E76BF3\"],\"font\":{\"color\":\"rgb(51,51,51)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"rgb(237,237,237)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"rgb(237,237,237)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"}}},\"xaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"tickmode\":\"array\",\"ticktext\":[\"0 - zero\",\"1 - one\",\"2 - two\",\"3 - three\",\"4 - four\",\"5 - five\",\"6 - six\",\"7 - seven\",\"8 - eight\",\"9 - nine\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"yaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"scaleanchor\":\"x\",\"scaleratio\":1,\"tickmode\":\"array\",\"ticktext\":[\"9 - nine\",\"8 - eight\",\"7 - seven\",\"6 - six\",\"5 - five\",\"4 - four\",\"3 - three\",\"2 - two\",\"1 - one\",\"0 - zero\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"margin\":{\"r\":0,\"t\":30,\"l\":0,\"b\":0},\"title\":{}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('adfdb8b8-11ba-48bb-b8e5-97c2e39fe9f4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# The confusion matrix has a lot activity outside of the main diagonal\n",
        "clip_evaluation_results.plot_confusion_matrix()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Exkmk50l_Kr"
      },
      "source": [
        "## Inspect the Images where the Model is Performing Worst\n",
        "\n",
        "We can create a view of the images labeled as nines on the dataset, to get a closer look at what CLIP is misclassifying.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0WTxH3smMkE"
      },
      "outputs": [],
      "source": [
        "nines_view = test_dataset.filter_labels(\"ground_truth\",\n",
        "                                        F(\"$ground_truth.label\") == \"9 - nine\"\n",
        "                                       )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IOzwOIZGzeS"
      },
      "source": [
        "#### The Purpose of `session.refresh()` in FiftyOne\n",
        "\n",
        "The `session.refresh()` command in FiftyOne syncs the state of the FiftyOne App interface with the state of our FiftyOne dataset object in the Python kernel.\n",
        "\n",
        "When we modify a dataset or view in your Python code by adding new fields, computing embeddings, applying models, filtering samples, or adding tags, these changes are made to the dataset object in your Python session. However, the FiftyOne App, which runs in a separate process, doesn't automatically know about these updates.\n",
        "\n",
        "Calling `session.refresh()` explicitly tells the FiftyOne App to reload the current view of the dataset from the backend. This ensures that the App's visualization and sample grid reflect the latest changes you've made in your notebook, allowing you to see and interact with the results of your code in the App's UI. Without `session.refresh()`, the App might show an outdated version of your dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "oFZbuxx-wv5L",
        "outputId": "30996c15-f067-4734-aef3-1e77ed7b2a95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "session.view = nines_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e74j5GZ8HAI5"
      },
      "source": [
        "## Clearing the CLIP Model from GPU memory\n",
        "\n",
        "After using the CLIP model, it's good practice to free up the GPU memory it occupied. This is done by:\n",
        "\n",
        "1. Deleting the model variable (`del clip_model`).\n",
        "2. Running Python's garbage collector (`gc.collect()`) to clean up references.\n",
        "3. Emptying the CUDA cache (`torch.cuda.empty_cache()`) to release cached memory on the GPU.\n",
        "\n",
        "This ensures that the GPU memory (aka VRAM) is available for the next batches of images and models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0uw_4muPB-8",
        "outputId": "23f91165-e795-46de-ce6c-c1b633c1c5b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA device memory from clip_model should be cleared.\n"
          ]
        }
      ],
      "source": [
        "# Delete the model variable\n",
        "del clip_model\n",
        "\n",
        "# Run Python's garbage collector\n",
        "gc.collect()\n",
        "\n",
        "# Empty the CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "print(\"CUDA device memory from clip_model should be cleared.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUd2c3J0q79_"
      },
      "source": [
        "## Reproducibility for Training Experiments\n",
        "\n",
        "To ensure full reproducibility of your training experiments, you need to set random seeds for all libraries and operations that involve randomness. This ensures that initial model weights, data shuffling, and any other random operations are the same across different runs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i-VJ7ajbq16D",
        "outputId": "23c761b5-dcc7-4b4e-da59-2993d395c67b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n"
          ]
        }
      ],
      "source": [
        "def set_seeds(seed=51):\n",
        "    \"\"\"\n",
        "    Set seeds for complete reproducibility across all libraries and operations.\n",
        "\n",
        "    Args:\n",
        "        seed (int): Random seed value\n",
        "    \"\"\"\n",
        "    # Set environment variables before other imports\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n",
        "\n",
        "    # Python random module\n",
        "    random.seed(seed)\n",
        "\n",
        "    # NumPy\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # PyTorch CPU\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    # PyTorch GPU (all devices)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n",
        "\n",
        "        # CUDA deterministic operations\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # OpenCV\n",
        "    cv2.setRNGSeed(seed)\n",
        "\n",
        "    # Albumentations (for data augmentation)\n",
        "    try:\n",
        "        A.seed_everything(seed)\n",
        "    except AttributeError:\n",
        "        # Older versions of albumentations\n",
        "        pass\n",
        "\n",
        "    # PyTorch deterministic algorithms (may impact performance)\n",
        "    try:\n",
        "        torch.use_deterministic_algorithms(True)\n",
        "    except RuntimeError:\n",
        "        # Some operations don't have deterministic implementations\n",
        "        print(\"Warning: Some operations may not be deterministic\")\n",
        "\n",
        "    print(f\"All random seeds set to {seed} for reproducibility\")\n",
        "\n",
        "\n",
        "\n",
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(51)\n",
        "\n",
        "# Additional reproducibility considerations:\n",
        "\n",
        "def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Create a DataLoader with deterministic behavior.\n",
        "\n",
        "    Args:\n",
        "        dataset: PyTorch Dataset instance\n",
        "        batch_size: Batch size\n",
        "        shuffle: Whether to shuffle data\n",
        "        **kwargs: Additional DataLoader arguments\n",
        "\n",
        "    Returns:\n",
        "        Training DataLoader with reproducible behavior\n",
        "    \"\"\"\n",
        "    # Use a generator with fixed seed for reproducible shuffling\n",
        "    generator = torch.Generator()\n",
        "    generator.manual_seed(51)\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        generator=generator if shuffle else None,\n",
        "        **kwargs\n",
        "    )\n",
        "\n",
        "# Example usage:\n",
        "# train_loader = create_deterministic_dataloader(\n",
        "#     torch_train_set,\n",
        "#     batch_size=64,\n",
        "#     shuffle=True,\n",
        "#     num_workers=4,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKrksi3nXk2k"
      },
      "source": [
        "## Creating a Custom Convolutional Neural Networks in PyTorch (Two Versions of LeNet-5)\n",
        "\n",
        "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/lenet5-architecture.png)\n",
        "\n",
        "While zero-shot classification with CLIP demonstrates the power of modern pre-trained models, understanding how to build and train convolutional neural networks from scratch remains fundamental to computer vision. **LeNet-5**, proposed by Yann LeCun in 1998, represents one of the earliest and most influential CNN architectures. Despite its age, LeNet-5 perfectly illustrates core CNN concepts including convolutional layers, pooling layers, and the transition from feature extraction to classification.\n",
        "\n",
        "LeNet-5's architecture is elegantly simple yet effective: it uses alternating convolutional and pooling layers to extract hierarchical features, followed by fully connected layers for classification. The network learns low-level features like edges and curves in early layers, then combines these into higher-level digit patterns in deeper layers. This hierarchical feature learning principle underlies virtually all modern CNN architectures.\n",
        "\n",
        "For MNIST digit classification, LeNet-5 provides an excellent baseline to compare against CLIP's zero-shot performance. While CLIP leverages massive-scale pre-training and vision-language understanding, LeNet-5 demonstrates what's possible with task-specific supervised learning on a much smaller scale. Building this model from scratch in PyTorch teaches essential concepts about gradient-based optimization, backpropagation, and the relationship between network architecture and learning capacity.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr3oiQDzibmw"
      },
      "source": [
        "Here is [a great animation](https://youtu.be/UxIS_PoVoz8?si=3ibZms7Hk1oSj55k) showcasing the architecture.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9bB2RaOyc3G"
      },
      "outputs": [],
      "source": [
        "class ClassicLeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    LeNet-5 CNN architecture for MNIST digit classification.\n",
        "\n",
        "    Original paper: \"Gradient-Based Learning Applied to Document Recognition\"\n",
        "    by LeCun et al. (1998)\n",
        "\n",
        "    Architecture (maintains original design with padding):\n",
        "    Input (28x28) -> Pad to (32x32) -> C1 (6@28x28) -> S2 (6@14x14) ->\n",
        "    C3 (16@10x10) -> S4 (16@5x5) -> C5 (120@1x1) -> F6 (84) -> Output (10)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ClassicLeNet5, self).__init__()\n",
        "\n",
        "        # Feature extraction layers\n",
        "        # C1: Convolutional layer - 6 feature maps, 5x5 kernels\n",
        "        # Add padding=2 to convert 28x28 input to 32x32, maintaining original design\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "\n",
        "        # S2: Subsampling layer (average pooling) - 2x2 with stride 2\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # C3: Convolutional layer - 16 feature maps, 5x5 kernels\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "\n",
        "        # S4: Subsampling layer (average pooling) - 2x2 with stride 2\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # C5: Convolutional layer - 120 feature maps, 5x5 kernels (original design)\n",
        "        # This reduces the 5x5 feature maps to 1x1\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
        "\n",
        "        # Classification layers\n",
        "        # F6: Fully connected layer with 84 units\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "\n",
        "        # Output layer: 10 classes for digits 0-9\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass through the network.\n",
        "\n",
        "        Args:\n",
        "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
        "\n",
        "        Returns:\n",
        "            Output logits of shape (batch_size, num_classes)\n",
        "        \"\"\"\n",
        "        # C1: Convolution + activation (padding converts 28x28 to 32x32, then conv to 28x28)\n",
        "        # Input: (batch, 1, 28, 28) -> Pad to (32, 32) -> Conv to (batch, 6, 28, 28)\n",
        "        x = torch.tanh(self.conv1(x))\n",
        "\n",
        "        # S2: Average pooling\n",
        "        # Input: (batch, 6, 28, 28) -> Output: (batch, 6, 14, 14)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # C3: Convolution + activation\n",
        "        # Input: (batch, 6, 14, 14) -> Output: (batch, 16, 10, 10)\n",
        "        x = torch.tanh(self.conv2(x))\n",
        "\n",
        "        # S4: Average pooling\n",
        "        # Input: (batch, 16, 10, 10) -> Output: (batch, 16, 5, 5)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # C5: Convolution + activation (original 5x5 kernel design)\n",
        "        # Input: (batch, 16, 5, 5) -> Output: (batch, 120, 1, 1)\n",
        "        x = torch.tanh(self.conv3(x))\n",
        "\n",
        "        # Flatten for fully connected layers\n",
        "        # Input: (batch, 120, 1, 1) -> Output: (batch, 120)\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "        # F6: Fully connected + activation\n",
        "        # Input: (batch, 120) -> Output: (batch, 84)\n",
        "        x = torch.tanh(self.fc1(x))\n",
        "\n",
        "        # Output layer (no activation - raw logits)\n",
        "        # Input: (batch, 84) -> Output: (batch, 10)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u31fRXOc1XA6"
      },
      "source": [
        "Below is an alternative and more modern implementation. Here the activation functions have been switched from tanh to ReLU and Max Pooling is used instead of Average Pooling. Feel free to choose either!\n",
        "\n",
        "Unlike the CLIP model, that has been pretrained, these networks are trained from scratch. We will use train portion of the MNIST dataset to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OXvBhUE1Nqz"
      },
      "outputs": [],
      "source": [
        "# Alternative modern version with ReLU and MaxPooling\n",
        "class ModernLeNet5(nn.Module):\n",
        "    \"\"\"\n",
        "    Modernized version of LeNet-5 with ReLU activations and max pooling.\n",
        "    Often performs better on MNIST than the original version.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(ModernLeNet5, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.fc2 = nn.Linear(84, num_classes)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(Fun.relu(self.conv1(x)))\n",
        "        x = self.pool(Fun.relu(self.conv2(x)))\n",
        "        x = Fun.relu(self.conv3(x))\n",
        "\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = Fun.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Add dropout for regularization\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ga26CjkcMAH"
      },
      "source": [
        "## Obtain the Training Dataset from FiftyOne's Zoo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NezpK2qr18h3",
        "outputId": "78ba4981-2b3f-458e-f013-73553bc1cd1f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/mnist/train'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'train' to '/root/fiftyone/mnist/train'\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 10.5MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 340kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 9.53MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   0% |/------------|   296/60000 [100.5ms elapsed, 20.3s remaining, 2.9K samples/s] "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [22.0s elapsed, 0s remaining, 2.7K samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [22.0s elapsed, 0s remaining, 2.7K samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info written to '/root/fiftyone/mnist/info.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/mnist/info.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'mnist' split 'train'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'mnist' split 'train'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [35.8s elapsed, 0s remaining, 1.8K samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [35.8s elapsed, 0s remaining, 1.8K samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'mnist-train-val' created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'mnist-train-val' created\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing metadata...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.core.metadata:Computing metadata...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [23.5s elapsed, 0s remaining, 2.9K samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60000/60000 [23.5s elapsed, 0s remaining, 2.9K samples/s]      \n"
          ]
        }
      ],
      "source": [
        "# We use the training split to train our LeNet model\n",
        "# We make this dataset persistent as we want to save our changes for multiple sessions\n",
        "train_val_dataset = foz.load_zoo_dataset(\"mnist\",\n",
        "                                         split='train',\n",
        "                                         dataset_name=\"mnist-train-val\",\n",
        "                                         persistent=True)\n",
        "\n",
        "train_val_dataset.compute_metadata()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "328v5V3E7nmk"
      },
      "source": [
        "## Splitting into Train and Validation\n",
        "\n",
        "The **validation set** serves as an unbiased evaluation mechanism during model development, acting as a proxy for real-world performance before touching the final test set. While the training set teaches the model to recognize patterns in handwritten digits, the validation set reveals whether the model has truly learned generalizable features or simply memorized the training data, a phenomenon known as overfitting.\n",
        "\n",
        "During training, we monitor both training and validation **loss** simultaneously. **Loss** is a numerical measure of how far off the model's predictions are from the correct answers - lower loss means better performance. We use categorical cross-entropy loss, which penalizes confident wrong predictions more heavily than uncertain ones. **Training loss** typically decreases steadily as the model learns, but **validation loss** tells the real story. If validation loss plateaus or begins increasing while training loss continues decreasing, the model is overfitting and memorizing training-specific details rather than learning robust digit recognition patterns. This signals when to stop training, adjust hyperparameters, or modify the architecture.\n",
        "\n",
        "The validation set also enables **hyperparameter tuning** without contaminating our final evaluation. We can experiment with different learning rates, batch sizes, regularization techniques, or architectural modifications, using validation loss to guide these decisions. Each configuration gets evaluated on the same held-out validation data, ensuring fair comparisons.\n",
        "\n",
        "**Important: the test set remains completely untouched** throughout the development process. Only after we've selected our final model configuration based on validation performance do we evaluate on the test set once, giving us an honest estimate of how the model will perform on truly unseen data. This three-way split (train/validation/test) is fundamental to responsible machine learning development and prevents the subtle \"data leakage\" that can make models appear better than they actually are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7M9_3Gri26Wo"
      },
      "outputs": [],
      "source": [
        "# The images come with the 'train' tag and this must be deleted\n",
        "# at the sample level.\n",
        "train_val_dataset.untag_samples([\"train\", \"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfPp1Huh2258",
        "outputId": "ae2f5f5d-31ee-4a59-c695-a0acd93693e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n",
            "Tag counts after split:\n",
            "{'validation': 9000, 'train': 51000}\n"
          ]
        }
      ],
      "source": [
        "set_seeds(51)\n",
        "# Create random 85%/15% split using tags\n",
        "four.random_split(train_val_dataset,\n",
        "                  {\"train\": 0.85, \"validation\": 0.15},\n",
        "                  # The seed makes the split reproducible\n",
        "                  seed=51)\n",
        "\n",
        "# Verify the split by counting tags\n",
        "tag_counts = train_val_dataset.count_sample_tags()\n",
        "print(\"Tag counts after split:\")\n",
        "print(tag_counts)\n",
        "\n",
        "# Separate validation and train FO datasets\n",
        "train_dataset = train_val_dataset.match_tags(\"train\").clone()\n",
        "val_dataset = train_val_dataset.match_tags(\"validation\").clone()\n",
        "\n",
        "# Set names for FO datasets using the 'name' property\n",
        "train_dataset.name = \"mnist-training-set\"\n",
        "val_dataset.name = \"mnist-validation-set\"\n",
        "\n",
        "# Define persistency\n",
        "train_dataset.persistent = True\n",
        "val_dataset.persistent = True\n",
        "\n",
        "# Verify no overlap between train and validation\n",
        "train_ids = set(train_dataset.values(\"id\"))\n",
        "val_ids = set(val_dataset.values(\"id\"))\n",
        "overlap = train_ids.intersection(val_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RvCWt129XuCb"
      },
      "source": [
        "## Moving the FiftyOne Data Splits to torch Datasets\n",
        "\n",
        "To train our PyTorch model, we need to convert our FiftyOne dataset views into PyTorch `Dataset` objects that can load and preprocess images during training. This bridge between FiftyOne's dataset management and PyTorch's training pipeline is important for maintaining both the metadata and annotations while enabling batch processing. PyTorch's `Dataset` class provides a standardized interface for data loading, handling tasks like image loading, preprocessing transforms, and label conversion. By creating a custom dataset class that works with FiftyOne's file paths and labels, we can leverage PyTorch's `DataLoader` for batching, shuffling, and parallel data loading while preserving all the dataset analysis capabilities that FiftyOne provides. This approach allows us to move between FiftyOne's visual analysis and PyTorch's training workflows without duplicating data or losing the rich metadata we've computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZu4eGYSNq-0"
      },
      "outputs": [],
      "source": [
        "# Custom PyTorch Dataset class for MNIST training data\n",
        "class CustomTorchImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, fiftyone_dataset,\n",
        "                 image_transforms=None,\n",
        "                 label_map=None,\n",
        "                 gt_field=\"ground_truth\"):\n",
        "        self.fiftyone_dataset = fiftyone_dataset\n",
        "        self.image_paths = self.fiftyone_dataset.values(\"filepath\")\n",
        "        self.str_labels = self.fiftyone_dataset.values(f\"{gt_field}.label\")\n",
        "        self.image_transforms = image_transforms\n",
        "\n",
        "        if label_map is None:\n",
        "            self.label_map = {str(i): i for i in range(10)}  # \"0\"->0, \"1\"->1, etc.\n",
        "        else:\n",
        "            self.label_map = label_map\n",
        "\n",
        "        print(f\"CustomTorchImageDataset initialized with {len(self.image_paths)} samples.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('L')\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading image {image_path}: {e}\")\n",
        "            return torch.randn(1, 28, 28), torch.tensor(-1, dtype=torch.long)\n",
        "\n",
        "        if self.image_transforms:\n",
        "            image = self.image_transforms(image)\n",
        "\n",
        "        label_str = self.str_labels[idx]\n",
        "        label_idx = self.label_map.get(label_str, -1)\n",
        "        if label_idx == -1:\n",
        "            print(f\"Warning: Label '{label_str}' not in label_map for image {image_path}\")\n",
        "\n",
        "        return image, torch.tensor(label_idx, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_F8qU_vFuS-"
      },
      "source": [
        "## Computing the Mean and Standard Deviation\n",
        "\n",
        "Before training neural networks, we compute the **mean and standard deviation** of our input data to apply **standard scaling** (also called normalization or standardization).\n",
        "\n",
        "\n",
        "**Standard Scaling Formula:**\n",
        "\n",
        "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
        "\n",
        "Where:\n",
        "- $z$ = standardized value\n",
        "- $x$ = original pixel value\n",
        "- $\\mu$ = mean of all pixel values in the dataset\n",
        "- $\\sigma$ = standard deviation of all pixel values in the dataset\n",
        "\n",
        "In PyTorch, this is implemented with torch.`transforms.Normalize((mean_intensity), (std_intensity))`\n",
        "\n",
        "This preprocessing step transforms our pixel values to have zero mean and unit variance, which provides several critical benefits:\n",
        "\n",
        "**Why Standard Scaling Matters:**\n",
        "\n",
        "**Gradient Optimization**: Neural networks learn through gradient descent, which works best when input features are on similar scales. Without scaling, features with larger magnitudes (like raw pixel values 0-255) can dominate the gradient updates, leading to slower convergence and unstable training.\n",
        "\n",
        "**Weight Initialization Compatibility**: Modern weight initialization schemes (Xavier, He initialization) assume inputs are roughly centered around zero with unit variance. Standard scaling ensures our data matches these assumptions, preventing vanishing or exploding gradients during early training.\n",
        "\n",
        "**Activation Function Efficiency**: Many activation functions (tanh, sigmoid) work optimally when inputs are centered around zero. Scaled inputs help neurons operate in the most responsive regions of these functions rather than saturating in flat regions.\n",
        "\n",
        "**Learning Rate Stability**: With standardized inputs, we can use higher learning rates without instability, as the optimization landscape becomes more uniform across different dimensions.\n",
        "\n",
        "For MNIST images, we transform raw pixel values from the range [0, 255] to approximately [-1, 1] with mean â‰ˆ 0, creating a more favorable training environment that typically results in faster convergence and better final performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "BwV9Hgh-E-PX",
        "outputId": "f30e5921-142d-473f-cf5d-df4b44270162"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing image intensity statistics from FiftyOne view...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 51000/51000 [00:06<00:00, 7633.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computed from 51000 images\n",
            "Total pixels: 39,984,000\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Mean: 0.1318, Std: 0.3075'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def compute_stats_fiftyone(fiftyone_view):\n",
        "    \"\"\"\n",
        "    Compute stats directly from FiftyOne using aggregations.\n",
        "    Requires images to be loaded as arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"Computing image intensity statistics from FiftyOne view...\")\n",
        "\n",
        "    # Get all image filepaths\n",
        "    filepaths = fiftyone_view.values(\"filepath\")\n",
        "\n",
        "    # Load all pixel values\n",
        "    all_pixels = []\n",
        "\n",
        "    for filepath in tqdm(filepaths):\n",
        "\n",
        "        try:\n",
        "            # Load image as grayscale array\n",
        "            image = Image.open(filepath).convert('L')\n",
        "            # Scale values to the range [0, 1]\n",
        "            pixels = np.array(image, dtype=np.float32) / 255.0\n",
        "            all_pixels.append(pixels.flatten())\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {filepath}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Concatenate all pixel values\n",
        "    all_pixels = np.concatenate(all_pixels)\n",
        "\n",
        "    # Compute statistics\n",
        "    mean = np.mean(all_pixels)\n",
        "    std = np.std(all_pixels)\n",
        "\n",
        "    print(f\"Computed from {len(filepaths)} images\")\n",
        "    print(f\"Total pixels: {len(all_pixels):,}\")\n",
        "\n",
        "    return mean, std\n",
        "\n",
        "mean_intensity, std_intensity = compute_stats_fiftyone(train_dataset)\n",
        "f\"Mean: {mean_intensity:.4f}, Std: {std_intensity:.4f}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO92PNlTB_Gi",
        "outputId": "9f970559-109f-4601-c445-81b8d0a2caed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'0 - zero': 0,\n",
              " '1 - one': 1,\n",
              " '2 - two': 2,\n",
              " '3 - three': 3,\n",
              " '4 - four': 4,\n",
              " '5 - five': 5,\n",
              " '6 - six': 6,\n",
              " '7 - seven': 7,\n",
              " '8 - eight': 8,\n",
              " '9 - nine': 9}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Map the string labels to numerical values (we need this for the PyTorch dataset)\n",
        "label_map = {string_label: index for index, string_label in enumerate(dataset_classes)}\n",
        "label_map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u_VZ-eoIpa6"
      },
      "source": [
        "Transform the PIL images into PyTorch tensors with scaling based on stats from the training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xr_lCpPDvBV"
      },
      "outputs": [],
      "source": [
        "image_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZpMBS8MBbdN",
        "outputId": "0e7005d3-caac-4a3c-b5f0-913f075951bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomTorchImageDataset initialized with 51000 samples.\n"
          ]
        }
      ],
      "source": [
        "torch_train_set = CustomTorchImageDataset(train_dataset,\n",
        "                                          label_map=label_map,\n",
        "                                          image_transforms=image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98QofkjSBqiE",
        "outputId": "2f0c8c09-1915-4dfb-c08e-e65de7937390"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomTorchImageDataset initialized with 9000 samples.\n"
          ]
        }
      ],
      "source": [
        "torch_val_set = CustomTorchImageDataset(val_dataset,\n",
        "                                     label_map=label_map,\n",
        "                                     image_transforms=image_transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4OX2J221s6u"
      },
      "source": [
        "### Create PyTorch DataLoaders\n",
        "\n",
        "DataLoaders wrap our custom datasets and handle the mechanics of training: batching samples together, shuffling data between epochs, and loading images in parallel using multiple CPU cores. The batch size determines how many images the model processes at once, affecting both memory usage and training dynamics. We shuffle the training data to prevent the model from learning spurious patterns based on sample order, but keep validation data unshuffled since evaluation order doesn't matter. Parallel loading with multiple workers speeds up training by preparing the next batch while the GPU processes the current one.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb36104f"
      },
      "outputs": [],
      "source": [
        "# Define batch size (you can adjust this based on your GPU memory)\n",
        "batch_size = 64\n",
        "num_workers = os.cpu_count()  # Number of CPU cores\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uiiduZxDxIZ"
      },
      "outputs": [],
      "source": [
        "train_loader = create_deterministic_training_dataloader(\n",
        "    torch_train_set,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wD9r7mMUDt7B",
        "outputId": "d69966a2-85a0-44ae-c138-92cb630ad058"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train and validation DataLoaders created successfully.\n",
            "Train DataLoader has 797 batches.\n",
            "Validation DataLoader has 141 batches.\n"
          ]
        }
      ],
      "source": [
        "val_loader = torch.utils.data.DataLoader(\n",
        "    torch_val_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # No need to shuffle validation data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Train and validation DataLoaders created successfully.\")\n",
        "print(f\"Train DataLoader has {len(train_loader)} batches.\")\n",
        "print(f\"Validation DataLoader has {len(val_loader)} batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkUDiGQT-U4m"
      },
      "source": [
        "## Instantiate the Loss Function (Categorical Cross Entropy)\n",
        "\n",
        "**Categorical Cross Entropy** is the standard loss function for multi-class classification problems like MNIST digit recognition. It measures how far our model's predicted probability distribution is from the true distribution (one-hot encoded labels).\n",
        "\n",
        "**Mathematical Formula:**\n",
        "$$\\text{CCE} = -\\sum_{i=1}^{C} y_i \\log(\\hat{y}_i)$$\n",
        "\n",
        "Where:\n",
        "- $C$ = number of classes (10 for MNIST digits 0-9)\n",
        "- $y_i$ = true label (1 for correct class, 0 for others)\n",
        "- $\\hat{y}_i$ = predicted probability for class $i$\n",
        "\n",
        "**Intuitions about Cross Entropy Loss**: The loss encourages the model to output high confidence (probability close to 1.0) for the correct class and low confidence for incorrect classes.\n",
        "\n",
        "For a perfectly correct prediction (probability = 1.0 for true class), the loss approaches 0 and gradients are small (meaning that there is little change on weights). For very wrong predictions (probability = 0.001 for the ground truth class), the loss approaches infinity, strongly penalizing confident mistakes and forcing the neural network to update its weights.\n",
        "\n",
        "PyTorch implements a numerically stable variant of CCE based on the LogSoftMax function. You can read more about it [here](https://docs.pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00k7b9P--XdQ"
      },
      "outputs": [],
      "source": [
        "ce_loss = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bK27g21hC68B"
      },
      "source": [
        "## Prepare Training and Validation for our Custom Model\n",
        "\n",
        "We define two functions to handle the training and validation phases of each epoch. The `train_epoch()` function puts the model in training mode, processes batches through forward passes, computes loss, and updates weights via backpropagation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvXszqWQ4dAn"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader):\n",
        "  batch_losses = []\n",
        "  model.train()\n",
        "  for images, labels in tqdm(train_loader, desc=\"Training: \"):\n",
        "      #import pdb; pdb.set_trace()\n",
        "\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      logits = model(images)\n",
        "      loss_value = ce_loss(logits, labels)\n",
        "      # Clear gradients from previous iteration (PyTorch accumulates by default)\n",
        "      optimizer.zero_grad()\n",
        "      # Computes the gradients with backpropagation\n",
        "      loss_value.backward()\n",
        "      # Updates the weights\n",
        "      optimizer.step()\n",
        "\n",
        "      batch_losses.append(loss_value.item())\n",
        "\n",
        "  train_loss = np.mean(batch_losses)\n",
        "  return train_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFGZQrERUTEA"
      },
      "source": [
        "The `val_epoch()` function switches the model to evaluation mode and computes validation loss without updating weights, giving us an unbiased measure of performance on held-out data. These functions return the average loss across all batches, which we'll track to monitor training progress and detect overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mAf9bjcX51pf"
      },
      "outputs": [],
      "source": [
        "def val_epoch(model, val_loader):\n",
        "  batch_losses = []\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for images, labels in tqdm(val_loader, desc=\"Validation: \"):\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "      # Forward pass\n",
        "      logits = model(images)\n",
        "      loss_value = ce_loss(logits, labels)\n",
        "      batch_losses.append(loss_value.item())\n",
        "  val_loss = np.mean(batch_losses)\n",
        "  return val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GK52LQDkDRfW"
      },
      "source": [
        "## Defining the Optimizer\n",
        "\n",
        "**Configuring the Learning Algorithm**\n",
        "\n",
        "The optimizer determines how the neural network updates its weights based on computed gradients during training. This choice affects training speed, stability, and final model performance.\n",
        "\n",
        "**Adam (Adaptive Moment Estimation)** is a variant of gradient descent that maintains running averages of both gradients and their squared values, allowing us to adapt the learning rate based on the historical behavior of each weight.\n",
        "\n",
        "```python\n",
        "optimizer = Adam(model.parameters(),\n",
        "                 lr=0.003, betas=(0.9, 0.999),\n",
        "                 eps=1e-08, weight_decay=0)\n",
        "```\n",
        "\n",
        "The learning rate (lr=0.003) controls the step size for weight updates, while the beta settings control how much history to consider when computing the adaptive rates. This configuration provides stable training for most computer vision tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-SPncALBtCi"
      },
      "outputs": [],
      "source": [
        "model = ModernLeNet5().to(device)\n",
        "\n",
        "# Define the optimizer (variant of stochastic gradient descent)\n",
        "optimizer = Adam(model.parameters(),\n",
        "                 lr=0.003, betas=(0.9, 0.999),\n",
        "                 eps=1e-08, weight_decay=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pqo9S2i7RjDl"
      },
      "source": [
        "## Training and Checkpointing the Model\n",
        "\n",
        "**Iterative Learning with Performance Monitoring**\n",
        "\n",
        "Training a neural network involves showing the model batches of data, computing prediction errors, and updating weights to minimize those errors. This process continues for multiple epochs, where each epoch represents one complete pass through the entire training dataset.\n",
        "\n",
        "**The Role of Validation-Based Checkpointing**\n",
        "\n",
        "During training, we monitor performance on both training and validation sets. Training loss decreases as the model learns, but validation loss reveals the true generalization capability. The validation set acts as a proxy for real-world performance since the model never sees these samples during weight updates.\n",
        "\n",
        "We save model checkpoints based on validation performance rather than training performance to prevent overfitting. A model might achieve low training loss by memorizing training examples, but this doesn't guarantee good performance on new data. By saving the model weights that achieve the best validation loss, we capture the point where the model has learned generalizable patterns without overfitting to training-specific details.\n",
        "\n",
        "```python\n",
        "if val_loss < best_val_loss:\n",
        "    best_val_loss = val_loss\n",
        "    best_model = model\n",
        "    torch.save(best_model.state_dict(), model_save_path)\n",
        "    print('Found and saved better weights for the model')\n",
        "```\n",
        "\n",
        "This checkpointing strategy ensures we retain the model configuration that will perform best on unseen test data, even if training continues and validation performance later degrades due to overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aT-TdBp8KGuI",
        "outputId": "c15b09f3-390c-4285-bf78-4416ff573eb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:07<00:00, 110.47it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 107.03it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Train Loss: 0.2175 - Val Loss: 0.0731\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 121.62it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 117.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Train Loss: 0.0790 - Val Loss: 0.0640\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 120.26it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 109.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Train Loss: 0.0626 - Val Loss: 0.0489\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 124.97it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 118.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Train Loss: 0.0533 - Val Loss: 0.0594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 119.03it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 117.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Train Loss: 0.0490 - Val Loss: 0.0462\n",
            "Found and saved better weights for the model\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 124.66it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 118.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Train Loss: 0.0426 - Val Loss: 0.0522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 118.45it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 117.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Train Loss: 0.0422 - Val Loss: 0.0522\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 125.05it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 107.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Train Loss: 0.0339 - Val Loss: 0.0590\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 120.25it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 112.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Train Loss: 0.0368 - Val Loss: 0.0512\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 797/797 [00:06<00:00, 122.10it/s]\n",
            "Validation: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 106.44it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Train Loss: 0.0356 - Val Loss: 0.0557\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Ensure reproducibility for the training process\n",
        "set_seeds(51) # You can change this number to get different results\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "best_model = None\n",
        "\n",
        "# Define the path to save the model within your hard-drive\n",
        "path = Path(os.getcwd()) # Feel free to change the path\n",
        "\n",
        "model_save_path = path / 'best_lenet.pth'\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_epoch(model, train_loader)\n",
        "    val_loss = val_epoch(model, val_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_model = model\n",
        "        # Save the best model\n",
        "        torch.save(best_model.state_dict(), model_save_path)\n",
        "        print('Found and saved better weights for the model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGC7oxwM9AGV"
      },
      "source": [
        "## Visualizing Training vs Validation Losses\n",
        "\n",
        "Plotting training and validation loss over epochs provides insights into model learning dynamics. These curves reveal whether the model is learning, overfitting, or underfitting the data.\n",
        "In healthy training, both curves decrease together, with training loss lower than validation loss. When training loss continues decreasing while validation loss plateaus or increases, this indicates overfitting where the model memorizes training data rather than learning generalizable patterns. If both curves plateau at high values, the model may be underfitting and require more capacity (e.g.more layers or more weights) or more training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "id": "ponD-bYqKvQy",
        "outputId": "2a4105c2-1e15-4eb6-c379-0472f39cf6ae"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(train_losses, label='Training Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.title('Training and Validation Loss')\n",
        "\n",
        "# Find the epoch with the best validation loss\n",
        "best_epoch = np.argmin(val_losses) + 1 # Add 1 because epochs are 1-indexed\n",
        "\n",
        "# Add a vertical red line at the epoch with the best validation loss\n",
        "plt.axvline(x=best_epoch, color='red', linestyle='--', label=f'Best Val Loss Epoch {best_epoch}')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUY0t6oPcmJH"
      },
      "source": [
        "## Reload the Best LeNet Model Weights\n",
        "\n",
        "After training completes, we need to load the saved model weights that achieved the best validation performance. This ensures we use the model configuration that generalizes best to unseen data, rather than the final training state which may have overfitted.\n",
        "The reloading process involves creating a new model instance with the same architecture, then loading the saved state dictionary containing the optimal weights. We must ensure the model is moved to the correct device (CPU or GPU) and set to evaluation mode for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzA4y5mpOImr",
        "outputId": "aa4f3834-b6c4-4a18-8516-d019402815b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully from /content/best_lenet.pth\n",
            "Model is on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Define the path where the best model was saved\n",
        "path = Path(os.getcwd())\n",
        "model_save_path = path / 'best_lenet.pth'\n",
        "\n",
        "# Instantiate a new model with the same architecture\n",
        "# Make sure you use the same model class that was trained\n",
        "loaded_model = ModernLeNet5()\n",
        "\n",
        "# Load the saved state dictionary into the new model instance\n",
        "# Make sure the model is on the correct device (CPU or GPU)\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
        "\n",
        "# Move the model to the device\n",
        "loaded_model = loaded_model.to(device)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "print(f\"Model loaded successfully from {model_save_path}\")\n",
        "print(f\"Model is on device: {next(loaded_model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VVt65BMRbZo"
      },
      "source": [
        "## Apply our Custom Model to the Test Set\n",
        "\n",
        "We have trained our LeNet-5 model and need to apply it to the test set to generate predictions and evaluate its performance. This step bridges the gap between PyTorch training and FiftyOne's analysis capabilities.\n",
        "\n",
        "**Why Store Predictions as FiftyOne Classifications?**\n",
        "\n",
        "Instead of storing raw predictions as strings or numbers, we use FiftyOne's [`Classification`](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) objects, which provide several key advantages:\n",
        "\n",
        "**Structured Data Storage**: Classification objects encapsulate the predicted label, confidence score, and raw logits in a standardized format that FiftyOne understands.\n",
        "\n",
        "**Evaluation Integration**: FiftyOne's evaluation framework (`evaluate_classifications()`) can compare Classification objects against ground truth labels, generating metrics like confusion matrices, per-class accuracy, and performance reports.\n",
        "\n",
        "**Querying and Filtering**: With Classification objects, we can filter samples by confidence thresholds, find misclassifications, or identify uncertain predictions using FiftyOne's query language:\n",
        "\n",
        "```python\n",
        "# Find high-confidence predictions\n",
        "high_conf = dataset.match(F(\"predictions.confidence\") > 0.95)\n",
        "\n",
        "# Find misclassifications  \n",
        "errors = dataset.match(F(\"predictions.label\") != F(\"ground_truth.label\"))\n",
        "```\n",
        "\n",
        "**Visual Analysis**: The FiftyOne App can visualize Classification objects with confidence scores, making it simple to spot patterns in model behavior and identify errors.\n",
        "\n",
        "**Model Comparison**: Storing predictions in this standardized format enables comparison between different models (like our LeNet vs. CLIP's zero-shot classification) using the same evaluation framework.\n",
        "\n",
        "**Confidence-Based Analysis**: The embedded confidence scores allow for analysis like identifying samples where the model is uncertain, which correspond to edge cases or potential labeling errors in the dataset.\n",
        "\n",
        "This approach transforms raw model outputs into queryable metadata that integrates with FiftyOne's computer vision workflow, enabling insights into model performance and behavior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtL5ZfG6YYXe",
        "outputId": "5dc5b451-f43e-4d23-97bd-1ce3d68d87bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CustomTorchImageDataset initialized with 10000 samples.\n",
            "Test DataLoader created successfully.\n",
            "Test DataLoader has 157 batches.\n",
            "Applying best LeNet model to the test set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:01<00:00, 116.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference on test set complete.\n",
            "Shape of collected logits: (10000, 10)\n",
            "Number of collected predictions: 10000\n",
            "Storing predictions and logits as FiftyOne Classifications...\n"
          ]
        }
      ],
      "source": [
        "## Apply best_model to the test set, store logits and confidence\n",
        "\n",
        "# Create a PyTorch Dataset for the test set\n",
        "torch_test_set = CustomTorchImageDataset(test_dataset,\n",
        "                                      label_map=label_map, # Use the same label map as training\n",
        "                                      image_transforms=image_transforms) # Use the same transforms\n",
        "\n",
        "# Create a PyTorch DataLoader for the test set\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    torch_test_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False, # No need to shuffle test data\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(\"Test DataLoader created successfully.\")\n",
        "print(f\"Test DataLoader has {len(test_loader)} batches.\")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Lists to store predictions and logits\n",
        "predictions = []\n",
        "all_logits = []\n",
        "\n",
        "# Run inference on the test set\n",
        "print(\"Applying best LeNet model to the test set...\")\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    for images, _ in tqdm(test_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "all_logits = np.concatenate(all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on test set complete.\")\n",
        "print(f\"Shape of collected logits: {all_logits.shape}\")\n",
        "print(f\"Number of collected predictions: {len(predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications...\")\n",
        "\n",
        "for i, sample in enumerate(test_dataset):\n",
        "    # Get the predicted class index and corresponding class name\n",
        "    predicted_idx = predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "\n",
        "    # Get logits for this sample\n",
        "    sample_logits = all_logits[i]\n",
        "\n",
        "    # Calculate confidence scores (softmax applied to logits)\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    # Create FiftyOne Classification object with prediction\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()  # Store raw logits\n",
        "    )\n",
        "\n",
        "    # Store the Classification object in the sample\n",
        "    sample[\"lenet_classification\"] = classification\n",
        "\n",
        "    sample.save()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0LdkBKqY4FT"
      },
      "source": [
        "### Verify the Stored Data Structure for Predictions\n",
        "We should see `Classification` objects with label, confidence, and logits fields. We need these to perform analytics on our FiftyOne dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "mgBctL3WY61A",
        "outputId": "84d755b6-00e7-4843-f231-13c904d3ba9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Verification ===\n",
            "Sample prediction type: <class 'fiftyone.core.labels.Classification'>\n",
            "Sample prediction: <Classification: {\n",
            "    'id': '68515249e6f3512ccfd470a0',\n",
            "    'tags': [],\n",
            "    'label': '7 - seven',\n",
            "    'confidence': 1.0,\n",
            "    'logits': array([-20.62574768, -10.11302567,  -3.73636937,  -1.80681992,\n",
            "            -8.75860214, -18.0674305 , -33.1974411 ,  16.05401611,\n",
            "           -15.19614124,  -5.64339733]),\n",
            "}>\n",
            "Prediction label: 7 - seven\n",
            "Prediction confidence: 1.0\n",
            "Prediction logits shape: 10\n",
            "\n",
            "FiftyOne App URL: https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Verification ===\")\n",
        "sample = test_dataset.first()\n",
        "print(f\"Sample prediction type: {type(sample.lenet_classification)}\")\n",
        "print(f\"Sample prediction: {sample.lenet_classification}\")\n",
        "print(f\"Prediction label: {sample.lenet_classification.label}\")\n",
        "print(f\"Prediction confidence: {sample.lenet_classification.confidence}\")\n",
        "print(f\"Prediction logits shape: {len(sample.lenet_classification.logits)}\")\n",
        "\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWumhgwGZSNE"
      },
      "source": [
        "### Create a View Showing only LeNet's Mistakes\n",
        "\n",
        "The FiftyOne `Classification` object allows to do filtering on the samples where we have issues.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBzGnUgIaYBj",
        "outputId": "26449746-6f21-4aa0-fd8f-1c9a07171e1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Misclassified samples: 114 samples\n"
          ]
        }
      ],
      "source": [
        "# Create a view showing only LeNet's misclassifications\n",
        "misclassified_view = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "print(f\"Misclassified samples: {len(misclassified_view)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAUyD2N1Z039"
      },
      "source": [
        "## Evaluating LeNet's Classification Performance\n",
        "\n",
        "After applying our trained LeNet model to the test set, we need to evaluate its performance against the ground truth labels. This evaluation goes beyond simple accuracy to provide detailed insights into where and how the model succeeds or fails.\n",
        "FiftyOne's evaluation framework generates metrics including per-class precision, recall, and F1-scores, along with confusion matrices that reveal which digit pairs the model most often confuses. This analysis helps identify weaknesses and guides future improvements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "FbdrK2HVdL6t",
        "outputId": "c4e554ac-7de3-41d3-ef32-7d2de8d0f246"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "lenet_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"lenet_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"lenet_eval\")\n",
        "\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdP_CpjJhPRZ",
        "outputId": "5a06921a-9b53-46db-cf4e-377b76b54ac6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    0 - zero      0.990     0.996     0.993       980\n",
            "     1 - one      0.993     0.994     0.993      1135\n",
            "     2 - two      0.986     0.991     0.989      1032\n",
            "   3 - three      0.978     0.992     0.985      1010\n",
            "    4 - four      0.983     0.997     0.990       982\n",
            "    5 - five      0.984     0.989     0.987       892\n",
            "     6 - six      0.997     0.984     0.991       958\n",
            "   7 - seven      0.988     0.975     0.981      1028\n",
            "   8 - eight      0.996     0.990     0.993       974\n",
            "    9 - nine      0.991     0.978     0.985      1009\n",
            "\n",
            "    accuracy                          0.989     10000\n",
            "   macro avg      0.989     0.989     0.989     10000\n",
            "weighted avg      0.989     0.989     0.989     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "lenet_evaluation_results.print_report(digits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 666
        },
        "id": "jahmUa2_SuM2",
        "outputId": "f70f1790-8f79-469b-8417-4c5027d13b4b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/fiftyone/core/plots/plotly.py:1591: UserWarning:\n",
            "\n",
            "Interactive plots are currently only supported in Jupyter notebooks. Support outside of notebooks and in Google Colab and Databricks will be included in an upcoming release. In the meantime, you can still use this plot, but note that (i) selecting data will not trigger callbacks, and (ii) you must manually call `plot.show()` to launch a new plot that reflects the current state of an attached session.\n",
            "\n",
            "See https://docs.voxel51.com/user_guide/plots.html#working-in-notebooks for more information.\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"001717d1-8515-48a0-bab5-812fb9722cb4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"001717d1-8515-48a0-bab5-812fb9722cb4\")) {                    Plotly.newPlot(                        \"001717d1-8515-48a0-bab5-812fb9722cb4\",                        [{\"mode\":\"markers\",\"opacity\":0.1,\"x\":[0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9,0,1,2,3,4,5,6,7,8,9],\"y\":[0,0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,2,3,3,3,3,3,3,3,3,3,3,4,4,4,4,4,4,4,4,4,4,5,5,5,5,5,5,5,5,5,5,6,6,6,6,6,6,6,6,6,6,7,7,7,7,7,7,7,7,7,7,8,8,8,8,8,8,8,8,8,8,9,9,9,9,9,9,9,9,9,9],\"type\":\"scatter\",\"uid\":\"e8fdb99f-c909-4841-baeb-9d2084880666\"},{\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hoverinfo\":\"skip\",\"showscale\":false,\"z\":[[0,0,0,2,8,6,0,5,1,987],[3,0,2,1,1,2,0,0,964,1],[0,5,8,7,2,0,0,1002,0,4],[3,2,1,0,6,2,943,0,1,0],[3,0,0,5,0,882,1,0,0,1],[0,0,0,0,979,0,0,0,0,3],[1,0,3,1002,0,3,0,1,0,0],[0,1,1023,2,0,0,0,5,1,0],[0,1128,0,6,0,1,0,0,0,0],[976,0,0,0,0,0,2,1,1,0]],\"zmax\":1128,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"b6308861-374f-46da-bb6c-8c17398ced44\"},{\"colorbar\":{\"len\":1,\"lenmode\":\"fraction\"},\"colorscale\":[[0.0,\"rgb(255,245,235)\"],[0.125,\"rgb(254,230,206)\"],[0.25,\"rgb(253,208,162)\"],[0.375,\"rgb(253,174,107)\"],[0.5,\"rgb(253,141,60)\"],[0.625,\"rgb(241,105,19)\"],[0.75,\"rgb(217,72,1)\"],[0.875,\"rgb(166,54,3)\"],[1.0,\"rgb(127,39,4)\"]],\"hovertemplate\":\"\\u003cb\\u003ecount: %{z}\\u003c\\u002fb\\u003e\\u003cbr\\u003eground_truth: %{y}\\u003cbr\\u003elenet_classification: %{x}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"opacity\":0.25,\"z\":[[0,0,0,2,8,6,0,5,1,987],[3,0,2,1,1,2,0,0,964,1],[0,5,8,7,2,0,0,1002,0,4],[3,2,1,0,6,2,943,0,1,0],[3,0,0,5,0,882,1,0,0,1],[0,0,0,0,979,0,0,0,0,3],[1,0,3,1002,0,3,0,1,0,0],[0,1,1023,2,0,0,0,5,1,0],[0,1128,0,6,0,1,0,0,0,0],[976,0,0,0,0,0,2,1,1,0]],\"zmax\":1128,\"zmin\":0,\"type\":\"heatmap\",\"uid\":\"ebef63f2-6514-41bc-ab08-ed31ad6fc6c5\"}],                        {\"clickmode\":\"event\",\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"rgb(51,51,51)\"},\"error_y\":{\"color\":\"rgb(51,51,51)\"},\"marker\":{\"line\":{\"color\":\"rgb(237,237,237)\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"baxis\":{\"endlinecolor\":\"rgb(51,51,51)\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"rgb(51,51,51)\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"},\"colorscale\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"rgb(237,237,237)\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"rgb(217,217,217)\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"tickcolor\":\"rgb(237,237,237)\",\"ticklen\":6,\"ticks\":\"inside\"}},\"colorscale\":{\"sequential\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]],\"sequentialminus\":[[0,\"rgb(20,44,66)\"],[1,\"rgb(90,179,244)\"]]},\"colorway\":[\"#F8766D\",\"#A3A500\",\"#00BF7D\",\"#00B0F6\",\"#E76BF3\"],\"font\":{\"color\":\"rgb(51,51,51)\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"rgb(237,237,237)\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"white\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"rgb(237,237,237)\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"yaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"},\"zaxis\":{\"backgroundcolor\":\"rgb(237,237,237)\",\"gridcolor\":\"white\",\"gridwidth\":2,\"linecolor\":\"white\",\"showbackground\":true,\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"zerolinecolor\":\"white\"}},\"shapedefaults\":{\"fillcolor\":\"black\",\"line\":{\"width\":0},\"opacity\":0.3},\"ternary\":{\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"},\"bgcolor\":\"rgb(237,237,237)\",\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\"}},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showgrid\":true,\"tickcolor\":\"rgb(51,51,51)\",\"ticks\":\"outside\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\"}}},\"xaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"tickmode\":\"array\",\"ticktext\":[\"0 - zero\",\"1 - one\",\"2 - two\",\"3 - three\",\"4 - four\",\"5 - five\",\"6 - six\",\"7 - seven\",\"8 - eight\",\"9 - nine\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"yaxis\":{\"constrain\":\"domain\",\"range\":[-0.5,9.5],\"scaleanchor\":\"x\",\"scaleratio\":1,\"tickmode\":\"array\",\"ticktext\":[\"9 - nine\",\"8 - eight\",\"7 - seven\",\"6 - six\",\"5 - five\",\"4 - four\",\"3 - three\",\"2 - two\",\"1 - one\",\"0 - zero\"],\"tickvals\":[0,1,2,3,4,5,6,7,8,9]},\"margin\":{\"r\":0,\"t\":30,\"l\":0,\"b\":0},\"title\":{}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('001717d1-8515-48a0-bab5-812fb9722cb4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lenet_evaluation_results.plot_confusion_matrix()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0Y0qZVhAqOn",
        "outputId": "c493eb97-41cb-4b41-ad78-791cafe0a6d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.7184225916862488, 0.9946229457855225, 0.9999970197677612, 1.0, 1.0]"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Compute quantiles of confidence\n",
        "test_dataset.quantiles(\"lenet_classification.confidence\", [0.01, 0.05, 0.25, 0.5, 0.75] )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JleYduhreBg6",
        "outputId": "bec60c95-f67a-4891-dcd6-856ab52fbe4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset:     mnist-test\n",
              "Media type:  image\n",
              "Num samples: 78\n",
              "Sample fields:\n",
              "    id:                            fiftyone.core.fields.ObjectIdField\n",
              "    filepath:                      fiftyone.core.fields.StringField\n",
              "    tags:                          fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)\n",
              "    metadata:                      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)\n",
              "    created_at:                    fiftyone.core.fields.DateTimeField\n",
              "    last_modified_at:              fiftyone.core.fields.DateTimeField\n",
              "    ground_truth:                  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    clip_embeddings:               fiftyone.core.fields.VectorField\n",
              "    clip_zero_shot_classification: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    clip_zero_shot_eval:           fiftyone.core.fields.BooleanField\n",
              "    lenet_classification:          fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Classification)\n",
              "    lenet_eval:                    fiftyone.core.fields.BooleanField\n",
              "View stages:\n",
              "    1. Match(filter={'$expr': {'$lt': [...]}})"
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "low_confidence_view = test_dataset.match(F(\"lenet_classification.confidence\") < 0.69)\n",
        "low_confidence_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "lKnI5jJceXMK",
        "outputId": "13b6faa4-04d6-4d09-aef5-eb3f78528994"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "session.view = low_confidence_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eSfe8gg-yF_"
      },
      "source": [
        "## Evaluating Sample Hardness and Mistakenness through Logits\n",
        "\n",
        "**Quantifying Prediction Uncertainty and Label Quality**\n",
        "\n",
        "Model logits contain rich information beyond simple predictions. By analyzing the raw output scores before softmax conversion (the logits), we can compute metrics that reveal which samples the model finds challenging and which labels may be questionable.\n",
        "\n",
        "**Hardness** measures prediction uncertainty based on the model's confidence distribution. Samples with high hardness have flat probability distributions, indicating the model struggles to distinguish between classes. These often represent genuinely difficult cases or edge cases in the data.\n",
        "\n",
        "**Mistakenness** identifies samples where the model's confident predictions disagree with ground truth labels. High mistakenness scores suggest potential annotation errors rather than model failures, as the model may have learned correct patterns that conflict with incorrect labels.\n",
        "\n",
        "```python\n",
        "# Compute hardness based on prediction uncertainty\n",
        "fob.compute_hardness(test_dataset, label_field='lenet_classification')\n",
        "\n",
        "# Compute mistakenness to identify potential label errors  \n",
        "fob.compute_mistakenness(test_dataset,\n",
        "                        pred_field=\"lenet_classification\",\n",
        "                        label_field=\"ground_truth\")\n",
        "```\n",
        "\n",
        "These metrics transform raw model scores (logits) into actionable insights for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tv9eUU8H7W8Q",
        "outputId": "c31b536b-39a4-4232-bfa6-c72ae4b16b84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing hardness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.hardness:Computing hardness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [18.8s elapsed, 0s remaining, 537.6 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [18.8s elapsed, 0s remaining, 537.6 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hardness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.hardness:Hardness computation complete\n"
          ]
        }
      ],
      "source": [
        "#Hardness is a measure computed based on model prediction output (through\n",
        "#logits) that summarizes a measure of the uncertainty the model had with the\n",
        "#sample. This makes hardness quantitative and can be used to detect things\n",
        "#like hard samples and annotation errors\n",
        "fob.compute_hardness(test_dataset,\n",
        "                     label_field='lenet_classification',\n",
        "                     )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnVwG6OHpQ7Z",
        "outputId": "9b44f562-00fb-4677-93e6-71be1531f46e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing mistakenness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.mistakenness:Computing mistakenness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [15.6s elapsed, 0s remaining, 645.1 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [15.6s elapsed, 0s remaining, 645.1 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mistakenness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.mistakenness:Mistakenness computation complete\n"
          ]
        }
      ],
      "source": [
        "# Evaluate sample mistakenness (how likely the sample is mislabeled)\n",
        "# Samples with high mistakenness often have conflicting model output and ground truth\n",
        "fob.compute_mistakenness(test_dataset,\n",
        "                         pred_field=\"lenet_classification\",\n",
        "                         label_field=\"ground_truth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "B6UHt8iEBFsq",
        "outputId": "0364b685-e42f-47b3-ed5a-3ec51ab77280"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mistakenness quantiles for train_dataset:\n",
            "[0.00015681982040405273, 0.0028665363788604736, 0.7455840557813644]\n"
          ]
        }
      ],
      "source": [
        "# Compute quantiles of mistakenness\n",
        "mistakenness_quantiles = test_dataset.quantiles(\n",
        "    \"mistakenness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nMistakenness quantiles for train_dataset:\")\n",
        "print(mistakenness_quantiles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ykeRfSFCgah"
      },
      "outputs": [],
      "source": [
        "suspicious_test_samples_view = test_dataset.match(\n",
        "                             F(\"mistakenness\") > mistakenness_quantiles[-1]\n",
        "                             ).sort_by(\"mistakenness\", reverse=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "8cHQbbD7Bms8",
        "outputId": "079daad2-803c-480e-dd76-2fbd9ac71807"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FiftyOne App URL showing top mistakenness in the test set: https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "# Set the session view to one of the views to visualize it in the App\n",
        "session.view = suspicious_test_samples_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL showing top mistakenness in the test set: {session.url}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K2DNWgg-qSj"
      },
      "source": [
        "## Add Classifications to the Validation Dataset\n",
        "\n",
        "We need to have Classification objects on the validation set to evaluate the quality of the validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUQeg3dY-xSP",
        "outputId": "c3a30cce-7627-40de-8adb-4b6d1848996d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applying best LeNet model to the validation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 141/141 [00:01<00:00, 115.91it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference on validation set complete.\n",
            "Shape of collected validation logits: (9000, 10)\n",
            "Number of collected validation predictions: 9000\n",
            "Storing predictions and logits as FiftyOne Classifications for validation set...\n",
            "Predictions and logits stored successfully as FiftyOne Classifications for validation set.\n"
          ]
        }
      ],
      "source": [
        "print(\"Applying best LeNet model to the validation set...\")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "\n",
        "# Lists to store predictions and logits\n",
        "val_predictions = []\n",
        "val_all_logits = []\n",
        "\n",
        "# Run inference on the validation set\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    for images, _ in tqdm(val_loader):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        val_all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        val_predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "val_all_logits = np.concatenate(val_all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on validation set complete.\")\n",
        "print(f\"Shape of collected validation logits: {val_all_logits.shape}\")\n",
        "print(f\"Number of collected validation predictions: {len(val_predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications for validation set...\")\n",
        "\n",
        "for i, sample in enumerate(val_dataset):\n",
        "    # Get the predicted class index and corresponding class name\n",
        "    predicted_idx = val_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "\n",
        "    # Get logits for this sample\n",
        "    sample_logits = val_all_logits[i]\n",
        "\n",
        "    # Calculate confidence scores (softmax applied to logits)\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    # Create FiftyOne Classification object with prediction\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()  # Store raw logits\n",
        "    )\n",
        "\n",
        "    # Store the Classification object in the sample\n",
        "    sample[\"lenet_validation_classification\"] = classification\n",
        "\n",
        "    sample.save()\n",
        "\n",
        "print(\"Predictions and logits stored successfully as FiftyOne Classifications for validation set.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmZvGqsLAwuC",
        "outputId": "15bcacde-9a67-4b54-9bc8-42a8bfe23017"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing hardness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.hardness:Computing hardness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9000/9000 [16.6s elapsed, 0s remaining, 547.0 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9000/9000 [16.6s elapsed, 0s remaining, 547.0 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hardness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.hardness:Hardness computation complete\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing mistakenness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.mistakenness:Computing mistakenness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9000/9000 [13.6s elapsed, 0s remaining, 683.4 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9000/9000 [13.6s elapsed, 0s remaining, 683.4 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mistakenness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.mistakenness:Mistakenness computation complete\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mistakenness quantiles for validation_dataset:\n",
            "[0.0002671182155609131, 0.004684776067733765, 0.797028124332428]\n",
            "\n",
            "Hardness quantiles for validation_dataset:\n",
            "[0.004648440794701663, 0.05443552491087599, 0.6974457442123232]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Evaluate sample hardness (how hard the sample is for the model)\n",
        "# Hardness is a measure computed based on model prediction output (through\n",
        "# logits) that summarizes a measure of the uncertainty the model had with the\n",
        "# sample. This makes hardness quantitative and can be used to detect things\n",
        "# like hard samples and annotation errors\n",
        "fob.compute_hardness(val_dataset,\n",
        "                     label_field='lenet_validation_classification',\n",
        "                     )\n",
        "\n",
        "# Evaluate sample mistakenness (how likely the sample is mislabeled)\n",
        "# Samples with high mistakenness often have conflicting model output and ground truth\n",
        "fob.compute_mistakenness(val_dataset,\n",
        "                         pred_field=\"lenet_validation_classification\",\n",
        "                         label_field=\"ground_truth\")\n",
        "\n",
        "# Compute quantiles of mistakenness for the validation set\n",
        "mistakenness_quantiles_val = val_dataset.quantiles(\n",
        "    \"mistakenness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nMistakenness quantiles for validation_dataset:\")\n",
        "print(mistakenness_quantiles_val)\n",
        "\n",
        "# Compute quantiles of hardness for the validation set\n",
        "hardness_quantiles_val = val_dataset.quantiles(\n",
        "    \"hardness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nHardness quantiles for validation_dataset:\")\n",
        "print(hardness_quantiles_val)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "IqMQrU09As8L",
        "outputId": "ecbd6d76-aaab-41a2-f87a-d20beda4d776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "FiftyOne App URL showing top mistakenness in the validation set: https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "# Create a view of samples with high mistakenness in the validation set\n",
        "suspicious_val_samples_view = val_dataset.match(\n",
        "                             F(\"mistakenness\") > mistakenness_quantiles_val[-1]\n",
        "                             ).sort_by(\"mistakenness\", reverse=True)\n",
        "\n",
        "# Set the session view to one of the views to visualize it in the App\n",
        "session.view = suspicious_val_samples_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL showing top mistakenness in the validation set: {session.url}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQVHjaBsCA9H"
      },
      "outputs": [],
      "source": [
        "bad_val_images_paths = set([\n",
        "    \"/root/fiftyone/mnist/train/data/005973.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/026623.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/025160.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/010383.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/031607.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/027027.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/013185.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/041291.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/032343.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/049961.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/031851.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/007348.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/035311.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/030417.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/008385.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/016749.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/031743.jpg\",\n",
        "    \"/root/fiftyone/mnist/train/data/023869.jpg\"\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqxRHKEyAoot"
      },
      "outputs": [],
      "source": [
        "# Create a view of samples with high hardness in the validation set\n",
        "hard_val_samples_view = val_dataset.match(\n",
        "                         F(\"hardness\") > hardness_quantiles_val[-1]\n",
        "                         ).sort_by(\"hardness\", reverse=True)\n",
        "\n",
        "# Set the session view to visualize hard samples\n",
        "session.view = hard_val_samples_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL showing top hardness in the validation set: {session.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAQh1hXIrSS7"
      },
      "source": [
        "## Creating Embeddings from the LeNet-5 Model\n",
        "\n",
        "**Extracting Internal Feature Representations**\n",
        "\n",
        "Neural networks process information through multiple layers, with each layer transforming input data into increasingly abstract representations. While we typically focus on the final output layer for predictions, the intermediate layers contain valuable feature representations that reveal what the model has learned about the input data.\n",
        "\n",
        "LeNet-5's architecture builds features hierarchically: early convolutional layers detect edges and simple patterns, while deeper layers combine these into more complex digit-specific features. The fully connected layers before the final classification layer contain rich embeddings that capture the essential characteristics the model uses to distinguish between digit classes.\n",
        "\n",
        "By extracting these learned embeddings, we can analyze how our trained model represents each digit internally. Unlike pre-trained embeddings from models like CLIP, these features are optimized for our specific task of handwritten digit recognition. This makes them particularly valuable for understanding model behavior, identifying challenging samples, and comparing how different digits cluster in the learned feature space.\n",
        "\n",
        "The extraction process uses PyTorch hooks to capture intermediate layer outputs during inference, allowing us to access the 84-dimensional feature vectors from the penultimate layer that encode each sample's representation in the model's learned space.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cxsL2g-Rstpq"
      },
      "outputs": [],
      "source": [
        "## Extract Embeddings from LeNet Model Using PyTorch Hooks\n",
        "\n",
        "def extract_lenet_embeddings(model, dataloader, device, layer_name='fc1'):\n",
        "    \"\"\"\n",
        "    Extract embeddings from a specified layer of the LeNet model using PyTorch hooks.\n",
        "\n",
        "    Args:\n",
        "        model: Trained LeNet model\n",
        "        dataloader: PyTorch DataLoader\n",
        "        device: Device to run inference on\n",
        "        layer_name: Name of the layer to extract embeddings from\n",
        "                   Options: 'conv3', 'fc1', or 'fc2'\n",
        "\n",
        "    Returns:\n",
        "        numpy array of embeddings\n",
        "    \"\"\"\n",
        "    # Dictionary to store the embeddings\n",
        "    embeddings_dict = {}\n",
        "\n",
        "    def hook_fn(module, input, output):\n",
        "        \"\"\"Hook function to capture layer outputs\"\"\"\n",
        "        # Flatten the output if it's from conv layers\n",
        "        if len(output.shape) > 2:\n",
        "            embeddings_dict['embeddings'] = output.view(output.size(0), -1).cpu().detach()\n",
        "        else:\n",
        "            embeddings_dict['embeddings'] = output.cpu().detach()\n",
        "\n",
        "    # Register the hook on the specified layer\n",
        "    layer_map = {\n",
        "        'conv3': model.conv3,  # Shape: (batch_size, 120, 1, 1) -> flattened to (batch_size, 120)\n",
        "        'fc1': model.fc1,     # Shape: (batch_size, 84) - most common choice\n",
        "        'fc2': model.fc2      # Shape: (batch_size, 10) - final logits\n",
        "    }\n",
        "\n",
        "    if layer_name not in layer_map:\n",
        "        raise ValueError(f\"Invalid layer_name. Choose from: {list(layer_map.keys())}\")\n",
        "\n",
        "    target_layer = layer_map[layer_name]\n",
        "    hook_handle = target_layer.register_forward_hook(hook_fn)\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    all_embeddings = []\n",
        "\n",
        "    print(f\"Extracting embeddings from {layer_name} layer...\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, _ in tqdm(dataloader, desc=\"Processing batches\"):\n",
        "            images = images.to(device)\n",
        "\n",
        "            # Forward pass (hook will capture the embeddings)\n",
        "            _ = model(images)\n",
        "\n",
        "            # Store the captured embeddings\n",
        "            batch_embeddings = embeddings_dict['embeddings'].numpy()\n",
        "            all_embeddings.append(batch_embeddings)\n",
        "\n",
        "    # Remove the hook to clean up\n",
        "    hook_handle.remove()\n",
        "\n",
        "    # Concatenate all embeddings\n",
        "    final_embeddings = np.concatenate(all_embeddings, axis=0)\n",
        "\n",
        "    print(f\"Extracted embeddings shape: {final_embeddings.shape}\")\n",
        "    print(f\"Embedding dimension: {final_embeddings.shape[1]}\")\n",
        "\n",
        "    return final_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_L9gSqK6mg5"
      },
      "source": [
        "# Create a new DataLoader for the train_set  for inference\n",
        "\n",
        "It's important to ensure that shuffling is disabled for this training data `DataLoader`, otherwise our predictions will not match the corresponding data points when we store them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d73fcf8b"
      },
      "outputs": [],
      "source": [
        "# This uses the torch_train_set which is derived from train_dataset\n",
        "train_inference_loader = torch.utils.data.DataLoader(\n",
        "    torch_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,    # CRITICAL: Must be False for ordered predictions\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Set the loaded model to evaluation mode\n",
        "loaded_model.eval()\n",
        "\n",
        "# Lists to store predictions and logits for the training set\n",
        "train_predictions = []\n",
        "train_all_logits = []\n",
        "\n",
        "# Run inference on the training set using the NON-SHUFFLED loader\n",
        "print(\"Applying LeNet model to the train_dataset (using non-shuffled loader)...\")\n",
        "with torch.inference_mode(): # Disable gradient calculation\n",
        "    # Use the new train_inference_loader\n",
        "    for images, _ in tqdm(train_inference_loader, desc=\"Processing train batches for inference\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Forward pass to get logits\n",
        "        logits = loaded_model(images)\n",
        "        train_all_logits.append(logits.cpu().numpy()) # Store logits\n",
        "\n",
        "        # Get predicted class indices\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        train_predictions.extend(predicted.cpu().numpy()) # Store predictions\n",
        "\n",
        "# Concatenate logits from all batches\n",
        "train_all_logits = np.concatenate(train_all_logits, axis=0)\n",
        "\n",
        "print(\"Inference on train_dataset complete.\")\n",
        "print(f\"Shape of collected train logits: {train_all_logits.shape}\")\n",
        "print(f\"Number of collected train predictions: {len(train_predictions)}\")\n",
        "\n",
        "# Store the predictions and logits back into the FiftyOne dataset as Classification objects\n",
        "print(\"Storing predictions and logits as FiftyOne Classifications for train_dataset...\")\n",
        "for i, sample in enumerate(tqdm(train_dataset, desc=\"Storing train classifications\")):\n",
        "    predicted_idx = train_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx] # Assuming dataset_classes is consistent\n",
        "    sample_logits = train_all_logits[i]\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()\n",
        "    )\n",
        "    sample[\"lenet_train_classification\"] = classification\n",
        "    sample.save()\n",
        "\n",
        "print(\"Predictions and logits stored successfully as FiftyOne Classifications for train_dataset.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1RwWM6aa19b"
      },
      "outputs": [],
      "source": [
        "lenet_train_evaluation_results = train_dataset.evaluate_classifications(\n",
        "    \"lenet_train_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"lenet_train_eval\"\n",
        ")\n",
        "session.refresh()\n",
        "lenet_train_evaluation_results.print_report(digits=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MimuXxRsTvP4"
      },
      "outputs": [],
      "source": [
        "session.view = train_dataset.view()\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOivFC0taQk6"
      },
      "source": [
        "## Creating a View of the Mislabeled Images in the Training Data\n",
        "\n",
        "**Identifying Potential Annotation Errors Through Model Predictions**\n",
        "\n",
        "When a trained model disagrees with ground truth labels, this often reveals annotation errors rather than model failures. A well-trained neural network that has learned robust patterns from thousands of examples may be more reliable than human annotators on ambiguous cases. This principle becomes valuable for quality control in large datasets where manual verification of every sample is impractical.\n",
        "\n",
        "**The Logic Behind Mislabel Detection**\n",
        "\n",
        "Models learn statistical patterns across the entire dataset. When a model with high accuracy predicts a different label than the ground truth, several scenarios are possible: the model made an error, the annotation is incorrect, or the sample is ambiguous. By examining these disagreements, we can identify samples that warrant human review.\n",
        "\n",
        "For MNIST, known annotation errors exist in the original dataset. Research has identified several hundred ambiguous or mislabeled samples where even human experts disagree on the correct digit. Our trained LeNet model, having learned from 50,000+ examples, may detect these problematic cases more than the original annotators.\n",
        "\n",
        "Creating a filtered view of these disagreements allows us to focus human attention on the most questionable samples. This approach scales annotation quality control from reviewing entire datasets to examining only the cases where model and annotator disagree.\n",
        "\n",
        "This approach to finding annotation errors represents a powerful application of model predictions for dataset improvement, transforming disagreements between model and labels into opportunities for enhanced data quality.\n",
        "\n",
        "We focus on the mislabeled images from the training set, as these are the ones that we should first **augment** to improve the performance of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auV2OrzEFzdp"
      },
      "outputs": [],
      "source": [
        "# A view of mislabeled images from the training set\n",
        "mislabeled_train_images_view = \\\n",
        "train_dataset.match(\n",
        "    F(\"lenet_train_classification.label\")!= F(\"ground_truth.label\"))\n",
        "\n",
        "mislabeled_train_images_view\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TX7nYlhciQHW"
      },
      "outputs": [],
      "source": [
        "session.view = mislabeled_train_images_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9rqX4iDi6Tq"
      },
      "source": [
        "## Extract Embeddings from the Trained LeNet\n",
        "\n",
        "**Capturing Learned Feature Representations**\n",
        "\n",
        "Neural networks learn hierarchical feature representations through their layers. While the final layer produces class predictions, intermediate layers contain rich embeddings that capture the visual patterns the model has learned to distinguish digits. These learned features often differ from pre-trained embeddings like CLIP, as they are optimized for the specific task of handwritten digit recognition.\n",
        "\n",
        "Extracting embeddings from our trained LeNet model allows us to analyze what visual concepts the network has learned. The fully connected layer before the final classification layer (fc1) produces 84-dimensional vectors that represent each digit in the feature space that the model uses for decision-making.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkdkbeSFuwKI"
      },
      "outputs": [],
      "source": [
        "print(\"Creating non-shuffled DataLoader for embedding extraction...\")\n",
        "\n",
        "# Create a DataLoader specifically for inference with NO SHUFFLING\n",
        "train_inference_loader = torch.utils.data.DataLoader(\n",
        "    torch_train_set,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "print(f\"Non-shuffled inference DataLoader created with {len(train_inference_loader)} batches\")\n",
        "\n",
        "# Extract embeddings from the fc1 layer (84-dimensional representations)\n",
        "print(\"Extracting LeNet embeddings with proper sample ordering...\")\n",
        "\n",
        "lenet_embeddings = extract_lenet_embeddings(\n",
        "    model=loaded_model,\n",
        "    dataloader=train_inference_loader,  # âœ… Use non-shuffled loader\n",
        "    device=device,\n",
        "    layer_name='fc1'  # 84-dimensional embeddings from fully connected layer\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nq7o80pLz5cd"
      },
      "outputs": [],
      "source": [
        "# Wrap embeddings to their associated filenames ()\n",
        "embeddings_dict = {img_path: emb for img_path, emb in zip(torch_train_set.image_paths, lenet_embeddings)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IErbAydN5hRz"
      },
      "source": [
        "## Store LeNet Embeddings in FiftyOne Dataset\n",
        "\n",
        "**Integrating Model Features with Dataset Metadata**\n",
        "\n",
        "After extracting embeddings from our trained LeNet model, we need to store them in our FiftyOne dataset to enable analysis and visualization. This integration allows us to leverage FiftyOne's powerful querying and visualization capabilities on the learned feature representations.\n",
        "\n",
        "Storing embeddings as sample fields transforms abstract neural network features into queryable fields. We can then compute similarity indices, create visualizations, and analyze how the model's learned representations relate to prediction accuracy and sample characteristics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iXPUdi3u2v3"
      },
      "outputs": [],
      "source": [
        "print(\"Storing LeNet embeddings in FiftyOne dataset...\")\n",
        "\n",
        "# Store embeddings in each sample\n",
        "for index, sample in enumerate(tqdm(train_dataset, desc=\"Storing embeddings\")):\n",
        "    sample[\"lenet_embeddings\"] = lenet_embeddings[index]\n",
        "    sample.save()\n",
        "\n",
        "print(\"LeNet embeddings stored successfully in samples from train_dataset.\")\n",
        "\n",
        "# Verify storage\n",
        "sample = train_dataset.first()\n",
        "print(f\"Sample LeNet embedding shape: {sample.lenet_embeddings.shape}\")\n",
        "print(f\"Embedding type: {type(sample.lenet_embeddings)}\")\n",
        "\n",
        "# Persisting the change\n",
        "train_dataset.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y41hy9q04zov"
      },
      "outputs": [],
      "source": [
        "train_dataset.first()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vglV17Mq4Pi4"
      },
      "outputs": [],
      "source": [
        "train_dataset.get_field_schema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqq_Ymg8brY"
      },
      "source": [
        "## Create a Similarity Index Based on the CNN embeddings\n",
        "\n",
        "After extracting embeddings from our trained LeNet model, we create a **similarity index**. This index allows for efficient searching of samples based on their learned feature representations. Similar to the CLIP similarity index, this enables us to quickly find training images that are visually similar according to what the LeNet model has learned, which is useful for exploring clusters and identifying related samples in the model's feature space.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bsu1gMZe8Kvm"
      },
      "outputs": [],
      "source": [
        "print(\"Computing similarity index based on LeNet embeddings...\")\n",
        "\n",
        "lenet_similarity_index = fob.compute_similarity(\n",
        "    train_dataset,  # Using the training dataset\n",
        "    embeddings=\"lenet_embeddings\",  # Field containing the LeNet embeddings\n",
        "    brain_key=\"lenet_cosine_similarity_index\",  # Unique identifier for this similarity index\n",
        "    backend=\"sklearn\",  # Can also use \"pinecone\" for large datasets\n",
        "    metric=\"cosine\"  # Similarity metric\n",
        ")\n",
        "\n",
        "print(f\"LeNet similarity index computed successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neWE1NCOKjKV"
      },
      "outputs": [],
      "source": [
        "# Display similarity index information\n",
        "print(f\"LeNet Similarity Index Details:\")\n",
        "print(f\"- Total samples indexed: {len(train_dataset)}\")\n",
        "print(f\"- Embeddings field: {lenet_similarity_index.config.embeddings_field}\")\n",
        "print(f\"- Metric: {lenet_similarity_index.config.metric}\")\n",
        "\n",
        "# Example 1: Find similar images to a specific digit using LeNet embeddings\n",
        "# Let's pick a sample from the training dataset\n",
        "query_sample = train_dataset.first()\n",
        "print(f\"\"\"\\nQuerying for images similar to sample: {query_sample.id}\n",
        "            with label {query_sample.ground_truth.label}\n",
        "            using LeNet embeddings\"\"\")\n",
        "\n",
        "# Use the sample ID (string) instead of the sample object\n",
        "similar_view_lenet = train_dataset.sort_by_similarity(\n",
        "    query_sample.id,  # Pass the sample ID\n",
        "    brain_key=\"lenet_cosine_similarity_index\",\n",
        "    k=10,  # Return top 10 most similar\n",
        "    reverse=False  # Most similar first\n",
        ")\n",
        "\n",
        "print(f\"Found {len(similar_view_lenet)} most similar samples using LeNet embeddings\")\n",
        "\n",
        "# Update FiftyOne App view to show similar samples\n",
        "session.view = similar_view_lenet\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FISLQ-LuSlaS"
      },
      "source": [
        "## Evaluating Image Representativeness and Uniqueness through their Embeddings\n",
        "\n",
        "**Image representativeness** and **uniqueness** are metrics for understanding dataset quality and composition that work hand-in-hand with clustering analysis.\n",
        "\n",
        "**Representativeness** measures how well a sample captures the central characteristics of its cluster, highly representative images sit near cluster centers and exemplify the common visual patterns within each group.\n",
        "\n",
        "**Uniqueness** identifies samples that are distant from any cluster centers, often representing edge cases, rare scenarios, or potential annotation errors that clustering algorithms struggle to categorize.\n",
        "\n",
        "These scores are normalized to [0, 1] and become particularly insightful when viewed alongside clustering results. Representative samples serve as ideal cluster exemplars for understanding what each group represents, while highly unique samples often fall between clusters or form singleton groups, indicating unusual data points that may require special handling in our machine learning pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huEFWGbiu6kV"
      },
      "outputs": [],
      "source": [
        "## Compute Uniqueness and Representativeness for LeNet Embeddings\n",
        "\n",
        "print(\"Computing uniqueness scores based on LeNet embeddings...\")\n",
        "fob.compute_uniqueness(train_dataset, embeddings='lenet_embeddings')\n",
        "\n",
        "print(\"Computing representativeness scores based on LeNet embeddings...\")\n",
        "fob.compute_representativeness(train_dataset, embeddings='lenet_embeddings')\n",
        "\n",
        "print(\"Uniqueness and representativeness computation complete.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FsJ4gY-7Fda7"
      },
      "outputs": [],
      "source": [
        "# Compute mistakenness on samples on the train_set\n",
        "# Mistakenness identifies samples where the model's confident predictions disagree with ground truth labels.\n",
        "# High mistakenness scores suggest potential annotation errors rather than model failures.\n",
        "fob.compute_mistakenness(train_dataset,\n",
        "                         pred_field=\"lenet_train_classification\",\n",
        "                         label_field=\"ground_truth\")\n",
        "\n",
        "print(\"Mistakenness computed successfully for train_dataset.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SgK1sJDHHEgr"
      },
      "outputs": [],
      "source": [
        "# Compute quantiles of mistakenness\n",
        "mistakenness_quantiles = train_dataset.quantiles(\n",
        "    \"mistakenness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nMistakenness quantiles for train_dataset:\")\n",
        "print(mistakenness_quantiles)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dm1VTvYHPlyT"
      },
      "outputs": [],
      "source": [
        "# Hardness is a measure computed based on model prediction output (through\n",
        "# logits) that summarizes a measure of the uncertainty the model had with the\n",
        "# sample. This makes hardness quantitative and can be used to detect things\n",
        "# like hard samples and annotation errors\n",
        "fob.compute_hardness(train_dataset,\n",
        "                     label_field='lenet_train_classification',\n",
        "                     )\n",
        "\n",
        "print(\"Hardness computed successfully for train_dataset.\")\n",
        "\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BlHELglv0NKr"
      },
      "outputs": [],
      "source": [
        "# Compute quantiles of hardness\n",
        "hardness_quantiles = train_dataset.quantiles(\n",
        "    \"hardness\",\n",
        "    [0.9, 0.95, 0.99]\n",
        ")\n",
        "\n",
        "print(\"\\nHardness quantiles for train_dataset:\")\n",
        "print(hardness_quantiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUt9Tx2z6LiF"
      },
      "source": [
        "## Create 2D Visualizations of LeNet Embeddings\n",
        "Reducing High-Dimensional Features to Visual Form\n",
        "Our LeNet embeddings exist in 84-dimensional space, making them impossible to visualize directly. Dimensionality reduction techniques transform these high-dimensional vectors into 2D points that preserve important relationships between samples.\n",
        "\n",
        "Principal Component Analysis (PCA) finds the directions of maximum variance in the data and projects samples onto the two most important axes. This linear transformation preserves global structure and shows which samples vary most from the dataset center.\n",
        "\n",
        "\n",
        "UMAP (Uniform Manifold Approximation and Projection) uses non-linear techniques to preserve local neighborhoods while revealing cluster structure. UMAP often separates distinct groups more clearly than PCA, making it valuable for identifying how the model groups similar digits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIIRMjCi6C1S"
      },
      "outputs": [],
      "source": [
        "print(\"Creating 2D visualizations of LeNet embeddings...\")\n",
        "\n",
        "# PCA visualization\n",
        "pca_viz_lenet = fob.compute_visualization(\n",
        "    train_dataset,\n",
        "    method=\"pca\",\n",
        "    embeddings=\"lenet_embeddings\",\n",
        "    num_dims=2,\n",
        "    brain_key=\"pca_lenet_embeddings\"\n",
        ")\n",
        "\n",
        "# UMAP visualization\n",
        "umap_viz_lenet = fob.compute_visualization(\n",
        "    train_dataset,\n",
        "    method=\"umap\",\n",
        "    embeddings=\"lenet_embeddings\",\n",
        "    num_dims=2,\n",
        "    brain_key=\"umap_lenet_embeddings\"\n",
        ")\n",
        "\n",
        "print(\"2D visualizations created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XB3nQoQyDGDU"
      },
      "source": [
        "#### PCA 2D Projection of the CNN Embedding Space\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/pca_lenet_embeddings.png?raw=true)\n",
        "\n",
        "\n",
        "#### UMAP 2D Projection of the CNN Embedding Space\n",
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/umap_lenet_embeddings.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c67c_ZJ0JHce"
      },
      "source": [
        "Inspect the projected embeddings on the FiftyOne app. Compare how the PCA and the UMAP projections look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mBoPU7nCUpq"
      },
      "outputs": [],
      "source": [
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "988VFPHSIGII"
      },
      "source": [
        "## Exploring Uniqueness and Representativeness with Dataset Aggregations\n",
        "\n",
        "**Understanding Score Distributions Through Statistical Analysis**\n",
        "\n",
        "We can use FiftyOne's aggregation methods to explore the distribution of uniqueness and representativeness values across our dataset. This analysis helps us identify meaningful thresholds for categorizing our samples based on their statistical properties.\n",
        "\n",
        "**Quantile Analysis for Threshold Selection**\n",
        "\n",
        "Quantiles divide our data into meaningful segments, revealing the distribution shape and enabling principled threshold selection. The 25th, 50th (median), 75th, and 95th percentiles help us understand where most samples fall and identify the truly exceptional cases.\n",
        "\n",
        "For uniqueness scores, the 95th percentile reveals the most unusual samples that fall far from any cluster centers. These often represent edge cases, annotation errors, or rare variants that deserve special attention. The 75th percentile for representativeness identifies samples that exemplify their clusters well, making them ideal candidates for visualization or as training exemplars.\n",
        "\n",
        "```python\n",
        "# Get quantiles for uniqueness scores\n",
        "uniqueness_quantiles = train_dataset.aggregate(fo.Quantiles(\"uniqueness\",\n",
        "                                          [0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "\n",
        "# Get quantiles for representativeness scores  \n",
        "representativeness_quantiles = train_dataset.aggregate(fo.Quantiles(\"representativeness\",\n",
        "                                    [0.05, 0.25, 0.5, 0.75, 0.95]))\n",
        "```\n",
        "\n",
        "These quantile boundaries enable us to create filtered views targeting specific sample types, such as the top 5% most unique samples or the most representative examples from each cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cz8rCYfFu_g6"
      },
      "outputs": [],
      "source": [
        "# Get quantiles for uniqueness based on LeNet embeddings\n",
        "\n",
        "quantiles_of_interest = [0.5, 0.75, 0.95, 0.99, 0.999]\n",
        "uniqueness_quantiles = train_dataset.quantiles(\"uniqueness\", quantiles_of_interest)\n",
        "quantile_labels = [f\"{label*100:.1f}% \" for label in quantiles_of_interest]\n",
        "print('Uniqueness quantiles')\n",
        "{label: value for label, value in zip(quantile_labels, uniqueness_quantiles)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmdFZa82BtDE"
      },
      "outputs": [],
      "source": [
        "quantiles_of_interest = [0.5, 0.75, 0.95, 0.99, 0.999]\n",
        "representativeness_quantiles = train_dataset.quantiles(\"uniqueness\", quantiles_of_interest)\n",
        "print('Representativeness quantiles')\n",
        "{label: value for label, value in zip(quantile_labels, representativeness_quantiles)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ug4hUH2nXYDP"
      },
      "source": [
        "## Create Views for Most Unique and Most Representative Samples\n",
        "\n",
        "We can use filtered Views of the dataset to get the most unique values and launch the FiftyOne app from them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld0QsUJyCyiE"
      },
      "outputs": [],
      "source": [
        "# Most unique samples (top 0.1% - these are often edge cases or outliers)\n",
        "most_unique_samples_view = train_dataset.match(\n",
        "                             F(\"uniqueness\") > uniqueness_quantiles[-1]\n",
        "                             ).sort_by(\"uniqueness\", reverse=True)\n",
        "\n",
        "print(f\"Most unique samples: {len(most_unique_samples_view)} samples\")\n",
        "\n",
        "# Most representative samples (top 0.1% - these exemplify their clusters well)\n",
        "most_representative_samples_view = train_dataset.match(\n",
        "    F(\"representativeness\") > representativeness_quantiles[-1]\n",
        "    ).sort_by(\"representativeness\", reverse=True)\n",
        "\n",
        "print(f\"Most representative samples: {len(most_representative_samples_view)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5VZ7AE3vCCG"
      },
      "outputs": [],
      "source": [
        "## Visualize most unique images\n",
        "session.view = most_unique_samples_view\n",
        "session.refresh()\n",
        "print(f\"FiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOiK_Jrh_bZC"
      },
      "outputs": [],
      "source": [
        "# Get the sample with the highest uniqueness score\n",
        "most_unique_sample = most_unique_samples_view.first()\n",
        "\n",
        "print(f\"\"\"Most unique sample ID: {most_unique_sample.id}\n",
        "Uniqueness score: {most_unique_sample.uniqueness:.4f}\n",
        "Ground truth label: {most_unique_sample.ground_truth.label}\"\"\")\n",
        "\n",
        "# Find images most similar to the most unique one using the LeNet similarity index\n",
        "# We want the most similar samples, so reverse=False\n",
        "similar_to_unique_view = train_dataset.sort_by_similarity(\n",
        "    most_unique_sample.id,\n",
        "    brain_key=\"lenet_cosine_similarity_index\", # Use the LeNet index\n",
        "    k=20, # Get the top 20 similar samples (including the unique one itself)\n",
        "    reverse=False\n",
        ")\n",
        "\n",
        "print(f\"Found {len(similar_to_unique_view)} samples most similar to the most unique one.\")\n",
        "\n",
        "# Visualize the samples similar to the most unique one\n",
        "session.view = similar_to_unique_view\n",
        "session.refresh()\n",
        "print(session.url)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9j3UwMpOCqm"
      },
      "outputs": [],
      "source": [
        "## Visualize most representative images\n",
        "session.view = most_representative_samples_view\n",
        "session.refresh()\n",
        "print(f\"FiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ua4aI4kVHd7w"
      },
      "source": [
        "## Obtain PyTorch Datasets from the Most Unique and Representative Samples\n",
        "\n",
        "We can create new PyTorch datasets from the samples that are most unique or representative and use them to retrain the model.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2QdUDxGHt2h"
      },
      "outputs": [],
      "source": [
        "most_unique_torch_train_ds = CustomTorchImageDataset(most_unique_samples_view,\n",
        "                                                     label_map=label_map,\n",
        "                                                     image_transforms=image_transforms)\n",
        "most_representative_torch_train_ds = CustomTorchImageDataset(most_representative_samples_view,\n",
        "                                                             label_map=label_map,\n",
        "                                                             image_transforms=image_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ICAFI-g4rWZr"
      },
      "outputs": [],
      "source": [
        "new_train_set = ConcatDataset([most_unique_torch_train_ds, most_representative_torch_train_ds])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_VLAyZEu2C3"
      },
      "source": [
        "**Suggested Exercise**\n",
        "Create a new DataLoader and retrain the model using **only** the most representative and unique samples. On the next sections there is code that can be used as guide."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waBtOaOoaaUm"
      },
      "source": [
        "## Augmenting the Misclassified Training Samples and Retrain\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_uLxSnsm0rm"
      },
      "source": [
        "### Effective Augmentations for MNIST\n",
        "\n",
        "\n",
        "### Geometric Transformations:\n",
        "\n",
        "* Small rotations (Â±10-15 degrees): Handwritten digits naturally vary in orientation\n",
        "* Small translations (Â±2-3 pixels): Accounts for centering variations in digit positioning\n",
        "* Slight scaling (0.9-1.1x): Handles size variations in handwriting\n",
        "* Moderate elastic deformations are  useful for MNIST because they simulate the natural variations in handwriting style. Imagine stretching and compressing parts of a digit as different people might write them.\n",
        "### Why These Work\n",
        "The principle behind effective augmentation is creating realistic variations that preserve the digit's identity while exposing the model to plausible distortions. MNIST digits are centered and normalized, so augmentations should introduce controlled variability without making digits unrecognizable.\n",
        "### Augmentations to Avoid\n",
        "* Heavy distortions like large rotations (>20Â°), extreme scaling, or aggressive elastic deformation can make digits ambiguous, a rotated \"6\" might look like a \"9\", or a heavily stretched \"1\" might resemble a \"7\".\n",
        "\n",
        "* Color-based augmentations (brightness, contrast) have limited benefit since MNIST is grayscale and already normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jujitJcyCvdN"
      },
      "source": [
        "## Retraining and Final Evaluation\n",
        "\n",
        "**Measuring the Impact of Augmented Data**\n",
        "\n",
        "After identifying misclassified training samples and creating augmented versions, we retrain our model to measure performance improvements. This process demonstrates how targeted data augmentation can address specific model weaknesses while maintaining overall performance.\n",
        "\n",
        "The retraining phase uses the original best model weights as a starting point, then fine-tunes on the combined dataset containing both original training data and augmented versions of problematic samples. We use a lower learning rate to preserve learned features while adapting to the new data.\n",
        "\n",
        "Final evaluation compares the retrained model against both the original LeNet performance and CLIP's zero-shot results. This comparison reveals the relative benefits of supervised learning with augmentation versus pre-trained model capabilities.\n",
        "\n",
        "```python\n",
        "# Load best model for retraining\n",
        "retrain_model = ModernLeNet5()\n",
        "retrain_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "\n",
        "# Retrain with combined dataset\n",
        "combined_train_dataset = ConcatDataset([torch_train_set, torch_augmented_dataset])\n",
        "```\n",
        "\n",
        "The evaluation metrics quantify whether targeted augmentation successfully improved model robustness and generalization on the held-out test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWhQyNotngBu"
      },
      "outputs": [],
      "source": [
        "## Data Augmentation for Misclassified Training Samples\n",
        "\n",
        "print(f\"Number of misclassified training samples: {len(mislabeled_train_images_view)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZkHGpJjgniw"
      },
      "source": [
        "\n",
        "## Samples to Filter-out During Fine-tuning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gX6UxRK3NoqR"
      },
      "outputs": [],
      "source": [
        "# Identify samples in the top 0.1 percentile of mistakenness (likely annotation errors)mistakenness_99th_percentile = train_dataset.quantiles(\"mistakenness\", [0.99])[0]\n",
        "mistakenness_99th_percentile = train_dataset.quantiles(\"mistakenness\", [0.999])[0]\n",
        "highly_mistaken_view = train_dataset.match(F(\"mistakenness\") > mistakenness_99th_percentile)\n",
        "\n",
        "# Identify samples in the top 1 percentile of hardness (likely genuinely hard samples)\n",
        "hardness_99th_percentile = train_dataset.quantiles(\"hardness\", [0.999])[0]\n",
        "highly_hard_view = train_dataset.match(F(\"hardness\") > hardness_99th_percentile)\n",
        "\n",
        "print(f\"Number of highly mistaken samples (top 1%): {len(highly_mistaken_view)}\")\n",
        "print(f\"Number of highly hard samples (top 1%): {len(highly_hard_view)}\")\n",
        "\n",
        "# Get the IDs of samples that are highly mistaken or highly hard\n",
        "highly_mistaken_ids = set(highly_mistaken_view.values(\"id\"))\n",
        "highly_hard_ids = set(highly_hard_view.values(\"id\"))\n",
        "\n",
        "# Combine the sets of IDs that are hard\n",
        "difficult_samples = highly_mistaken_ids.union(highly_hard_ids)\n",
        "\n",
        "print(f\"Total number of difficult samples (union of highly mistaken and highly hard): {len(difficult_samples)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A9gijcmokiKk"
      },
      "outputs": [],
      "source": [
        "# Get the IDs of the misclassified samples\n",
        "misclassified_sample_ids = set(mislabeled_train_images_view.values(\"id\"))\n",
        "\n",
        "print(f\"Total number of misclassified samples in the training set: {len(misclassified_sample_ids)}\")\n",
        "\n",
        "# Find the intersection of difficult samples and misclassified samples\n",
        "intersection_difficult_misclassified_ids = difficult_samples.intersection(misclassified_sample_ids)\n",
        "\n",
        "print(f\"Number of samples that are both difficult and misclassified: {len(intersection_difficult_misclassified_ids)}\")\n",
        "\n",
        "# Create a FiftyOne view containing only these intersection samples\n",
        "intersection_difficult_misclassified_view = train_dataset.select(\n",
        "    list(intersection_difficult_misclassified_ids)\n",
        "    ).sort_by('hardness',\n",
        "              reverse=True)\n",
        "\n",
        "\n",
        "print(f\"Created a FiftyOne view for the intersection samples with {len(intersection_difficult_misclassified_view)} samples.\")\n",
        "\n",
        "\n",
        "# Inspect the samples in ids_to_remove, we can decide whether\n",
        "# whether we augment them or exclude them from the fine-tuning data\n",
        "session.view = intersection_difficult_misclassified_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFRSg5ufWfwe"
      },
      "source": [
        "### Finding the Needles in the Haystack\n",
        "\n",
        "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/mistaken_images.png)\n",
        "\n",
        "Using our metrics we have reduced significantly the search space and the manual inspection for our problematic samples. Isn't it interesting to find these samples in one of the most canonical datasets in machine learning?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4jWhhtJTCEu"
      },
      "outputs": [],
      "source": [
        "filenames_of_samples_to_exclude = set([\n",
        "                                      \"/root/fiftyone/mnist/train/data/002902.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/003693.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/035247.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/018383.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/033507.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/039424.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/025547.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/047218.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/001921.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/004477.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/050606.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/030050.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/024614.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/014583.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/034521.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/050995.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/004477.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/033319.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/034405.jpg\"\n",
        "                                      \"/root/fiftyone/mnist/train/data/007260.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/001401.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/002149.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/000903.jpg\",\n",
        "                                      \"/root/fiftyone/mnist/train/data/001921.jpg\"\n",
        "                                      ])\n",
        "\n",
        "# Use match() to filter by filepath\n",
        "samples_to_exclude_view = train_dataset.match(\n",
        "    fo.ViewField(\"filepath\").is_in(filenames_of_samples_to_exclude)\n",
        ")\n",
        "print('Samples to exclude from new training runs')\n",
        "session.view = samples_to_exclude_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvC4F-c9u4vJ"
      },
      "outputs": [],
      "source": [
        "# get the ids from samples_to_exclude_view\n",
        "ids_of_samples_to_exclude = samples_to_exclude_view.values(\"id\")\n",
        "ids_of_samples_to_exclude"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHYCoyZIXn8h"
      },
      "source": [
        "## Create a New Training Dataset without the Excluded Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m04hU2qDXWtl"
      },
      "outputs": [],
      "source": [
        "new_train_dataset = train_dataset.exclude(ids_of_samples_to_exclude).clone()\n",
        "\n",
        "print(f\"Original train_dataset has {len(train_dataset)} samples.\")\n",
        "print(f\"Excluding {len(new_train_set)} samples.\")\n",
        "print(f\"New train_dataset view has {len(new_train_set)} samples.\")\n",
        "\n",
        "new_train_dataset.name = \"cleaned-up-mnist-training-set\"\n",
        "new_train_dataset.persistent = True\n",
        "print(f\"New persistent dataset created with {len(new_train_dataset)} samples.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUt5zP1eozkF"
      },
      "outputs": [],
      "source": [
        "# To work with this view in PyTorch, create a CustomTorchImageDataset from it\n",
        "new_torch_train_set = CustomTorchImageDataset(\n",
        "    new_train_dataset,  # Changed from new_train_set to new_train_dataset\n",
        "    label_map=label_map, # Use the same label map\n",
        "    image_transforms=image_transforms # Use the same transforms\n",
        ")\n",
        "\n",
        "print(f\"New PyTorch dataset created from the filtered view with {len(new_torch_train_set)} samples.\")\n",
        "\n",
        "# You can now use new_torch_train_set to create a new DataLoader for retraining\n",
        "# new_train_loader = create_deterministic_training_dataloader(\n",
        "#     new_torch_train_set,\n",
        "#     batch_size=batch_size,\n",
        "#     shuffle=True,\n",
        "#     num_workers=num_workers,\n",
        "#     pin_memory=True\n",
        "# )\n",
        "# print(f\"New train DataLoader created with {len(new_train_loader)} batches.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyPUYKG4VKBN"
      },
      "source": [
        "**Another possibility:** using these samples to train a model with an 'IDK' (I don't know class).\n",
        "\n",
        "Also consider that the validation and test set might have dirty samples too. Ideally we should repeat this process with them too.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VK3EL0cHf6tA"
      },
      "source": [
        "## Define Augmentations\n",
        "\n",
        "**Configuring Transformations for MNIST Digit Recognition**\n",
        "\n",
        "Data augmentation applies controlled transformations to training images, creating variations that help models generalize to new handwriting styles. For MNIST, effective augmentations simulate natural variations in digit writing without changing the digit's identity.\n",
        "\n",
        "**Seeding for Reproducibility**: Set random seeds before defining augmentations to ensure consistent results across training runs. This enables reproducible experiments and fair model comparisons.\n",
        "\n",
        "**MNIST-Specific Transformations**: Small rotations, translations, and elastic deformations work well for handwritten digits. These transformations mimic natural handwriting variations while preserving digit readability. Avoid extreme distortions that could make a \"6\" look like a \"9\" or render digits unrecognizable.\n",
        "\n",
        "**Exploration**:\n",
        "To inspect the effect of the defined augmentations on the images, you can try [using the FiftyOne plugin for this](https://github.com/jacobmarks/fiftyone-albumentations-plugin).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8xShb1LCkxi"
      },
      "outputs": [],
      "source": [
        "# Install the augmentations plugin\n",
        "!fiftyone plugins download https://github.com/jacobmarks/fiftyone-albumentations-plugin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cw0Edh8ZEG_l"
      },
      "outputs": [],
      "source": [
        "mnist_augmentations_1 = A.Compose([\n",
        "\n",
        "    # Use Affine transform for shifting, scaling, and rotating\n",
        "    A.Affine(\n",
        "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},  # Â±10% translation\n",
        "        scale=(0.9, 1.1),     # Â±10% scaling\n",
        "        rotate=(-5, 5),     # Â±5Â° rotation\n",
        "        p=0.8\n",
        "    ),\n",
        "\n",
        "    # Elastic deformations to simulate handwriting style variations\n",
        "    A.ElasticTransform(\n",
        "        alpha=20,             # Strength of distortion\n",
        "        sigma=5,              # Smoothness of distortion\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        p=0.6\n",
        "    ),\n",
        "\n",
        "    # Mild perspective transformations\n",
        "    A.Perspective(scale=(0.01, 0.03), p=0.2),\n",
        "\n",
        "    # Mild grid distortion\n",
        "    A.GridDistortion(num_steps=2, distort_limit=0.05, p=0.2),\n",
        "\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVcUoxD3Cu15"
      },
      "outputs": [],
      "source": [
        "# Define second set of augmentations\n",
        "mnist_augmentations_2 = A.Compose([\n",
        "\n",
        "    # Equalize the image histogram (can help with contrast variations)\n",
        "    A.Equalize(p=0.3),\n",
        "\n",
        "    # Affine with shear (simulates slanted handwriting)\n",
        "    A.Affine(\n",
        "        shear=(-5, 5),  # Add shear\n",
        "        translate_percent={\"x\": (-0.03, 0.03), \"y\": (-0.03, 0.03)}, # Smaller translation\n",
        "        scale=(0.98, 1.02), # Smaller scaling\n",
        "        rotate=(-2, 2), # Smaller rotation\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        p=0.9\n",
        "    ),\n",
        "\n",
        "    # Mild perspective transformations\n",
        "    A.Perspective(scale=(0.01, 0.03), p=0.2),\n",
        "\n",
        "    # Mild grid distortion\n",
        "    A.GridDistortion(num_steps=2, distort_limit=0.05, p=0.2),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IptT5b8iC0Rs"
      },
      "outputs": [],
      "source": [
        "# Define a third set of augmentations for MNIST\n",
        "mnist_augmentations_3 = A.Compose([\n",
        "    # Use Affine transform for shifting, scaling, and rotating\n",
        "    A.Affine(\n",
        "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},  # Â±10% translation\n",
        "        scale=(0.95, 1.15),     # Â±15% scaling\n",
        "        rotate=(-7, 7),         # Â±7Â° rotation\n",
        "        p=0.8\n",
        "    ),\n",
        "\n",
        "    # Elastic deformation with different parameters\n",
        "    A.ElasticTransform(\n",
        "        alpha=15,\n",
        "        sigma=4,\n",
        "        border_mode=cv2.BORDER_CONSTANT,\n",
        "        p=0.4\n",
        "    ),\n",
        "\n",
        "    # Coarse Dropout (simulates occlusions or pen lifts)\n",
        "    A.CoarseDropout(\n",
        "        p=0.3\n",
        "    ),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f11PddjugRS9"
      },
      "source": [
        "## Create the torch Dataset for the Augmented Set\n",
        "\n",
        "**Building a Custom PyTorch Dataset for Augmented Training Data**\n",
        "\n",
        "Creating a PyTorch Dataset for augmented data requires handling multiple variations of problematic samples while maintaining compatibility with PyTorch's DataLoader system. Our `AugmentedMNISTDataset` class applies augmentations during training, ensuring each epoch sees different variations of misclassified samples.\n",
        "\n",
        "**Key Design Principles:**\n",
        "\n",
        "**Dynamic Augmentation**: Apply transformations during data loading rather than pre-generating samples. This saves disk space while providing variation potential.\n",
        "\n",
        "**Deterministic Access**: Use mathematical mapping for consistent sample ordering while allowing multiple augmented versions of each base sample. The formula `base_idx = idx // (augment_factor + 1)` determines which original sample to use.\n",
        "\n",
        "**Integration**: Inherit from PyTorch's `Dataset` interface for compatibility with DataLoader features like batching, shuffling, and multi-process loading.\n",
        "\n",
        "**Memory and Performance Considerations:**\n",
        "\n",
        "**Memory Usage**: Store file paths rather than loaded images to maintain small memory footprint regardless of augmentation factor.\n",
        "\n",
        "**Caching Strategy**: Consider implementing an LRU cache to store recent base images, reducing disk I/O during training.\n",
        "\n",
        "**Augmentation Randomness**: Each call to `__getitem__` may produce different results due to random augmentation parameters.\n",
        "\n",
        "**Error Resilience**: Robust error handling ensures training continues even if individual samples fail to load or augment.\n",
        "\n",
        "This design provides a foundation for targeted data augmentation while maintaining flexibility to experiment with different augmentation strategies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DTZEWNCHDejq"
      },
      "outputs": [],
      "source": [
        "class AugmentedMNISTDataset(Dataset):\n",
        "    \"\"\"\n",
        "    PyTorch dataset that applies augmentations to misclassified MNIST samples.\n",
        "    Each sample can be augmented multiple times to create more training data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, fiftyone_view,\n",
        "                 label_map,\n",
        "                 base_transforms,\n",
        "                 augmentations=None,\n",
        "                 augment_factor=5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fiftyone_view: FiftyOne view of misclassified samples\n",
        "            label_map: Mapping from string labels to indices\n",
        "            base_transforms: Base PyTorch transforms (normalization, etc.)\n",
        "            augmentations: Albumentations transform pipeline\n",
        "            augment_factor: How many augmented versions to create per sample\n",
        "        \"\"\"\n",
        "        self.image_paths = fiftyone_view.values(\"filepath\")\n",
        "        self.str_labels = fiftyone_view.values(\"ground_truth.label\")\n",
        "        self.label_map = label_map\n",
        "        self.base_transforms = base_transforms\n",
        "        self.augmentations = augmentations\n",
        "        self.augment_factor = augment_factor\n",
        "\n",
        "        print(f\"AugmentedMNISTDataset: {len(self.image_paths)} base samples\")\n",
        "        print(f\"With augmentation factor {augment_factor}: {len(self)} total samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths) * (self.augment_factor + 1)  # +1 for original\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Determine which base sample and whether to augment\n",
        "        base_idx = idx // (self.augment_factor + 1)\n",
        "        aug_idx = idx % (self.augment_factor + 1)\n",
        "\n",
        "        # Load image\n",
        "        image_path = self.image_paths[base_idx]\n",
        "        image = Image.open(image_path).convert('L')\n",
        "\n",
        "        # Convert to numpy for albumentations\n",
        "        image_np = np.array(image, dtype=np.uint8)\n",
        "\n",
        "        # Apply augmentation if not the first version (original)\n",
        "        if aug_idx > 0 and self.augmentations is not None:\n",
        "            augmented = self.augmentations(image=image_np)\n",
        "            image_np = augmented['image']\n",
        "\n",
        "        # Convert back to PIL for PyTorch transforms\n",
        "        image = Image.fromarray(image_np, mode='L')\n",
        "\n",
        "        # Apply base transforms (normalization, tensor conversion)\n",
        "        if self.base_transforms:\n",
        "            image = self.base_transforms(image)\n",
        "\n",
        "        # Get label\n",
        "        label_str = self.str_labels[base_idx]\n",
        "        label_idx = self.label_map.get(label_str, -1)\n",
        "\n",
        "        return image, torch.tensor(label_idx, dtype=torch.long)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBQXUeAwDUvt"
      },
      "outputs": [],
      "source": [
        "# Calculate the 1th percentile (lowest percentile) of confidence scores for lenet_train_classification\n",
        "confidence_quantiles = train_dataset.quantiles(\"lenet_train_classification.confidence\", [0.01])\n",
        "q10_confidence = confidence_quantiles[0]\n",
        "\n",
        "print(f\"The 1th percentile of confidence is: {q10_confidence:.4f}\")\n",
        "\n",
        "# Create a view of samples that were correctly classified AND have confidence <= 1th percentile\n",
        "low_confidence_correct_view = train_dataset.match(\n",
        "    (F(\"lenet_train_classification.label\") == F(\"ground_truth.label\")) &\n",
        "    (F(\"lenet_train_classification.confidence\") <= q10_confidence)\n",
        ").sort_by(\"lenet_train_classification.confidence\")\n",
        "\n",
        "print(f\"\\nNumber of correctly classified samples with confidence <= {q10_confidence:.4f}: {len(low_confidence_correct_view)}\")\n",
        "\n",
        "# We can launch the FiftyOne App to visualize this view\n",
        "session.view = low_confidence_correct_view\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMh0_z9nNWxR"
      },
      "source": [
        "## Define Samples to Augment\n",
        "\n",
        "This section focuses on identifying specific subsets of samples from the training data that are most likely to benefit from augmentation. We will select samples that were previously misclassified, are highly unique or representative in the embedding space, or were correctly classified but with low confidence. Augmenting these samples aims to improve the model's performance on challenging or underrepresented examples.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGM7NUVvtIVc"
      },
      "outputs": [],
      "source": [
        "dataset_name = \"digit-samples-to-augment\"\n",
        "\n",
        "if dataset_name in fo.list_datasets():\n",
        "    print(f\"Dataset '{dataset_name}' already exists. Deleting it.\")\n",
        "    fo.delete_dataset(dataset_name)\n",
        "\n",
        "# Always create a new dataset after deleting or if it didn't exist\n",
        "dataset_to_augment = fo.Dataset(dataset_name)\n",
        "\n",
        "for sample in mislabeled_train_images_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "for sample in most_unique_samples_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "for sample in most_representative_samples_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "for sample in low_confidence_correct_view:\n",
        "    dataset_to_augment.add_sample(sample)\n",
        "\n",
        "dataset_to_augment.save()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yALZCSEAcbYm"
      },
      "outputs": [],
      "source": [
        "# Filter out excluded samples\n",
        "dataset_to_augment = dataset_to_augment.exclude(ids_of_samples_to_exclude).clone()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGH3ZAyrDM1A"
      },
      "outputs": [],
      "source": [
        "torch_augmented_dataset = AugmentedMNISTDataset(\n",
        "    dataset_to_augment,\n",
        "    label_map=label_map,\n",
        "    base_transforms=image_transforms,\n",
        "    augmentations=mnist_augmentations_1, # We can also try mnist_augmentations_2 or mnist_augmentations_3\n",
        "    augment_factor=9  # Create this number of augmented versions per input sample\n",
        ")\n",
        "\n",
        "print(f\"Original misclassified samples: {len(mislabeled_train_images_view)}\")\n",
        "print(f'')\n",
        "print(f\"Total augmented dataset size: {len(torch_augmented_dataset)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qbY0VlychC9v"
      },
      "outputs": [],
      "source": [
        "# Combine the original dataset with its augmentation\n",
        "combined_dataset = ConcatDataset([new_torch_train_set, torch_augmented_dataset])\n",
        "print(f\"Combined dataset size: {len(combined_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IpAymXsyhqC8"
      },
      "outputs": [],
      "source": [
        "# Create new DataLoader for combined dataset\n",
        "combined_train_loader = create_deterministic_training_dataloader(\n",
        "    combined_dataset,\n",
        "    batch_size=64,\n",
        "    shuffle=True,\n",
        "    num_workers=num_workers,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"Combined DataLoader has {len(combined_train_loader)} batches.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBokBRd7L9j_"
      },
      "source": [
        "## Visualize Sample Augmentations in FiftyOne\n",
        "\n",
        "**Inspecting Augmentation Quality Before Training**\n",
        "\n",
        "Visualizing augmented samples ensures transformations improve model training rather than corrupt the data. FiftyOne provides an interface to compare original images with their augmented variants, helping validate that augmentations preserve digit identity while adding useful variation.\n",
        "\n",
        "**Quality Control**: Review augmented samples to confirm that rotations, translations, and elastic deformations remain within acceptable bounds. A properly augmented \"3\" should still look like a \"3\" despite the applied transformations.\n",
        "\n",
        "**Parameter Tuning**: Visual inspection helps adjust augmentation parameters. If elastic deformations make digits unrecognizable or rotations create ambiguous orientations, reduce the transformation intensity before training.\n",
        "\n",
        "**Dataset Creation**: The visualization process creates temporary augmented samples that can be loaded into FiftyOne for side-by-side comparison with originals, enabling systematic evaluation of augmentation effectiveness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "508hMKjcFKfY"
      },
      "outputs": [],
      "source": [
        "# Create a small dataset with original and augmented versions for visualization\n",
        "def create_augmentation_samples(view, augmentations, num_samples=50):\n",
        "    \"\"\"Create FiftyOne samples showing original and augmented versions\"\"\"\n",
        "\n",
        "    dataset_name = \"augmented_images_training_set\"\n",
        "\n",
        "    if dataset_name in fo.list_datasets():\n",
        "        aug_viz_dataset = fo.load_dataset(dataset_name)\n",
        "    else:\n",
        "        aug_viz_dataset = fo.Dataset(dataset_name)\n",
        "\n",
        "    sample_paths = view.values(\"filepath\")[:num_samples]\n",
        "    sample_labels = view.values(\"ground_truth.label\")[:num_samples]\n",
        "\n",
        "    for i, (path, label) in enumerate(zip(sample_paths, sample_labels)):\n",
        "        # Load original image\n",
        "        original_image = Image.open(path).convert('L')\n",
        "        original_np = np.array(original_image)\n",
        "\n",
        "        # Create sample for original image\n",
        "        original_sample = fo.Sample(filepath=path)\n",
        "        original_sample.tags = [\"original\"]\n",
        "        original_sample[\"ground_truth\"] = fo.Classification(label=label)\n",
        "        original_sample[\"augmentation_type\"] = \"original\"\n",
        "        aug_viz_dataset.add_sample(original_sample)\n",
        "\n",
        "        # Create 3 augmented versions\n",
        "        for aug_idx in range(3):\n",
        "            # Apply augmentation\n",
        "            augmented = augmentations(image=original_np)['image']\n",
        "\n",
        "            # Save augmented image temporarily\n",
        "            aug_image = Image.fromarray(augmented, mode='L')\n",
        "            temp_path = f\"/tmp/aug_{i}_{aug_idx}.png\"\n",
        "            aug_image.save(temp_path)\n",
        "\n",
        "            # Create FiftyOne sample for augmented image\n",
        "            aug_sample = fo.Sample(filepath=temp_path)\n",
        "            aug_sample.tags = [\"augmented\"]\n",
        "            aug_sample[\"ground_truth\"] = fo.Classification(label=label)\n",
        "            aug_sample[\"augmentation_type\"] = f\"augmented_{aug_idx + 1}\"\n",
        "            aug_sample[\"original_sample_id\"] = str(i)\n",
        "\n",
        "            aug_viz_dataset.add_sample(aug_sample)\n",
        "\n",
        "    return aug_viz_dataset\n",
        "\n",
        "print(\"Creating augmentation visualization dataset...\")\n",
        "\n",
        "# Create the visualization dataset\n",
        "aug_viz_dataset = create_augmentation_samples(\n",
        "    dataset_to_augment,\n",
        "    mnist_augmentations_1,\n",
        "    num_samples=50\n",
        ")\n",
        "\n",
        "print(f\"Created visualization dataset with {len(aug_viz_dataset)} samples\")\n",
        "print(f\"Original samples: {len(aug_viz_dataset.match_tags('original'))}\")\n",
        "print(f\"Augmented samples: {len(aug_viz_dataset.match_tags('augmented'))}\")\n",
        "\n",
        "# Launch FiftyOne App to visualize the augmentations\n",
        "session.view = aug_viz_dataset.view()\n",
        "\n",
        "print(f\"\\nAugmentation Visualization URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXV9LMYUXuVl"
      },
      "outputs": [],
      "source": [
        "# Create views for easy comparison\n",
        "original_view = aug_viz_dataset.match_tags(\"original\")\n",
        "augmented_view = aug_viz_dataset.match_tags(\"augmented\")\n",
        "\n",
        "print(f\"\\nTo compare:\")\n",
        "print(f\"- View original samples: session.view = original_view\")\n",
        "print(f\"- View augmented samples: session.view = augmented_view\")\n",
        "print(f\"- Group by original sample: Use 'original_sample_id' field to group related samples\")\n",
        "\n",
        "# Add some helpful aggregations\n",
        "print(f\"\\nSample distribution:\")\n",
        "print(f\"By augmentation type: {aug_viz_dataset.count_values('augmentation_type')}\")\n",
        "print(f\"By ground truth label: {aug_viz_dataset.count_values('ground_truth.label')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjfiVoK0HTSp"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/augmentation_mnist_vis.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0sIham2JVEw"
      },
      "source": [
        "## Fine-Tuning with Augmented Data\n",
        "\n",
        "The retraining process combines our original training data with the augmented versions of misclassified samples, allowing the model to learn from its previous mistakes. We use a lower learning rate to preserve the knowledge already learned while adapting to the new augmented examples.\n",
        "\n",
        "During retraining, we monitor both training and validation loss to ensure the model improves without overfitting. The validation set continues to serve as our guide for saving the best model weights, ensuring we capture improvements in generalization rather than just memorization of the augmented data.\n",
        "\n",
        "```python\n",
        "for epoch in range(retrain_epochs):\n",
        "    train_loss = train_epoch(retrain_model, combined_train_loader)\n",
        "    val_loss = val_epoch(retrain_model, val_loader)\n",
        "    \n",
        "    if val_loss < best_retrain_val_loss:\n",
        "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
        "```\n",
        "\n",
        "This targeted approach allows us to address specific model weaknesses while maintaining overall performance on the broader dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2MiRH66n-7r"
      },
      "outputs": [],
      "source": [
        "## Load Best Model for Retraining\n",
        "\n",
        "# Ensure reproducibility for the retraining process\n",
        "set_seeds(51) # Use the same seed as initial training for consistency\n",
        "\n",
        "# Load the previously saved best model\n",
        "best_model_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
        "retrain_model = ModernLeNet5()\n",
        "retrain_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "retrain_model = retrain_model.to(device)\n",
        "\n",
        "print(f\"Loaded best model from {best_model_path} for retraining\")\n",
        "\n",
        "# Use a lower learning rate for fine-tuning\n",
        "retrain_optimizer = Adam(retrain_model.parameters(), lr=0.0001,\n",
        "                       weight_decay=1e-3)\n",
        "# 10x stronger regularization\n",
        "# 30x smaller learning rate\n",
        "\n",
        "print(\"\\nStarting retraining with augmented data...\")\n",
        "\n",
        "retrain_epochs = 15\n",
        "retrain_losses = []\n",
        "retrain_val_losses = []\n",
        "\n",
        "# Track the best validation loss during retraining\n",
        "best_retrain_val_loss = float('inf')\n",
        "retrain_model_save_path = Path(os.getcwd()) / 'retrained_lenet.pth'\n",
        "\n",
        "for epoch in range(retrain_epochs):\n",
        "    print(f\"\\n--- Retrain Epoch {epoch+1}/{retrain_epochs} ---\")\n",
        "\n",
        "    # Training phase\n",
        "    retrain_model.train()\n",
        "    batch_losses = []\n",
        "\n",
        "    for images, labels in tqdm(combined_train_loader, desc=\"Retraining\"):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = retrain_model(images)\n",
        "        loss_value = ce_loss(logits, labels)\n",
        "\n",
        "        # Backward pass\n",
        "        retrain_optimizer.zero_grad()\n",
        "        loss_value.backward()\n",
        "        retrain_optimizer.step()\n",
        "\n",
        "        batch_losses.append(loss_value.item())\n",
        "\n",
        "    train_loss = np.mean(batch_losses)\n",
        "    retrain_losses.append(train_loss)\n",
        "\n",
        "    # Validation phase\n",
        "    retrain_model.eval()\n",
        "    val_batch_losses = []\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        for images, labels in tqdm(val_loader, desc=\"Validation\"):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = retrain_model(images)\n",
        "            loss_value = ce_loss(logits, labels)\n",
        "            val_batch_losses.append(loss_value.item())\n",
        "\n",
        "    val_loss = np.mean(val_batch_losses)\n",
        "    retrain_val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
        "\n",
        "    # Save best model during retraining\n",
        "    if val_loss < best_retrain_val_loss:\n",
        "        best_retrain_val_loss = val_loss\n",
        "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
        "        print(\"âœ“ Saved improved retrained model\")\n",
        "\n",
        "print(f\"\\nRetraining complete! Best model saved to {retrain_model_save_path}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gh07-Kxclihj"
      },
      "source": [
        "## Plot Re-Training Progress\n",
        "\n",
        "**Visualizing Training Dynamics Across Phases**\n",
        "\n",
        "Training visualization reveals model learning patterns during both initial training and retraining with augmented data. Loss curves show convergence behavior, overfitting signals, and the impact of targeted data augmentation.\n",
        "\n",
        "**Two-Panel Analysis**\n",
        "\n",
        "The left panel shows retraining progress in isolation, tracking how the model adapts to augmented misclassified training samples.\n",
        "\n",
        "The right panel provides historical context by plotting the complete training timeline. The vertical line marks where retraining begins, allowing comparison between original learning dynamics and fine-tuning behavior.\n",
        "\n",
        "**Key Patterns to Observe**\n",
        "\n",
        "- **Convergence Speed**: Retraining typically converges faster than initial training due to pre-learned features\n",
        "- **Loss Magnitude**: Validation loss during retraining should remain close to or below original best values  \n",
        "- **Stability**: Smooth loss curves indicate stable learning, while oscillations suggest learning rate issues\n",
        "- **Gap Analysis**: Small train-validation gaps indicate good generalization\n",
        "\n",
        "**Interpreting Results**\n",
        "\n",
        "Successful retraining shows decreasing validation loss without diverging from training loss. If validation loss increases while training loss decreases, we have trained the model enough. We will be using the state of the model that achieved the lowest validation loss.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx6vDV6dlcNp"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot retraining losses\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(retrain_losses, label='Retrain - Training Loss', marker='o')\n",
        "plt.plot(retrain_val_losses, label='Retrain - Validation Loss', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title(f'Retraining Progress ({retrain_epochs} Epochs)')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Plot comparison with original training (if we have the data)\n",
        "plt.subplot(1, 2, 2)\n",
        "if 'train_losses' in locals() and 'val_losses' in locals():\n",
        "    epochs_orig = range(1, len(train_losses) + 1)\n",
        "    epochs_retrain = range(len(train_losses) + 1, len(train_losses) + 1 + len(retrain_losses))\n",
        "\n",
        "    plt.plot(epochs_orig, train_losses, label='Original Train', alpha=0.7)\n",
        "    plt.plot(epochs_orig, val_losses, label='Original Val', alpha=0.7)\n",
        "    plt.plot(epochs_retrain, retrain_losses, label='Retrain Train', marker='o', linewidth=2)\n",
        "    plt.plot(epochs_retrain, retrain_val_losses, label='Retrain Val', marker='s', linewidth=2)\n",
        "\n",
        "    plt.axvline(x=len(train_losses), color='red', linestyle='--', alpha=0.5, label='Retrain Start')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Complete Training History')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJqpzGKCl9-_"
      },
      "source": [
        "## Applying the Fine-tuned Model to the Test Set\n",
        "\n",
        "Following is a brief description of what we are doing in this section.\n",
        "\n",
        "**Evaluating Augmented Model Performance**\n",
        "\n",
        "* Load the best retrained model weights and apply to the test dataset.\n",
        "\n",
        "* Generate predictions and logits for performance comparison against the original model.\n",
        "\n",
        "**Model Loading Process**\n",
        "\n",
        "* Instantiate a fresh model architecture and load the saved state dictionary from retraining.\n",
        "\n",
        "* Move to the GPU (if available) and set evaluation mode for inference without gradient computation.\n",
        "\n",
        "**Inference Pipeline**\n",
        "\n",
        "* Process test images through the fine-tuned model using the same preprocessing pipeline as training.\n",
        "\n",
        "* Extract both predicted class indices and raw logits for comprehensive evaluation.\n",
        "\n",
        "**Result Storage**\n",
        "\n",
        "Collect predictions and logits in arrays for analysis. This enables comparison between original model performance and improvements from targeted data augmentation on challenging samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XGX6iRvzl7-e"
      },
      "outputs": [],
      "source": [
        "print(\"\\nApplying retrained model to test set...\")\n",
        "\n",
        "# Load the best retrained model\n",
        "final_model = ModernLeNet5()\n",
        "final_model.load_state_dict(torch.load(retrain_model_save_path, map_location=device))\n",
        "final_model = final_model.to(device)\n",
        "final_model.eval()\n",
        "\n",
        "# Apply retrained model to test set\n",
        "retrained_predictions = []\n",
        "retrained_logits = []\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for images, _ in tqdm(test_loader, desc=\"Evaluating retrained model\"):\n",
        "        images = images.to(device)\n",
        "\n",
        "        logits = final_model(images)\n",
        "        retrained_logits.append(logits.cpu().numpy())\n",
        "\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        retrained_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Concatenate all results\n",
        "retrained_logits = np.concatenate(retrained_logits, axis=0)\n",
        "\n",
        "print(f\"Retrained model evaluation complete.\")\n",
        "print(f\"Predictions shape: {len(retrained_predictions)}\")\n",
        "print(f\"Logits shape: {retrained_logits.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRvv1QKXlmqW"
      },
      "source": [
        "## Store Retrained Model Predictions in FiftyOne\n",
        "\n",
        "Here's what we are doing next:\n",
        "\n",
        "**Integrating Fine-tuned Model Results**\n",
        "\n",
        "* Store retrained model predictions as [FiftyOne Classification](https://docs.voxel51.com/api/fiftyone.core.labels.html#fiftyone.core.labels.Classification) objects in the test dataset.\n",
        "\n",
        "* Each sample receives prediction label, confidence score, and raw logits for comprehensive analysis.\n",
        "\n",
        "**Classification Object Creation**\n",
        "\n",
        "Convert raw model outputs into structured FiftyOne format. Apply softmax to logits for confidence scores and map predicted indices to class labels using the established label mapping.\n",
        "\n",
        "**Dataset Integration**\n",
        "\n",
        "Add retrained predictions as a new field alongside existing CLIP and the original LeNet predictions. This enables direct comparison between all model variants within the same dataset framework.\n",
        "\n",
        "**Analysis Preparation**\n",
        "\n",
        "Structured storage enables filtering, querying, and evaluation using FiftyOne's built-in tools.\n",
        "\n",
        "* [FiftyOne's Views Cheat Sheet](https://docs.voxel51.com/cheat_sheets/views_cheat_sheet.html)\n",
        "* [FiftyOne's Filtering Cheat Sheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
        "\n",
        "With this we compare model performance and identify samples where fine-tuning improved or degraded predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVxXrY0ulZQS"
      },
      "outputs": [],
      "source": [
        "print(\"Storing fine-tuned model predictions in FiftyOne...\")\n",
        "\n",
        "for i, sample in enumerate(tqdm(test_dataset, desc=\"Storing retrained predictions\")):\n",
        "    predicted_idx = retrained_predictions[i]\n",
        "    predicted_label = dataset_classes[predicted_idx]\n",
        "    sample_logits = retrained_logits[i]\n",
        "\n",
        "    confidences = Fun.softmax(torch.tensor(sample_logits), dim=0).numpy()\n",
        "    predicted_confidence = float(confidences[predicted_idx])\n",
        "\n",
        "    classification = fo.Classification(\n",
        "        label=predicted_label,\n",
        "        confidence=predicted_confidence,\n",
        "        logits=sample_logits.tolist()\n",
        "    )\n",
        "\n",
        "    sample[\"retrained_lenet_classification\"] = classification\n",
        "    sample.save()\n",
        "\n",
        "print(\"Retrained model predictions stored successfully!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VNYKjHNlo2G"
      },
      "source": [
        "## Evaluate Retrained Model Performance\n",
        "\n",
        "**Comprehensive Assessment of Fine-tuned Model**\n",
        "\n",
        "Evaluate the retrained model using FiftyOne's classification evaluation framework. Generate detailed performance metrics including accuracy, precision, recall, and F1-scores for direct comparison with the original model.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhPYCJOirl_J"
      },
      "source": [
        "**Performance Comparison Analysis**\n",
        "\n",
        "Compare original LeNet performance against the retrained model across key metrics. Calculate improvement deltas to quantify the impact of targeted data augmentation on model robustness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Dk_WJoFlWZu"
      },
      "outputs": [],
      "source": [
        "print(\"\\nEvaluating retrained model performance...\")\n",
        "\n",
        "# Evaluate retrained model\n",
        "retrained_evaluation_results = test_dataset.evaluate_classifications(\n",
        "    \"retrained_lenet_classification\",\n",
        "    gt_field=\"ground_truth\",\n",
        "    eval_key=\"retrained_lenet_eval\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RETRAINED MODEL EVALUATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "retrained_evaluation_results.print_report(digits=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB4JTMyXrvOU"
      },
      "source": [
        "## Compare Original vs Retrained Performance\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isaBZU-oreCa"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE COMPARISON\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Get metrics for both models\n",
        "original_metrics = lenet_evaluation_results.metrics()\n",
        "retrained_metrics = retrained_evaluation_results.metrics()\n",
        "\n",
        "print(f\"{'Metric':<20} {'Original':<12} {'Retrained':<12} {'Improvement':<12}\")\n",
        "print(\"-\" * 56)\n",
        "\n",
        "metrics_to_compare = ['accuracy', 'precision', 'recall', 'f1']\n",
        "\n",
        "for metric in metrics_to_compare:\n",
        "    if metric in original_metrics and metric in retrained_metrics:\n",
        "        orig_val = original_metrics[metric]\n",
        "        retrain_val = retrained_metrics[metric]\n",
        "        improvement = retrain_val - orig_val\n",
        "\n",
        "        print(f\"{metric:<20} {orig_val:<12.4f} {retrain_val:<12.4f} {improvement:+.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMl8hmbnls9i"
      },
      "source": [
        "## Analysis of Misclassified Samples After Retraining\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMbC8GmWr_Xl"
      },
      "source": [
        "**Misclassification Analysis**\n",
        "\n",
        "* Identify samples fixed by retraining versus those newly misclassified.\n",
        "\n",
        "* Compute net improvement in correct predictions to assess overall effectiveness of the augmentation strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8O6ygP1lSlK"
      },
      "outputs": [],
      "source": [
        "# Find samples that were misclassified before but correct now\n",
        "originally_wrong = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "now_correct = originally_wrong.match(\n",
        "    F(\"retrained_lenet_classification.label\") == F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "print(f\"\\nSamples fixed by retraining: {len(now_correct)}\")\n",
        "\n",
        "# Find samples that were correct before but wrong now\n",
        "originally_correct = test_dataset.match(\n",
        "    F(\"lenet_classification.label\") == F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "now_wrong = originally_correct.match(\n",
        "    F(\"retrained_lenet_classification.label\") != F(\"ground_truth.label\")\n",
        ")\n",
        "\n",
        "print(f\"Samples broken by retraining: {len(now_wrong)}\")\n",
        "\n",
        "# Net improvement\n",
        "net_improvement = len(now_correct) - len(now_wrong)\n",
        "print(f\"Net improvement in correct predictions: {net_improvement}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MySUV8G8sO8U"
      },
      "source": [
        "## Launch FiftyOne App for Insights on the  Results\n",
        "\n",
        "\n",
        "* Launch FiftyOne App with comparative views showing all model predictions. Enable exploration of specific cases where fine-tuning improved or degraded performance for model debugging.\n",
        "\n",
        "* Create filtered view containing ground truth labels and predictions from both original and retrained models. This enables direct comparison between model variants within the same interface.\n",
        "\n",
        "* Access interactive confusion matrices, per-class metrics, and sample-level analysis through FiftyOne's evaluation framework. Filter by prediction differences to identify which samples benefited from augmentation.\n",
        "\n",
        "* Use FiftyOne's query language to create views of samples fixed by retraining, samples broken by retraining, or samples where both models agree or disagree with ground truth labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3wrq9VJ-fLA"
      },
      "source": [
        "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/samples_fixed_by_fine_tuning.png?raw=true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VlKoxG9ZsNwM"
      },
      "outputs": [],
      "source": [
        "print(\"\\nLaunching FiftyOne App to explore results...\")\n",
        "\n",
        "# Create a view showing the comparison\n",
        "comparison_view = test_dataset.select_fields([\n",
        "    \"ground_truth\",\n",
        "    \"lenet_classification\",\n",
        "    \"retrained_lenet_classification\"\n",
        "])\n",
        "\n",
        "session.view = comparison_view\n",
        "session.refresh()\n",
        "print(f\"FiftyOne App URL: {session.url}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaTxNzER9c58"
      },
      "outputs": [],
      "source": [
        "# View the samples that are fixed by fine-tuning\n",
        "session.view = now_correct\n",
        "session.refresh()\n",
        "print(session.url)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrUJiyGt9Vq6"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AUGMENTATION AND FINE-TUNING SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Identified {len(mislabeled_train_images_view)} misclassified training samples\")\n",
        "print(f\"Applied MNIST-appropriate augmentations (rotation, translation, elastic deform, etc.)\")\n",
        "print(f\"Created {len(torch_augmented_dataset)} augmented training samples\")\n",
        "print(f\"Retrained model for {retrain_epochs} epochs with combined dataset\")\n",
        "print(f\"Evaluated performance on test set\")\n",
        "print(f\"Net improvement: {net_improvement} correctly classified samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoMJ0qu3peE1"
      },
      "source": [
        "## Key Insights\n",
        "Careful data augmentation has the potential to help with misclassified samples: By specifically targeting the samples that our model struggled with during initial training, we can create additional training examples that help the model learn to handle these edge cases more effectively. However we must do this carefully in order to not damage the model.\n",
        "\n",
        "Small rotations and elastic deformations are effective for handwritten digits: These transformations mimic natural variations in human handwriting without making digits ambiguous. The key is finding the right balance - enough variation to improve robustness, but not so much that a \"6\" looks like a \"9\".\n",
        "\n",
        "Fine-tuning with augmented data can improve model robustness: Rather than training from scratch, fine-tuning the already-trained model with augmented versions of problematic samples allows us to specifically address weaknesses while preserving the knowledge already learned.\n",
        "\n",
        "FiftyOne enables easy tracking of prediction changes across model versions: By storing predictions from both the original and retrained models in the same dataset, we can directly compare performance and identify which specific samples improved or degraded, enabling targeted analysis and further improvements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chH1kiI3N8W7"
      },
      "source": [
        "## Takeaways\n",
        "\n",
        "This tutorial demonstrates several fundamental principles:\n",
        "\n",
        "- **Zero-shot vs. Supervised Learning**: Modern pre-trained models can often achieve competitive performance without task-specific training, but custom models allow for domain-specific optimization\n",
        "- **Embeddings as Universal Representations**: High-dimensional vectors capture semantic similarity and enable powerful analysis and visualization techniques  \n",
        "- **Visual Debugging**: FiftyOne's interactive capabilities make it easy to understand model behavior and identify improvement opportunities\n",
        "- **Data Quality Matters**: Systematic analysis of your dataset often leads to more significant performance gains than model architecture changes\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOoKTaHllqTm"
      },
      "source": [
        "# Image Classification Getting Started Series Summary\n",
        "\n",
        "This comprehensive series walks you through the core components of working with\n",
        "classification data in FiftyOne: from loading and visualizing datasets, to creating embeddings, evaluating models, and finding systematic errors.\n",
        "\n",
        "## Summary of Steps\n",
        "\n",
        "### Step 1: Understanding the MNIST Dataset\n",
        "\n",
        "Explore the MNIST dataset structure and load it into FiftyOne. Learn about the 60,000 training images and 10,000 test images of handwritten digits, and understand why this dataset remains fundamental for classification research.\n",
        "\n",
        "### Step 2: Creating and Visualizing Image Embeddings\n",
        "\n",
        "Learn how neural networks represent images as high-dimensional vectors. Generate embeddings with CLIP and visualize them using dimensionality reduction techniques like PCA and UMAP to understand image similarity and clustering.\n",
        "\n",
        "### Step 3: Zero-shot Classification with CLIP\n",
        "\n",
        "Discover how modern vision-language models can classify images without explicit training on your dataset. Use pre-computed embeddings to perform efficient zero-shot classification with meaningful text prompts.\n",
        "\n",
        "### Step 4: Evaluating Dataset Quality through Embeddings\n",
        "\n",
        "Assess your dataset's composition by analyzing embedding distributions. Learn to identify representative samples, outliers, and potential data quality issues using FiftyOne's analysis capabilities.\n",
        "\n",
        "### Step 5: Traditional Supervised Classification\n",
        "\n",
        "Build a custom Convolutional Neural Network (LeNet-5) from scratch in PyTorch. Understand the fundamentals of supervised learning including convolutional layers, training loops, and optimization.\n",
        "\n",
        "### Step 6: Bridging FiftyOne and PyTorch\n",
        "\n",
        "Master the integration between FiftyOne's dataset management and PyTorch's training capabilities. Convert datasets to DataLoaders while maintaining metadata and handling preprocessing efficiently.\n",
        "\n",
        "### Step 7: Model Comparison and Benchmarking\n",
        "\n",
        "Compare your custom CNN against CLIP's zero-shot classification using FiftyOne's evaluation framework. Learn comprehensive evaluation metrics and statistical significance testing.\n",
        "\n",
        "### Step 8: Analyzing Model Predictions\n",
        "\n",
        "Interpret model behavior by examining prediction confidence and identifying hard samples. Use logit analysis to find systematic misclassification patterns and debug model failures.\n",
        "\n",
        "### Step 9: Data Augmentation Strategies\n",
        "\n",
        "Improve model performance through principled data augmentation. Learn which geometric transformations and elastic deformations help MNIST classification while avoiding destructive augmentations.\n",
        "\n",
        "### Step 10: Advanced Error Analysis\n",
        "\n",
        "Create targeted views of model failures using FiftyOne's visualization capabilities. Identify false positives, false negatives, and the most problematic samples for systematic model improvement.\n",
        "\n",
        "---\n",
        "\n",
        "This series is part of the **Getting Started with FiftyOne** initiative. For\n",
        "more tutorials, head to [FiftyOne Documentation](https://docs.voxel51.com/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YM6Hf32Vqyix"
      },
      "source": [
        "## Suggested Exercises\n",
        "\n",
        "1. **Sample Quality Analysis**: Notice that we computed the uniqueness, representativeness, and hardness of the samples on the training set. Can you create filtered views of other batches of retraining data based on them?\n",
        "\n",
        "Try:\n",
        "* Selecting the most unique samples or those with highest hardness scores for additional augmentation\n",
        "* Filtering out the most ambiguous or questionable cases out of the fine-tuning training data\n",
        "\n",
        "2. **Active Learning**: Use the uniqueness, representativeness and hardness metrics to implement an [active learning pipeline](https://voxel51.com/blog/supercharge-your-annotation-workflow-with-active-learning) that selects the most informative samples for manual annotation or additional augmentation.\n",
        "\n",
        "3. **Dataset Exploration**: Apply these techniques to other classification datasets like [CIFAR-10](https://docs.voxel51.com/dataset_zoo/datasets.html#dataset-zoo-cifar10) or [Fashion-MNIST](https://docs.voxel51.com/dataset_zoo/datasets.html#dataset-zoo-fashion-mnist)\n",
        "\n",
        "4. **Architecture Comparison**: Implement and compare different CNN architectures (e.g. [Network in Network](https://arxiv.org/pdf/1312.4400), [ResNet](https://arxiv.org/pdf/1512.03385)) on MNIST\n",
        "\n",
        "5. **Transfer Learning**: Use pre-trained ImageNet models and fine-tune them for digit classification\n",
        "\n",
        "6. **Custom Augmentations**: Design and test novel augmentation strategies specific to handwritten digits, try augmenting the dataset [adding colors to the digits](https://paperswithcode.com/dataset/colored-mnist) and see how this change impacts the model's performance.\n",
        "\n",
        "7. **CLIP Model Variants and Prompting**: Experiment with different CLIP model variants available in FiftyOne's Model Zoo and compare their zero-shot performance on MNIST. Test how different text prompts affect accuracy across model sizes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTgM1-9DatJf"
      },
      "source": [
        "\n",
        "## Resources and Further Reading\n",
        "\n",
        "- [FiftyOne Documentation](https://docs.voxel51.com/)\n",
        "- [FiftyOne's Filtering Cheatsheet](https://docs.voxel51.com/cheat_sheets/filtering_cheat_sheet.html)\n",
        "- [FiftyOne Model Zoo](https://docs.voxel51.com/user_guide/model_zoo/index.html)\n",
        "- [FiftyOne Dataset Zoo](https://docs.voxel51.com/user_guide/dataset_zoo/index.html)\n",
        "- [PyTorch Classification Tutorial](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html)\n",
        "- [CLIP Paper: Learning Transferable Visual Representations](https://arxiv.org/abs/2103.00020)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c26WH1uGr-Wh"
      },
      "source": [
        "## Next Steps\n",
        "\n",
        "Now that you've completed the Image Classification Getting Started series, here are\n",
        "some suggested next steps to deepen your journey with FiftyOne:\n",
        "\n",
        "-   **Explore Object Detection**  \n",
        "    Learn how to work with bounding boxes, evaluate detection models, and find annotation mistakes in object detection datasets.\n",
        "\n",
        "-   **Try Multi-label Classification**  \n",
        "    Extend these concepts to scenarios where images can belong to multiple classes simultaneously, common in real-world applications.\n",
        "\n",
        "-   **Experiment with FiftyOne Plugins**  \n",
        "    Enhance your workflow with powerful plugins for advanced augmentations, active learning tools, and integrations with annotation platforms.\n",
        "\n",
        "-   **Connect with the Community**  \n",
        "    Share your findings, ask questions, or browse community projects on the\n",
        "    [FiftyOne Discord](https://community.voxel51.com) or\n",
        "    [GitHub Discussions](https://github.com/voxel51/fiftyone/discussions).\n",
        "\n",
        "-   **Apply to Your Own Datasets**  \n",
        "    Adapt these workflows to your real-world classification projects. Whether\n",
        "    it's medical imaging, satellite analysis, or manufacturing quality control â€” FiftyOne supports diverse domains.\n",
        "\n",
        "-   **Dive into Advanced Topics**  \n",
        "    Explore segmentation, video analysis, and 3D data in the\n",
        "    [official documentation](https://docs.voxel51.com/).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cac0742"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "babf2ae5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "8946438b",
        "outputId": "d4e509d3-8e47-4a05-d2d0-5f08e0688f60"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created a view for bad validation images with 18 samples.\n",
            "\n",
            "FiftyOne App URL showing bad validation images: https://5151-gpu-t4-hm-1n0yoy4ljcacb-a.europe-west4-0.prod.colab.dev?polling=true\n"
          ]
        }
      ],
      "source": [
        "# Create a view of samples with file paths in the bad_val_images_paths set\n",
        "bad_val_images_view = val_dataset.match(\n",
        "    fo.ViewField(\"filepath\").is_in(bad_val_images_paths)\n",
        ")\n",
        "\n",
        "print(f\"Created a view for bad validation images with {len(bad_val_images_view)} samples.\")\n",
        "\n",
        "# Set the session view to visualize these samples\n",
        "session.view = bad_val_images_view\n",
        "session.refresh()\n",
        "print(f\"\\nFiftyOne App URL showing bad validation images: {session.url}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "174bb0a373a74f48a553d5c39b885009": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1dfa8c1055e24386a3842bbd29a4f000",
              "IPY_MODEL_93b1e06982384dc59266823e5d0fa0db",
              "IPY_MODEL_91ae5f5fa74c42cc86d680a87648d30e"
            ],
            "layout": "IPY_MODEL_2743b9036442401d982cc40066b313e8"
          }
        },
        "1dfa8c1055e24386a3842bbd29a4f000": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9a82587c2164fe195b2dcb6b94819d5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4e633d835f4c43258d6b6040734b451f",
            "value": "Epochsâ€‡completed:â€‡100%|â€‡"
          }
        },
        "2743b9036442401d982cc40066b313e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "392a80de6b1f404291b4acd97139e4fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e633d835f4c43258d6b6040734b451f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b4efe8d06224db99536cd5679f9e3fa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91ae5f5fa74c42cc86d680a87648d30e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1849c27cc774e11bcbedbbe874ae02a",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c036d26df71f4d5ca6ecdbc3ab4f9f6e",
            "value": "â€‡500/500â€‡[00:05]"
          }
        },
        "93b1e06982384dc59266823e5d0fa0db": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b4efe8d06224db99536cd5679f9e3fa",
            "max": 500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_392a80de6b1f404291b4acd97139e4fa",
            "value": 500
          }
        },
        "b1849c27cc774e11bcbedbbe874ae02a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9a82587c2164fe195b2dcb6b94819d5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c036d26df71f4d5ca6ecdbc3ab4f9f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
