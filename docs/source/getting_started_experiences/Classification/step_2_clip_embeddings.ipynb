{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image Embeddings with CLIP\n",
    "\n",
    "In this notebook, we'll generate image embeddings for the MNIST test set using a pre-trained CLIP model. Embeddings are powerful vector representations that capture the semantic content of images. We will then use dimensionality reduction techniques (PCA and UMAP) to visualize these high-dimensional vectors in 2D, allowing us to visually explore the structure of our dataset.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Loading pre-trained models from the FiftyOne Model Zoo\n",
    "*   Computing image embeddings with CLIP\n",
    "*   Assigning embeddings to dataset samples\n",
    "*   Dimensionality reduction: PCA and UMAP\n",
    "*   Visualizing embedding plots in FiftyOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's add our imports and load the test dataset we created in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "import fiftyone.brain as fob\n",
    "\n",
    "# Ensure the test dataset exists\n",
    "if \"mnist-test-set\" in fo.list_datasets():\n",
    "    test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "else:\n",
    "    print(\"Test dataset not found. Please run '1_explore_mnist.ipynb' first.\")\n",
    "\n",
    "session = fo.launch_app(test_dataset, auto=False)\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Image Embeddings with CLIP\n",
    "\n",
    "Image embeddings are high-dimensional vectors that translate visual concepts into a format that machine learning models can compare. Similar images will have similar embedding vectors.\n",
    "\n",
    "We will use a pre-trained CLIP model from the FiftyOne Model Zoo to generate these embeddings. FiftyOne makes this process simple with the `compute_embeddings()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = foz.load_zoo_model(\"clip-vit-base32-torch\",\n",
    "                                device=device)\n",
    "print(f\"The model is loaded on {clip_model._device}\")\n",
    "\n",
    "total_params = sum(p.numel() for p in clip_model._model.parameters())\n",
    "print(f\"The CLIP model has {total_params:,} parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will take about 3 min on a Google Colab instance with GPU enabled\n",
    "clip_embeddings = test_dataset.compute_embeddings(model=clip_model,\n",
    "                                        batch_size=512,\n",
    "                                        num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is a NumPy array where each row is a 512-dimensional vector representing an image. Now, we'll attach each embedding to its corresponding sample in the FiftyOne dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the format and shape of our embeddings vector\n",
    "print(type(clip_embeddings), clip_embeddings.shape)\n",
    "\n",
    "test_dataset.add_sample_field(\"clip_embeddings\", fo.VectorField)\n",
    "test_dataset.set_values(\"clip_embeddings\", clip_embeddings)\n",
    "test_dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Similarity Index\n",
    "\n",
    "A **similarity index** allows for efficient searching of similar samples based on their embeddings. Instead of a slow, brute-force search, the index organizes embeddings for fast retrieval. We'll build one on our new CLIP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_index = fob.compute_similarity(\n",
    "    test_dataset,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    brain_key=\"clip_cosine_similarity_index\",\n",
    "    backend=\"sklearn\",\n",
    "    metric=\"cosine\"\n",
    ")\n",
    "\n",
    "print(\"Similarity index computed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily find the most similar images to any given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_sample = test_dataset.first()\n",
    "print(f\"Querying for images similar to sample: {query_sample.id} with label {query_sample.ground_truth.label}\")\n",
    "\n",
    "similar_view = test_dataset.sort_by_similarity(\n",
    "    query_sample.id,\n",
    "    brain_key=\"clip_cosine_similarity_index\",\n",
    "    k=10\n",
    ")\n",
    "\n",
    "session.view = similar_view\n",
    "session.refresh()\n",
    "print(f\"Found {len(similar_view)} most similar samples. Check the App: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a 2D Projection of the Embeddings\n",
    "\n",
    "Our 512-dimensional embeddings are impossible to visualize directly. We use dimensionality reduction techniques like **PCA** and **UMAP** to project them into 2D space. This is a lossy compression, but it helps us visually identify clusters, outliers, and patterns in the data.\n",
    "\n",
    "- **PCA (Principal Component Analysis)**: A linear method that preserves global variance.\n",
    "- **UMAP (Uniform Manifold Approximation and Projection)**: A non-linear method that excels at preserving local neighborhood structures and revealing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_visualization = fob.compute_visualization(test_dataset,\n",
    "                                              method=\"pca\",\n",
    "                                              embeddings=\"clip_embeddings\",\n",
    "                                              num_dims=2,\n",
    "                                              brain_key=\"pca_visualization_clip_embeds\")\n",
    "\n",
    "umap_visualization = fob.compute_visualization(test_dataset,\n",
    "                                              method=\"umap\",\n",
    "                                              embeddings=\"clip_embeddings\",\n",
    "                                              num_dims=2,\n",
    "                                              brain_key=\"umap_visualization_clip_embeds\")\n",
    "print(\"PCA and UMAP visualizations computed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Embeddings in the FiftyOne App\n",
    "\n",
    "To see your new 2D projections:\n",
    "\n",
    "1. In the App, click the **`+`** icon next to \"Samples\".\n",
    "2. Select **Embeddings**.\n",
    "3. Choose `pca_visualization_clip_embeds` or `umap_visualization_clip_embeds` from the dropdown.\n",
    "\n",
    "You can now interact with the plot. Try coloring the points by `ground_truth.label` to see if CLIP's embeddings naturally separate the digits into clusters.\n",
    "\n",
    "![](https://github.com/andandandand/practical-computer-vision/blob/main/images/image_embeddings_zero_cluster.gif?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With our dataset enriched with CLIP embeddings and visualizations, we're ready to use these features for a classification task.\n",
    "\n",
    "Proceed to `3_zero_shot_classification.ipynb` to perform zero-shot classification with CLIP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}