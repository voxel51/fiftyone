{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Data Augmentation and Retraining\n",
    "\n",
    "In this final notebook, we'll use the insights gained from our analysis to improve our model. We will identify challenging samples from the training setâ€”those that were misclassified, are highly unique, or had low prediction confidence. We will then apply targeted **data augmentation** to these samples and fine-tune our LeNet model on this enriched dataset.\n",
    "\n",
    "**Key concepts covered:**\n",
    "*   Identifying problematic samples for augmentation\n",
    "*   Defining effective data augmentation strategies for MNIST\n",
    "*   Creating a combined dataset of original and augmented data\n",
    "*   Fine-tuning a pre-trained model\n",
    "*   Comparing performance before and after fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Let's begin by setting up our environment, including all necessary imports and helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as Fun\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset, ConcatDataset\n",
    "from torch.optim import Adam\n",
    "\n",
    "import fiftyone as fo\n",
    "from fiftyone import ViewField as F\n",
    "import albumentations as A\n",
    "\n",
    "# Redefine model and dataset classes\n",
    "class ModernLeNet5(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(ModernLeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 120, kernel_size=4)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = nn.Linear(120, 84)\n",
    "        self.fc2 = nn.Linear(84, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(Fun.relu(self.conv1(x)))\n",
    "        x = self.pool(Fun.relu(self.conv2(x)))\n",
    "        x = Fun.relu(self.conv3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = Fun.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n",
    "    generator = torch.Generator().manual_seed(51)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, generator=generator if shuffle else None, **kwargs)\n",
    "\n",
    "def set_seeds(seed=51):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Samples for Augmentation\n",
    "\n",
    "First, we need to get our model's predictions on the training set to identify which samples it misclassified. We'll need to run inference on the training set, similar to how we did for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 512 misclassified training samples.\n"
     ]
    }
   ],
   "source": [
    "# Load datasets and model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_dataset = fo.load_dataset(\"mnist-training-set\")\n",
    "test_dataset = fo.load_dataset(\"mnist-test-set\")\n",
    "model_save_path = Path(os.getcwd()) / 'best_lenet.pth'\n",
    "loaded_model = ModernLeNet5().to(device)\n",
    "loaded_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# Recreate transforms and dataloaders\n",
    "mean_intensity, std_intensity = 0.1307, 0.3081 # Pre-computed\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.ToImage(), transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize((mean_intensity,), (std_intensity,))\n",
    "])\n",
    "dataset_classes = sorted(train_dataset.distinct(\"ground_truth.label\"))\n",
    "label_map = {label: i for i, label in enumerate(dataset_classes)}\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "class CustomTorchImageDataset(Dataset):\n",
    "    def __init__(self, fo_dset, xforms, l_map):\n",
    "        self.fo_dset, self.xforms, self.l_map = fo_dset, xforms, l_map\n",
    "        self.img_paths = self.fo_dset.values(\"filepath\")\n",
    "        self.labels = self.fo_dset.values(\"ground_truth.label\")\n",
    "    def __len__(self): return len(self.img_paths)\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.img_paths[idx]).convert('L')\n",
    "        label = self.l_map[self.labels[idx]]\n",
    "        return self.xforms(img), torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "torch_train_set = CustomTorchImageDataset(train_dataset, image_transforms, label_map)\n",
    "train_inference_loader = torch.utils.data.DataLoader(torch_train_set, batch_size=64, shuffle=False)\n",
    "\n",
    "# Add predictions to training set if they don't exist\n",
    "if \"lenet_train_classification\" not in train_dataset.get_field_schema():\n",
    "    train_preds, train_logits = [], []\n",
    "    with torch.inference_mode():\n",
    "        for imgs, _ in tqdm(train_inference_loader, desc=\"Getting train preds\"):\n",
    "            logits = loaded_model(imgs.to(device))\n",
    "            train_logits.append(logits.cpu().numpy())\n",
    "            train_preds.extend(torch.max(logits, 1)[1].cpu().numpy())\n",
    "    train_logits = np.concatenate(train_logits)\n",
    "    \n",
    "    classifications = []\n",
    "    for i in range(len(train_dataset)):\n",
    "        pred_idx = train_preds[i]\n",
    "        logits = train_logits[i]\n",
    "        conf = float(Fun.softmax(torch.tensor(logits), dim=0)[pred_idx])\n",
    "        classifications.append(fo.Classification(label=dataset_classes[pred_idx], confidence=conf, logits=logits.tolist()))\n",
    "    train_dataset.set_values(\"lenet_train_classification\", classifications)\n",
    "    train_dataset.save()\n",
    "\n",
    "# Create view of misclassified training samples\n",
    "mislabeled_train_images_view = train_dataset.match(F(\"lenet_train_classification.label\") != F(\"ground_truth.label\"))\n",
    "print(f\"Found {len(mislabeled_train_images_view)} misclassified training samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Augmentations\n",
    "\n",
    "Effective augmentation for MNIST involves creating realistic variations that a model might encounter. We'll use small geometric transformations (rotation, translation, scaling) and elastic deformations to simulate natural handwriting styles. We will use the `albumentations` library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(51)\n",
    "mnist_augmentations = A.Compose([\n",
    "    A.Affine(\n",
    "        translate_percent={\"x\": (-0.1, 0.1), \"y\": (-0.1, 0.1)},\n",
    "        scale=(0.9, 1.1),\n",
    "        rotate=(-10, 10),\n",
    "        p=0.8\n",
    "    ),\n",
    "    A.ElasticTransform(\n",
    "        alpha=20, sigma=5, border_mode=cv2.BORDER_CONSTANT, p=0.6\n",
    "    ),\n",
    "    A.GridDistortion(num_steps=3, distort_limit=0.1, p=0.3),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an Augmented Dataset\n",
    "\n",
    "We'll define a new PyTorch `Dataset` class that takes our misclassified samples and applies these augmentations on the fly. For each misclassified sample, it will generate multiple augmented versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentedMNISTDataset(Dataset):\n",
    "    def __init__(self, fiftyone_view, label_map, base_transforms, augmentations, augment_factor=5):\n",
    "        self.image_paths = fiftyone_view.values(\"filepath\")\n",
    "        self.str_labels = fiftyone_view.values(\"ground_truth.label\")\n",
    "        self.label_map = label_map\n",
    "        self.base_transforms = base_transforms\n",
    "        self.augmentations = augmentations\n",
    "        self.augment_factor = augment_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths) * self.augment_factor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_idx = idx // self.augment_factor\n",
    "        image = Image.open(self.image_paths[base_idx]).convert('L')\n",
    "        image_np = np.array(image, dtype=np.uint8)\n",
    "        augmented = self.augmentations(image=image_np)['image']\n",
    "        image = Image.fromarray(augmented).convert(\"L\")\n",
    "        if self.base_transforms: image = self.base_transforms(image)\n",
    "        label_idx = self.label_map.get(self.str_labels[base_idx], -1)\n",
    "        return image, torch.tensor(label_idx, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original training set size: 51000\n",
      "Augmented samples added: 5120\n",
      "Combined dataset size: 56120\n"
     ]
    }
   ],
   "source": [
    "torch_augmented_dataset = AugmentedMNISTDataset(\n",
    "    mislabeled_train_images_view,\n",
    "    label_map=label_map,\n",
    "    base_transforms=image_transforms,\n",
    "    augmentations=mnist_augmentations,\n",
    "    augment_factor=10\n",
    ")\n",
    "\n",
    "# Combine original training set with the new augmented samples\n",
    "combined_dataset = ConcatDataset([torch_train_set, torch_augmented_dataset])\n",
    "print(f\"Original training set size: {len(torch_train_set)}\")\n",
    "print(f\"Augmented samples added: {len(torch_augmented_dataset)}\")\n",
    "print(f\"Combined dataset size: {len(combined_dataset)}\")\n",
    "\n",
    "# Create a new DataLoader for fine-tuning\n",
    "combined_train_loader = create_deterministic_training_dataloader(\n",
    "    combined_dataset, batch_size=64, shuffle=True, num_workers=os.cpu_count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-Tuning the Model\n",
    "\n",
    "We'll now fine-tune our model. We start with the best weights from our initial training and train for a few more epochs on the combined dataset. We use a **lower learning rate** for fine-tuning to make small, careful adjustments to the already-learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 1:   0%|          | 0/877 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:12<00:00, 71.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Validation Loss: 0.0393\n",
      "Saved improved retrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:12<00:00, 67.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Validation Loss: 0.0388\n",
      "Saved improved retrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:16<00:00, 52.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Validation Loss: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:12<00:00, 69.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Validation Loss: 0.0387\n",
      "Saved improved retrained model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:12<00:00, 70.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 - Validation Loss: 0.0405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:12<00:00, 67.56it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 - Validation Loss: 0.0407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 64.22it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 - Validation Loss: 0.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 66.72it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 - Validation Loss: 0.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 65.59it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 - Validation Loss: 0.0431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 66.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 - Validation Loss: 0.0426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 65.25it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 - Validation Loss: 0.0434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 65.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 - Validation Loss: 0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 63.62it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13 - Validation Loss: 0.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 63.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 - Validation Loss: 0.0441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retraining Epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 877/877 [00:13<00:00, 64.31it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 - Validation Loss: 0.0441\n"
     ]
    }
   ],
   "source": [
    "set_seeds(51)\n",
    "# Load the best model to start fine-tuning\n",
    "retrain_model = ModernLeNet5().to(device)\n",
    "retrain_model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# Use a smaller learning rate for fine-tuning\n",
    "retrain_optimizer = Adam(retrain_model.parameters(), lr=0.0001)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# Reload the validation loader\n",
    "val_dataset = fo.load_dataset(\"mnist-validation-set\")\n",
    "torch_val_set = CustomTorchImageDataset(val_dataset, image_transforms, label_map)\n",
    "val_loader = torch.utils.data.DataLoader(torch_val_set, batch_size=64)\n",
    "\n",
    "# Training loop\n",
    "retrain_epochs = 15\n",
    "best_retrain_val_loss = float('inf')\n",
    "retrain_model_save_path = Path(os.getcwd()) / 'retrained_lenet.pth'\n",
    "\n",
    "for epoch in range(retrain_epochs):\n",
    "    retrain_model.train()\n",
    "    # Simplified training and validation epoch functions for brevity\n",
    "    for images, labels in tqdm(combined_train_loader, desc=f\"Retraining Epoch {epoch+1}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        retrain_optimizer.zero_grad()\n",
    "        logits = retrain_model(images)\n",
    "        loss = ce_loss(logits, labels)\n",
    "        loss.backward()\n",
    "        retrain_optimizer.step()\n",
    "\n",
    "    retrain_model.eval()\n",
    "    val_losses = []\n",
    "    with torch.inference_mode():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            logits = retrain_model(images)\n",
    "            val_losses.append(ce_loss(logits, labels).item())\n",
    "    \n",
    "    avg_val_loss = np.mean(val_losses)\n",
    "    print(f\"Epoch {epoch+1} - Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss < best_retrain_val_loss:\n",
    "        best_retrain_val_loss = avg_val_loss\n",
    "        torch.save(retrain_model.state_dict(), retrain_model_save_path)\n",
    "        print(\"Saved improved retrained model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation\n",
    "\n",
    "Finally, let's evaluate our newly fine-tuned model on the test set and compare its performance to the original model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating retrained model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 157/157 [00:03<00:00, 50.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrained predictions stored.\n"
     ]
    }
   ],
   "source": [
    "# Load the best retrained model\n",
    "final_model = ModernLeNet5().to(device)\n",
    "final_model.load_state_dict(torch.load(retrain_model_save_path, map_location=device))\n",
    "final_model.eval()\n",
    "\n",
    "# Create test loader\n",
    "torch_test_set = CustomTorchImageDataset(test_dataset, image_transforms, label_map)\n",
    "test_loader = torch.utils.data.DataLoader(torch_test_set, batch_size=64)\n",
    "\n",
    "# Run inference with the retrained model\n",
    "retrained_predictions, retrained_logits = [], []\n",
    "with torch.inference_mode():\n",
    "    for images, _ in tqdm(test_loader, desc=\"Evaluating retrained model\"):\n",
    "        logits = final_model(images.to(device))\n",
    "        retrained_logits.append(logits.cpu().numpy())\n",
    "        retrained_predictions.extend(torch.max(logits.data, 1)[1].cpu().numpy())\n",
    "\n",
    "retrained_logits = np.concatenate(retrained_logits, axis=0)\n",
    "\n",
    "# Store retrained predictions in FiftyOne\n",
    "for i, sample in enumerate(test_dataset):\n",
    "    pred_idx = retrained_predictions[i]\n",
    "    logits = retrained_logits[i]\n",
    "    conf = float(Fun.softmax(torch.tensor(logits), dim=0)[pred_idx])\n",
    "    sample[\"retrained_lenet_classification\"] = fo.Classification(\n",
    "        label=dataset_classes[pred_idx], confidence=conf, logits=logits.tolist()\n",
    "    )\n",
    "    sample.save()\n",
    "\n",
    "print(\"Retrained predictions stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Original Model Performance ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0 - zero       0.98      1.00      0.99       980\n",
      "     1 - one       0.99      1.00      0.99      1135\n",
      "     2 - two       0.99      0.99      0.99      1032\n",
      "   3 - three       0.99      0.99      0.99      1010\n",
      "    4 - four       0.99      0.99      0.99       982\n",
      "    5 - five       0.98      0.99      0.99       892\n",
      "     6 - six       0.99      0.98      0.99       958\n",
      "   7 - seven       0.99      0.97      0.98      1028\n",
      "   8 - eight       0.99      0.99      0.99       974\n",
      "    9 - nine       0.98      0.98      0.98      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "\n",
      "--- Retrained Model Performance ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    0 - zero       1.00      0.99      1.00       980\n",
      "     1 - one       1.00      1.00      1.00      1135\n",
      "     2 - two       1.00      0.99      0.99      1032\n",
      "   3 - three       0.99      1.00      0.99      1010\n",
      "    4 - four       0.99      0.99      0.99       982\n",
      "    5 - five       0.99      0.99      0.99       892\n",
      "     6 - six       0.99      0.99      0.99       958\n",
      "   7 - seven       0.98      1.00      0.99      1028\n",
      "   8 - eight       0.99      0.99      0.99       974\n",
      "    9 - nine       0.99      0.98      0.99      1009\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n",
      "\n",
      "Accuracy Improvement: +0.0050\n"
     ]
    }
   ],
   "source": [
    "# Evaluate original and retrained models\n",
    "original_eval = test_dataset.evaluate_classifications(\"lenet_classification\", eval_key=\"original_eval\")\n",
    "retrained_eval = test_dataset.evaluate_classifications(\"retrained_lenet_classification\", eval_key=\"retrained_eval\")\n",
    "\n",
    "print(\"\\n--- Original Model Performance ---\")\n",
    "original_eval.print_report()\n",
    "\n",
    "print(\"\\n--- Retrained Model Performance ---\")\n",
    "retrained_eval.print_report()\n",
    "\n",
    "# Compare performance\n",
    "orig_accuracy = original_eval.metrics()['accuracy']\n",
    "retrain_accuracy = retrained_eval.metrics()['accuracy']\n",
    "print(f\"\\nAccuracy Improvement: {retrain_accuracy - orig_accuracy:+.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Changes\n",
    "\n",
    "Let's see exactly which samples were fixed by retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples fixed by retraining: 74\n",
      "Samples broken by retraining: 24\n",
      "Net improvement in correct predictions: 50\n",
      "Connected to FiftyOne on port 5151 at 0.0.0.0.\n",
      "If you are not connecting to a remote session, you may need to start a new session and specify a port\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=11b51075-dea2-4743-9aed-bc5d61c381f4\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x755320973940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"800\"\n",
       "            src=\"http://0.0.0.0:5151/?notebook=True&subscription=12f1c4de-52b2-4f38-ac62-3b2cab596d0b\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7550d8159a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "View the fixed samples in the App: http://0.0.0.0:5151/\n"
     ]
    }
   ],
   "source": [
    "originally_wrong = test_dataset.match(F(\"lenet_classification.label\") != F(\"ground_truth.label\"))\n",
    "now_correct = originally_wrong.match(F(\"retrained_lenet_classification.label\") == F(\"ground_truth.label\"))\n",
    "\n",
    "now_wrong = test_dataset.match(\n",
    "    (F(\"lenet_classification.label\") == F(\"ground_truth.label\")) & \n",
    "    (F(\"retrained_lenet_classification.label\") != F(\"ground_truth.label\"))\n",
    ")\n",
    "\n",
    "print(f\"Samples fixed by retraining: {len(now_correct)}\")\n",
    "print(f\"Samples broken by retraining: {len(now_wrong)}\")\n",
    "print(f\"Net improvement in correct predictions: {len(now_correct) - len(now_wrong)}\")\n",
    "\n",
    "session = fo.launch_app(test_dataset)\n",
    "session.view = now_correct\n",
    "print(f\"\\nView the fixed samples in the App: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You have completed the entire workflow from data exploration to model training, analysis, and targeted improvement. You've seen how a generalist model like CLIP provides a baseline, and how a specialized, supervised model can achieve superior performance. Most importantly, you've learned how to use model predictions and embeddings to analyze your dataset, find problematic samples, and use that information to make your model even better.\n",
    "\n",
    "Please see `summary.md` for a full recap and suggested next steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
